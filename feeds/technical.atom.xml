<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Langr's Blog - technical</title><link href="jakublangr.com/" rel="alternate"></link><link href="http://www.jakublangr.com/feeds/technical.atom.xml" rel="self"></link><id>jakublangr.com/</id><updated>2019-05-11T21:00:00+01:00</updated><subtitle>About data and all things awesome</subtitle><entry><title>GANs &amp; applied ML @ ICLR 2019</title><link href="jakublangr.com/iclr19-gans.html" rel="alternate"></link><published>2019-05-11T21:00:00+01:00</published><updated>2019-05-11T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2019-05-11:jakublangr.com/iclr19-gans.html</id><summary type="html">&lt;p&gt;Understanding the bleeding-edge GAN papers from one of the world's best known AI research conferences. Some musings on the academic workshops on "applied ML".&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have just returned from ICLR 2019 in New Orleans and what a fruitful year that was on GAN papers-we saw papers on image synthesis (BigGAN), audio (WaveGAN), feature selection (KnockoffGAN), 3D, text &amp;amp; tabular and many other! The second part of this article is then focused on more practical ML considerations.&lt;/p&gt;
&lt;h1&gt;Fortune favours the prepared&lt;/h1&gt;
&lt;p&gt;Before going to ICLR, I made a list of all the talks &amp;amp; workshops that had something that I wanted to learn. This meant a very busy Monday—where at one point four interesting workshops were running in parallel. It also meant a busy Tuesday where the organizers put 37 GAN papers into the day. This meant starting the poster session early and finishing late. I have kept track of all this using a &lt;a href="https://docs.google.com/spreadsheets/d/1H6Atx9GRflN94fOt1Eil5UqHMWWReWUksjVqqbKhtmU/edit#gid=0"&gt;spreadsheet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have included links to all the papers I mention and there are even links to the &lt;a href="https://slideslive.com/iclr"&gt;livestreamed workshops&lt;/a&gt; as well as the &lt;a href="https://www.facebook.com/pg/iclr.cc/videos/?ref=page_internal"&gt;plenary session, which also heavily featured GANs&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Generated Adversarial section&lt;/h2&gt;
&lt;p&gt;Here I want to explore the changes specifically just discussing Generative Adversarial Networks (GANs). As many have said, this is an exciting new technology that—unlike most of other ML—has only been around for less than 5 years. In the spirit of the previous &lt;a href="https://towardsdatascience.com/all-icml-gans-generative-papers-2018-62b4521bf92"&gt;ICML 2018 article&lt;/a&gt;, I talked to academics so you don't have to, but given the volume of the content, it is no longer possible to go through every paper, so I will just pick some main themes.&lt;/p&gt;
&lt;h1&gt;Theme 1: Image synthesis is maturing&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.iangoodfellow.com/slides/"&gt;Ian Goodfellow frequently talks&lt;/a&gt; of how deep learning revolutions in 2014, enabled "Cambrian explosion" of downstream machine learning. This is because in any technical field the first order of business is to make a technology work reliably and that enables a whole wealth of downstream applications. &lt;/p&gt;
&lt;p&gt;This has somewhat happened with image synthesis. Now that &lt;a href="https://arxiv.org/abs/1809.11096"&gt;BigGAN can reliably generate very diverse high-fidelity images&lt;/a&gt;, we can start thinking about applying it for other use-cases. One example is using BigGAN as a way to augment the existing training data (i.e. artificially increasing the number of data points by synthesizing new ones). Now even though there was another paper accepted at ICLR that showed the limitations of this technique (https://openreview.net/forum?id=rJMw747l_4). The sheer fact that this is a proposal that is seriously studied seems like a good sign.&lt;/p&gt;
&lt;p&gt;Another downstream task that we may care about is image synthesis with fewer labels. In the original BigGAN, we are using all labels in ImageNet to synthesize the 1,000 types of objects. &lt;a href="https://arxiv.org/pdf/1903.02271.pdf"&gt;However in another ICLR paper&lt;/a&gt;, we can see equally high quality pictures with just 10% of the labels and even better results than BigGAN with just 20%.&lt;/p&gt;
&lt;p&gt;Furthermore, ICLR featured &lt;a href="https://openreview.net/forum?id=rkxoNnC5FQ"&gt;several&lt;/a&gt; &lt;a href="https://openreview.net/forum?id=ryxwJhC9YX"&gt;papers&lt;/a&gt; that had interesting proposals to achieve more granular control over the generated images so that that giraffe you always wanted (and therefore domain-adapted from all your photos) instead of your stupid car can look just right.&lt;/p&gt;
&lt;p&gt;I am just amazed at how quickly the field is moving that in less than 5 years since the original paper, we have managed to produce 1000 classes of 512x512 images that are realistic enough to be used in downstream applications. In the words of &lt;a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg"&gt;Károly Zsolnai-Fehér&lt;/a&gt;, what a time to be alive!&lt;/p&gt;
&lt;h1&gt;Theme 2: Exotic data types / applications.&lt;/h1&gt;
&lt;p&gt;Another substantial theme in this year's ICLR was the presence of more "exotic" data types and applications. I'll just go through a couple of the more interesting ones. To me, this again seems somewhat indicative of the growing maturity of GANs as a field. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=ByMVTsR5KQ"&gt;WaveGAN&lt;/a&gt;: is a conditional synthesis of audio using GANs. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=S1lvm305YQ"&gt;TimbreTron&lt;/a&gt;: uses CycleGAN to transfer music from one instrument (domain) to music of another (domain) instrument.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=S1zk9iRqF7"&gt;PateGAN&lt;/a&gt;: is a GAN to generate synthetic data with differential privacy guarantees.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=ByeZ5jC5YQ"&gt;KnockoffGAN&lt;/a&gt;: is a way to do robust feature selection with GANs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=HJxB5sRcFQ"&gt;LayoutGAN&lt;/a&gt;: way to generate UI wire-frames using GANs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/group?id=ICLR.cc/2019/Workshop/DeepGenStruct"&gt;CompositionalGAN&lt;/a&gt;: ways to generate realistic looking compositions of 3D objects.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=SJeXSo09FQ"&gt;3D point cloud generation&lt;/a&gt;, &lt;a href=""&gt;Protein backbone generation&lt;/a&gt; &amp;amp; &lt;a href="https://openreview.net/group?id=ICLR.cc/2019/Workshop/DeepGenStruct"&gt;Generating labelled graph&lt;/a&gt;: these papers are outside of my area of expertise and papers in this broad area featured at ICML 2018 as well, but it is great to see that the work is continuing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Theme 3: Theoretical advances&lt;/h1&gt;
&lt;p&gt;As always, there were many papers dealing with some aspect of training (&lt;a href="https://openreview.net/forum?id=S1GkToR5tm"&gt;rejection sampling&lt;/a&gt;, &lt;a href="https://openreview.net/forum?id=S1erHoR5t7"&gt;relativistic GAN&lt;/a&gt;, &lt;a href="https://openreview.net/forum?id=HyxPx3R9tm"&gt;variational discriminator bottleneck&lt;/a&gt;) or some theoretical property of generative models (e.g. &lt;a href="https://openreview.net/forum?id=SyMhLo0qKQ"&gt;latent space interpolations&lt;/a&gt; or &lt;a href="https://arxiv.org/pdf/1802.05701.pdf"&gt;invertibility of GANs&lt;/a&gt;). &lt;/p&gt;
&lt;p&gt;While academics tend to love this area, at ICML '18, the results were somewhat mixed. I felt that many papers introduced a &lt;em&gt;huge&lt;/em&gt; amount of extra complexity to derive some properties that I did not think are hugely interesting or do not expect them to become the de facto standard the same way e.g. Wasserstein GAN or gradient penalties are.&lt;/p&gt;
&lt;p&gt;Fortunately, at ICLR that was not the case. All three techniques from above plus &lt;a href="https://openreview.net/forum?id=SJgw_sRqFQ"&gt;averaging during training&lt;/a&gt; all look like simple, effective techniques that could easily become the standard pattern for the future state of the art.&lt;/p&gt;
&lt;h2&gt;Applied Machine Learning&lt;/h2&gt;
&lt;p&gt;As someone who still frequently has to worry about how I am going to productionize the systems I am building. I was very pleasantly surprised that even the workshop organizers from ICLR thought this was important. So I was trying to capture all the interesting content from the following workshops:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/view/icml-reproducibility-workshop/home"&gt;Reproducibility in ML&lt;/a&gt;: this ended up being a pretty useful workshop in the end. But there was about a grand total of 7 people when I was there, so I wonder what that says about the state of our field. Generally, I regard reproducibility to be an incredibly important topic, because reproducibility is really the &lt;strong&gt;level 0&lt;/strong&gt; of understanding how deployed ML systems behave. So all this talk about fairness &amp;amp; bias is almost pointless if we do not get this right.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://debug-ml-iclr2019.github.io/"&gt;Debugging ML&lt;/a&gt;: This was pretty useful workshop, but a lot of the presentations sadly either did not release code or were very academic. I will definitely try to investigate &lt;a href="https://dawn.cs.stanford.edu//2019/03/11/modelassertions/"&gt;Model Assertions&lt;/a&gt;, as the idea makes a lot of sense to me. Overall, debugging again is extremely key for us to ensure that we somewhat understood how the models are building. Everything from adversarial examples to neural nets being able to fit randomly assigned labels are all indicators that we need more tools to understand deep learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://lld-workshop.github.io/"&gt;Learning from limited labelled data&lt;/a&gt;: This is incredibly interesting as little data is a frequent business reality. I was encouraged by the fact that Christopher Re was involved, however, I do not feel that for me there were any particularly strong takeaways.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deep-gen-struct.github.io/"&gt;Generating highly structured data&lt;/a&gt;: Despite Bengio's crowded talk in the beginning, I did not find the oral presentations to be &lt;em&gt;that&lt;/em&gt; useful, though I highly recommend checking out the accepted papers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, I am constantly amazed at the rate of progress of ML and academic conferences have their disadvantages, but if you plan &amp;amp; prepare accordingly, you will get much more of them than any other conference I have ever been to.&lt;/p&gt;</content><category term="AI artificial intelligence"></category></entry><entry><title>List of ICML GAN Papers</title><link href="jakublangr.com/icml-gans.html" rel="alternate"></link><published>2018-08-07T21:00:00+01:00</published><updated>2018-08-07T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2018-08-07:jakublangr.com/icml-gans.html</id><summary type="html">&lt;p&gt;All ICML GANs / Generative Papers: I went to Stockholm and spoke to academics, so you don't have to! Domain adaptation, 3D GANs, Data Inputation using GANs and much more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;In all seriousness, however, I do respect greatly all the amazing work that  the researchers at ICML have presented. I would not be capable of anywhere near their level of work so kudos to them for pushing the field forward!&lt;/p&gt;
&lt;p&gt;ICML overall was an awesome experience. But this piece is not going to be about my thoughts, impressions or experiences. The industry of slowly slipping into academic conferences I thought I might take it one step further and present what are the most interesting results for the practitioners perspective that came up at  this conference.&lt;/p&gt;
&lt;p&gt;This means I will be discounting some of the academic  contributions if I do not see how they could apply to someone whose ultimate goal is not first and foremost to publish. This also means that I will be using more accessible language and will not go into any given paper.  I will try to present this in a more accessible way, which also means I will here and there contribute some opinions and thoughts that some of you may disagree with. My hope here is that this will be useful especially to the newcomers to the field to give them some context from the about 50 papers that I have read in preparation for this conference. &lt;/p&gt;
&lt;p&gt;Very frequently the explanations will be quick and dirty  as exemplified by the fact that not every single paper is strictly speaking a generative adversarial network. So here it goes.&lt;/p&gt;
&lt;p&gt;As part of my training budget at &lt;a href="http://mudano.com"&gt;Mudano&lt;/a&gt;, I chose to go to ICML, a top 3 conference in Machine Learning. It was a very enriching and unique experience. But in order to get the most out of the conference I decided to keep track of most papers I have encountered. These are then presented chronologically as they appeared in the conference. I'm not including Saturday and Sunday which were mostly about workshops. [1]&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Wednesday 11th&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://medianetlab.ee.ucla.edu/papers/ICML_RadialGAN.pdf"&gt;&lt;b&gt;RadialGAN&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;The promise of GANs has always partially lied in semi-supervised learning and data augmentation. This work basically allows us to leverage multiple datasets from several sources to typically obtain better performance, even if some of the datasets are not of high quality or related to the task at hand. This is done by first translating the dataset into some shared latent space and then translating this space back to the target domain for the task at hand.&lt;/p&gt;
&lt;p&gt;My thoughts: I really like this paper. Potentially very useful in the industry. Also very clear presentation. One of my colleagues was really excited by the potential. This paper really can be a game-changer, if it applies to your problem. &lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/radial_gan.png" alt="Radial GAN" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://mlg.eng.cam.ac.uk/adrian/ICML18-Discovering.pdf"&gt;&lt;b&gt;Discovering Interpretable Representations for Both Deep Generative and Discriminative Models&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This is basically a human-in-the-loop Active Learning way of making sure we can interpret the latent space most (human)-meaningfully. Potentially decreases the accuracy, but allows us to say which dimensions of the latent space are affecting what properties.&lt;/p&gt;
&lt;p&gt;My thoughts: Potentially interesting, because frequently figuring out how the latent space translates to the generated samples is non-trivial. Presentation was not very clear.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://avg.is.tuebingen.mpg.de/publications/meschedericml2018"&gt;&lt;b&gt;Which Training Methods for GANs do actually Converge?&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;In this paper, Lars et al. present a way--starting from a toy example--to impose a specific class of gradient penalties on the GANs throughout training. They show results matching state of the art (SOTA) [2] even with older architectures. The authors furthermore demonstrate some interesting properties in the local stability around the Nash Equilibrium of the GAN. Code is also available in the link above. &lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/convergene.png" alt="GAN convergence" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;p&gt;My thoughts: Quite an impressive feat. This technique is quite complicated, but taking the time to understand is probably worth the time. Others have looked at similar ideas. But in this case, results really speak for themselves. PGGAN-quality results without progressive growing!&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/tao18b/tao18b.pdf"&gt;&lt;b&gt;Chi-square Generative Adversarial Network&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This work connects so far three disconnected approaches to GANs--see the picture below. &lt;/p&gt;
&lt;p&gt;My thoughts: My take on this is work is that this is probably an important piece of theory, but the use for practitioners is likely somewhat limited.  Fully explaining what &lt;a href="https://arxiv.org/abs/1705.08584"&gt;Moment Matching Discrepancy&lt;/a&gt; is beyond the scope of this piece. We can simplify IPM to be simply distilling the difference of generated and real distribution to &lt;a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance"&gt;Earth Mover's distance&lt;/a&gt; and f-GANs are based on &lt;a href="https://theintelligenceofinformation.files.wordpress.com/2017/01/fganpapertable1.jpg?w=700"&gt;f-divergences&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Seems a bit too convoluted for practitioners.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/chi_square.png" alt="Chi Square GAN" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="https://core.ac.uk/display/129356887"&gt;&lt;b&gt;A Classification-Based Study of Covariate Shift in GAN Distributions&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This is a cool study of detecting mode collapse. This generally makes sense especially for academics trying to detect problems with training. This means we can now finally start talking about the effect of these missing modes and gaps. &lt;/p&gt;
&lt;p&gt;My thoughts: I think there's loads of value in having a consistent / unique benchmark for evaluating training quality. But there are metrics that I've liked better, but this could probably be used as an ensemble.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/covariate_shift_gans.png" alt="Covariate Shift in GANs" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/bojchevski18a/bojchevski18a.pdf"&gt;&lt;b&gt;NetGAN: Generating Graphs via Random Walks&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Thoughts: Generally interesting work showing that GANs can be applied to generate even graphs that are for many reasons more complicated. Achieves state of the art according to link prediction accuracy. &lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/net_gan.png" alt="NetGAN" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;p&gt;Thursday 12th&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/khrulkov18a/khrulkov18a.pdf"&gt;&lt;b&gt;Geometry Score: A Method For Comparing Generative Adversarial Networks&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This work shows that using topological analysis we can construct a general metric to evaluate how much of the original dataset we have managed to cover. Using an approximation, we can calculate how much of the diversity is in our generated dataset.&lt;/p&gt;
&lt;p&gt;Thoughts: I am actually really excited by this work as it allows us to evaluate GANs in &lt;em&gt;any&lt;/em&gt; domain and inspect it for mode collapse. So far, generic evaluation methods have been completely missing. Maybe we will even see a measure of quality one day.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/geometry_score.png" alt="Geometry Score" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf"&gt;&lt;b&gt;Optimizing the Latent Space of Generative Networks&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper deviates from the typical GAN set up, but sets out to create a model that would be able to generate better samples. Here, we focus on the issue of mode collapse and generating samples that are not the same, but are similar enough.&lt;/p&gt;
&lt;p&gt;Thoughts: I have a bit of mixed feelings about the paper. On one hand, I think that the presentation was surprisingly opinionated--unusual for academics--and I did not agree with a lot of the statements. On the other hand, the informal poster discussion was actually really good and informative. In this discussion, one of the researchers made an interesting point that mode collapse is the reasons why GANs work. That's a fascinating assertion and I would love to see whether that is true. The funny thing about this paper is that when I was researching it, I found out that it narrowly got rejected from ICLR so the authors republished to ICML.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/glo.png" alt="Generative Latent-space Optimization" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/cao18a/cao18a.pdf"&gt;&lt;b&gt;Adversarial Learning with Local Coordinate Coding&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper attempts to solve a similar problem as the previous paper but making the latent space more complex. The presentation was not very clear, but the gist is exploring the manifold assumption--which states that there exists a mapping from some low dimensional latent space onto the complex manifold (e.g. images). All GANs ultimately rely on this assumption, but if you think about this mapping, it seems odd that you could represent all the images from such a low space. For some reason, this paper does not compare to the state of the art methods.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/llc.png" alt="Local Coordinate Coding" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/achlioptas18a/achlioptas18a.pdf"&gt;&lt;b&gt;Learning Representations and Generative Models for 3D Point Clouds&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Who does not love 3D point clouds? So awesome. In this paper, the authors create a more powerful model to generate 3D point clouds of generic objects. While there is still a lot of work to be done, it looks awesome. &lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/3d_gan.png" alt="Generative Model for 3D point clouds" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/pan18c/pan18c.pdf"&gt;&lt;b&gt;Theoretical Analysis of Image-to-Image Translation with Adversarial Learning&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper analyzes the paired translation GANs and says that essentially there is two components to the loss for paired image-to-image translation. The identity loss that ensures the image is correct and then the adversarial loss, which ensures the image is sharp.&lt;/p&gt;
&lt;p&gt;Thoughts: Interesting theoretical work, but probably not for practitioners just yet. Also there are no pictures in the paper so here's at least a blurry one (sorry!) from the poster session.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/poster.jpg" alt="Image-to-Image Analysis" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/johnson18a/johnson18a.pdf"&gt;&lt;b&gt;Composite Functional Gradient Learning of Generative Adversarial Models&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Thoughts: This paper introduces a more complicated training regime that seems to be theoretically supported, but then goes and creates an approximate version that adds an if statement to the training procedure, which to me smells of ad-hoc. The results surpass state of the art, but I generally like methods that add less complexity.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/composite_gan.png" alt="Composite GAN" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/sajjadi18a/sajjadi18a.pdf"&gt;&lt;b&gt;Tempered Adversarial Networks&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;The high level idea behind this paper is interesting. Similar to PGGAN, authors think that the problem that GANs are presented with from the outset is a bit too hard. So instead, the authors create a network that deforms the images a bit so that the generator has an easier job. A decayed version of this, beats the state of the art.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/bang18a/bang18a.pdf"&gt;&lt;b&gt;Improved Training of Generative Adversarial Networks using Representative Features&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper basically concatenates an autoencoded version of the generated image that gets passed to the discriminator as help. The author’s poster was incredibly clear in describing the architecture, but unfortunately, the pictures seem to have disappeared from my phone.&lt;/p&gt;
&lt;p&gt;Their results are better than the state of the art, but not by much, so I am not sure whether it is actually that much of a breakthrough.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/rep_feat_gan.png" alt="Representative Features GAN" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/liu18d/liu18d.pdf"&gt;&lt;b&gt;A Two-Step Computation of the Exact GAN Wasserstein Distance&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper introduced linear programming to compute the exact Wasserstein distance tractably and then use that to improve the training. The problem with the Wasserstein distance is that when you think about how combinatorially complex it is for even simple point clouds, computing the exact distance seems pretty complicated. This paper achieves that and beats state of the art.&lt;/p&gt;
&lt;p&gt;The dataset that this paper uses as benchmarks are both quite simple and limited (MNIST, CIFAR-10), so I would love to see how well this method does on e.g. Celeb-A HQ or IMAGENET-1000.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://arxiv.org/abs/1802.08768"&gt;&lt;b&gt;Is Generator Conditioning Causally Related to GAN Performance?&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Some as with some other papers, I have read this paper ages ago, but the authors did a great job of succinctly summarizing the results, especially for the poster. Basically, they use Jacobian clamping to curb the Generator updates and therefore achieve a much more stable training. The performance is not substantially improved, but the increase in stability is worth it.&lt;/p&gt;
&lt;p&gt;As a practitioner, if you have problems with GAN stability, this is the paper to try.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/yoon18a/yoon18a.pdf"&gt;&lt;b&gt;GAIN: Missing Data Imputation using Generative Adversarial Nets&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper is probably one of the most interesting papers for practitioners, because it deals with a problem that a lot of us encounter--missing data. It creates a GAN-setup with a hint mechanism to basically smartly infer the missing values to match the distribution. We know GANs are good at creating synthetic distributions. The hint mechanism is necessary, because the problem for the discriminator is too hard--there are many combinatorially valid permutations of partially missing / real data to make this intractable. &lt;/p&gt;
&lt;p&gt;I have already pitched this paper to some of my colleagues. Enough said.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf"&gt;&lt;b&gt;Black-box Adversarial Attacks with Limited Queries and Information&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This is one of the few realistic adversarial attack papers. Technically no GAN / generative modelling involved--other than the perturbations I guess--but very interesting, realistic ways of doing adversarial attacks.&lt;/p&gt;
&lt;p&gt;I still don't think Deep Learning models are widespread enough and that there's enough trust in them for this to cause any real harm, but this paper &lt;a href="https://www.youtube.com/watch?v=kgTocVLNvYI"&gt;engages with the real issues.&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/seward18a/seward18a.pdf"&gt;&lt;b&gt;First Order Generative Adversarial Networks&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;The idea behind this paper is that in a case of gradient penalties (such as WGAN-GP) rather than optimizing for the WGAN loss and then adding the penalty, you optimize for the loss &lt;em&gt;with&lt;/em&gt; the penalty directly. Authors say that there are pathological cases when optimizing for loss and &lt;em&gt;then&lt;/em&gt; adding the penalty makes the &lt;em&gt;generated distribution itself&lt;/em&gt; less close to the target distribution. There were some informal grilling (not by me) / proofs of this around the poster but I won't hold the authors to it, haha.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://deepmind.com/blog/learning-to-generate-images/"&gt;&lt;b&gt;Synthesizing Programs for Images using Reinforced Adversarial Learning&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Here, I am not linking the paper itself but rather an excellent blog post by DeepMind, which does a great job of explaining this paper.
TL;DR: GAN to generate brushstrokes using Photoshop-like API does a good job of learning to arbitrarily draw.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/amodio18a/amodio18a.pdf"&gt;&lt;b&gt;MAGAN: Aligning Biological Manifolds&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Okay, so here I am now pretty sure that MAGAN stands for Manifold Alignment GAN (though never stated in the presentation), but when I first saw it being presented by an American, I thought it had some political connotations, haha. This GAN basically ensures that we align two manifolds always with the same correspondence (rather than stochastically as is the case with other algorithms) by adding in a correspondence loss. &lt;/p&gt;
&lt;p&gt;Interesting tidbit, in a "question" after the talk someone alleged that there already was a paper at last NIPS doing exactly this. Author was unaware of this paper. Your thoughts?&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/chapfuwa18a/chapfuwa18a.pdf"&gt;&lt;b&gt;Adversarial Time-to-Event Modeling&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;In a similar domain as the GAIN paper, except now focused on time-series and basically getting a better probability distribution (basically being able to infer it automagically) for time distribution of certain events--such as complications in hospital settings.&lt;/p&gt;
&lt;p&gt;Potentially very useful for people working with time-series.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf"&gt;&lt;b&gt;CyCADA: Cycle-Consistent Adversarial Domain Adaptation&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This paper deals with an issue that loads of practitioners find out over and over a-GAN: our models do not generalize (excuse the joke). Frequently, you may take a model that was e.g. trained on ImageNet, deploy it and see that it does very poorly. Just because the real world is a lot more complicated than ImageNet even if we only care about the ImageNet classes. CyCADA extends CycleGAN to basically be able to do domain to domain translation with correct semantics and thereby potentially enabling building e.g. self-driving cars in safe, scalable computer-generated environment and then translating the ML system into the real world.&lt;/p&gt;
&lt;p&gt;Really interesting! Will definitely try.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="images/gans/cycada.png" alt="Cycle-Consistent Domain Adaptation" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a.pdf"&gt;&lt;b&gt;Autoregressive Quantile Networks for Generative Modeling&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;So technically there is nothing GAN in this paper. "Just" autoregressive models, but they achieve results comparable to the state of the art in GANs. Really impressive work, but as with every autoregressive model this setup, will have trouble scaling. After the talk, authors proposed using an autoencoder to scale this up, but that brings a whole set of other challenges.&lt;/p&gt;
&lt;p&gt;Interesting approach, but similar (though lower quality) works have been presented and have never gotten over the 32x32 pixel limit, which is also the case here. Until that is solved (and many have tried), not so sure whether this is really scalable; also note GANs are already at 1024x1024.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/lucas18a/lucas18a.pdf"&gt;&lt;b&gt;Mixed batches and symmetric discriminators for GAN training&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Unlike regular GAN, this paper uses mixed batches of real and synthetic data to make the discriminator even better. As the authors say in the summary "a simple architectural
trick makes it possible to provably recover all functions of the batch as an unordered set".&lt;/p&gt;
&lt;p&gt;I genuinely love the paper, because it is an elegant idea and the summary actually summarizes the paper. Whether or not this become the dominant framework is yet to be seen, but I think the way the authors reference other architectures is a bit odd (they got them all from one paper).&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/pu18a/pu18a.pdf"&gt;&lt;b&gt;JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;Here, the authors have an architecture similar to CycleGAN, but rather than to infer solely the conditional distributions, it jointly learns the marginal probability distributions for each domain. We start from noise for generation of X and then condition on the X in generation of Y from the marginal. &lt;/p&gt;
&lt;p&gt;I have found the presentation to be a bit unclear, but the results seem really interesting. The text generation seems really impressive, but then the authors said this was actually done by an autoencoder from a latent space that the GAN generated.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/almahairi18a/almahairi18a.pdf"&gt;&lt;b&gt;Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This is a really neat extension to CycleGAN in its standard form by injecting latent space during the first and the second generation. Recall that Cycle-consistency loss is measured as diff(X1, X2), where X1 -&amp;gt; Y -&amp;gt; X2. Basically the authors give us extra variables to use to create samples that have certain qualities. For example, if we have an outline of a shoe in the Y domain we can generate a sample in the X domain, where the same type of shoe is blue or orange or whatever we choose. &lt;/p&gt;
&lt;p&gt;If you like CycleGAN, but want more control over the translation, you will love this.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://proceedings.mlr.press/v80/li18d/li18d.pdf"&gt;&lt;b&gt;On the Limitations of First-Order Approximation in GAN Dynamics&lt;/a&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;This is a pure theory paper that just reasons mostly on the level of simple examples. The key takeaway is an explanation why multiple discriminator updates probably make sense. This paper takes it to the extreme and shows good convergence properties in the case of optimal discriminator. But other than that, probably not of interest to practitioners at the moment. &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;So that wraps up ICML for the generative adversarial networks papers. I hope that you found this useful and if you'd like me to produce were from NIPS where I am going in December (or a write up on how to get the most out of a conference) please let me know.&lt;/p&gt;
&lt;p&gt;Thank you Karen Trippler and &lt;a href="https://dawn.cs.stanford.edu/benchmark/"&gt;Mihai Ermaliuc&lt;/a&gt; for your thoughts and comments!&lt;/p&gt;
&lt;p&gt;Want to join the conversation? See more at &lt;a href="http://jakublangr.com"&gt;jakublangr.com&lt;/a&gt; or tweet &lt;a href="https://twitter.com/langrjakub"&gt;@langrjakub&lt;/a&gt;! I am also writing a book about &lt;a href="https://www.manning.com/books/gans-in-action?a_aid=gans-action&amp;amp;a_bid=fd02700a"&gt;Generative Adversarial Networks, which you can check out here&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;[1] I went to the reproducible machine learning one which should hopefully be at least somewhat familiar to most machine learning practitioners. But if there is interest I can read that up too.&lt;/p&gt;
&lt;p&gt;[2] SOTA just means beating the previous best academic result at this benchmark. For instance, &lt;a href="https://dawn.cs.stanford.edu/benchmark/"&gt;DAWNBench&lt;/a&gt; is the current repository of state of the art for time to get to 93% classification accuracy on Imagenet.&lt;/p&gt;</content><category term="AI artificial intelligence"></category></entry><entry><title>Coding for GANS &amp; (Semi)-Learning</title><link href="jakublangr.com/gans-code.html" rel="alternate"></link><published>2017-09-10T14:00:00+01:00</published><updated>2017-09-10T14:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-09-10:jakublangr.com/gans-code.html</id><summary type="html">&lt;p&gt;As it often happens, I get busy at work and forget to publish something I should have really done months ago. Well, this code is that thing. But at least it's now nice and shiny.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;I'll jump straight into what we have explained on a high-level &lt;a href="http://jakublangr.com/gans-tutorial.html"&gt;last time&lt;/a&gt;. The code is also available on &lt;a href="https://github.com/jakubLangr/Gans-Semi-Supervised/blob/master/gans_semi_supervised_learning.ipynb"&gt;GitHub&lt;/a&gt; and on &lt;a href="https://medium.com/@james.langr"&gt;Medium&lt;/a&gt;. This part is identical to the Jupyter notebook, except it is lacking the code output.&lt;/p&gt;
&lt;h2&gt;Generative Adverserial Networks &amp;amp; Semi-Supervised Learning&lt;/h2&gt;
&lt;h1&gt;By Jakub Langr (originally written March 2017)&lt;/h1&gt;
&lt;p&gt;This code was written for me to experiment with some of the recent advancements in AI. I chose specificially semi-supervised learning and Generative Adverserial Networks (GANs) to push myself. Some of the code was done as a homework for the &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv"&gt;Creative Applications of Deep Learning Course&lt;/a&gt;, which was extremely helpful in helping me learn about modern AI. Some of the broad framework came as pre-coded set-up and explanations for the last part of the course by &lt;a href="https://www.linkedin.com/in/pkmital"&gt;Parag Mital&lt;/a&gt;, but this usage of his code is completely novel and took a lot of engineering, stitching together and abstractions.
In this Jupyter Notebook I do the following things:
1. Import all the necessary dependencies (as well as some that I just used during development but not in final version)
2. Use a GAN approach to generate synethic images. 
    + More specifically, &lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"&gt;this recently extremely popular unsupervised technique&lt;/a&gt; can learn the higher representations of what constitutes a human face (along with many attributes in the latent space) on the &lt;a href="mmlab.ie.cuhk.edu.hk/projects/CelebA.html"&gt;Celeb Dataset&lt;/a&gt; by competing another network to fool each other (explained later)
    + Alternatively, one can think of this approach as using an auto-encoder-style gene generative model that tries to generate new examples based on a seeding factor.
3. This seeding factor or 'latent feature space' invariably encode some aspects of the generative models and once understood, can be used to predictiably manipulate the nature of generated images--e.g. baldness, gender or smile. 
4. We can therefore generate an almost infinite supply of new examples and because we know how we manipulate the latent space, we can know their labels. In this example, we created 40,000 of Men and Women faces that can now be used for further training
5. Then we train the next layer classifier on the synthetic data for a binary classification of men or women faces.  Instead of training a new classifier from scratch, however, we use a &lt;code&gt;transfer learning&lt;/code&gt; approach using Oxford's &lt;code&gt;Visual Geometry Group&lt;/code&gt; or &lt;code&gt;vgg16&lt;/code&gt; pre-trained network to get higher accuracy without having to training for days on a massive cluster.
6. We use the different &lt;code&gt;vgg16&lt;/code&gt; Celebrity face predictions (&lt;code&gt;2623&lt;/code&gt; to be exact) and train a simple fully connected two-layer neural network on the synthetic examples with the labels. (This is in stead of the typical transfer learning approach that cuts off the last layer and trains on those. Here we simply split that into 2 steps)
7. Use the 100 hand-labelled (by me) examples to evalute the accuracy of the new classifier.&lt;/p&gt;
&lt;h3&gt;Motivation&lt;/h3&gt;
&lt;p&gt;This is really exciting because it allow us to train classifier with having virtually no labelled data as long as we have lots of unlabelled data, &lt;a href="http://jakublangr.com/ai-2016-review.html"&gt;which is a tremendously promising strategy especially for smaller companies with smaller datasets&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Brief definition of terms:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised learning&lt;/strong&gt;: is basically using unlabelled data in addition labelled data during the traing process&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generative Adverserial Netwokrs&lt;/strong&gt;: explained in detail below&lt;/p&gt;
&lt;p&gt;The code was done in &lt;code&gt;Tensorflow 1.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# First check the Python version&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version_info&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;You are running an older version of Python!&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;You should consider updating to Python 3.4.0 or&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;higher as the libraries built for this course&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;have only been tested in Python 3.4 and higher.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Try installing the Python 3.5 version of anaconda&amp;#39;&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;and then restart `jupyter notebook`:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;https://www.continuum.io/downloads&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now get necessary libraries&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tflearn&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;joblib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delayed&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage.transform&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="kp"&gt;resize&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.misc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;imresize&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.ndimage.filters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gaussian_filter&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ipyd&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;libs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset_utils&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_utils&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Make sure you have started notebook in the same directory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;as the provided zip file which includes the &amp;#39;libs&amp;#39; folder&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;and the file &amp;#39;utils.py&amp;#39; inside of it.  You will NOT be able&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;to complete this assignment unless you restart jupyter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;notebook inside the directory created by extracting&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;the zip file or cloning the github repo.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We&amp;#39;ll tell matplotlib to inline any drawn figures like so:&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ggplot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a name="part-1---generative-adversarial-networks-gan--deep-convolutional-gan-dcgan"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Generative Adversarial Networks (GAN) / Deep Convolutional GAN (DCGAN)&lt;/h1&gt;
&lt;p&gt;&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recall that a Generative Adversarial Network is two networks, a generator and a discriminator.  The "generator" takes a feature vector and decodes this feature vector to become an image. The discriminator is exactly like the encoder of the Autoencoder, except it can only have 1 value in the final layer.  We use a sigmoid to squash this value between 0 and 1, and then interpret the meaning of it as: 1, the image you gave me was real, or 0, the image you gave me was generated by the generator, it's a FAKE! So the discriminator is like an encoder which takes an image and then perfoms lie detection.  Are you feeding me lies?  Or is the image real?  &lt;/p&gt;
&lt;p&gt;Consider the AutoEncoders for instance.  The loss function operated partly on the input space.  It said, per pixel, what is the difference between my reconstruction and the input image?  The l2-loss per pixel.  Recall at that time we suggested that this wasn't the best idea because per-pixel differences aren't representative of our own perception of the image.  One way to consider this is if we had the same image, and translated it by a few pixels.  We would not be able to tell the difference, but the per-pixel difference between the two images could be enormously high.&lt;/p&gt;
&lt;p&gt;The GAN does not use per-pixel difference.  Instead, it trains a distance function: the discriminator.  The discriminator takes in two images, the real image and the generated one, and learns what a similar image should look like!  That is really the amazing part of this network and has opened up some very exciting potential future directions for unsupervised learning.  Another network that also learns a distance function is known as the siamese network.  We didn't get into this network in this course, but it is commonly used in facial verification, or asserting whether two faces are the same or not.&lt;/p&gt;
&lt;p&gt;The GAN network is notoriously a huge pain to train!  For that reason, we won't actually be training it.  Instead, we'll discuss an extension to this basic network called the VAEGAN (Variational Auto Encoder GAN). For now, let's stick with creating the GAN.&lt;/p&gt;
&lt;p&gt;Let's first create the two networks: the discriminator and the generator.  We'll first begin by building a general purpose encoder which we'll use for our discriminator.  What we want is for the input placeholder to be encoded using a list of dimensions for each of our encoder's layers.  In the case of a convolutional network, our list of dimensions should correspond to the number of output filters.  We also need to specify the kernel heights and widths for each layer's convolutional network.&lt;/p&gt;
&lt;p&gt;We'll first need a placeholder.  This will be the "real" image input to the discriminator and the discrimintator will encode this image into a single value, 0 or 1, saying, yes this is real, or no, this is not real.&lt;/p&gt;
&lt;p&gt;This description was kindly provided by Parag under &lt;a href="http://jakublangr.com/ai-2016-review.html"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;net = CV.get_celeb_vaegan_model()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We'll load the graph_def contained inside this dictionary.  It follows the same idea as the &lt;code&gt;inception&lt;/code&gt;, &lt;code&gt;vgg16&lt;/code&gt;, and &lt;code&gt;i2v&lt;/code&gt; pretrained networks.  It is a dictionary with the key &lt;code&gt;graph_def&lt;/code&gt; defined, with the graph's pretrained network.  It also includes &lt;code&gt;labels&lt;/code&gt; and a &lt;code&gt;preprocess&lt;/code&gt; key.  We'll have to do one additional thing which is to turn off the random sampling from variational layer.  This isn't really necessary but will ensure we get the same results each time we use the network.  We'll use the &lt;code&gt;input_map&lt;/code&gt; argument to do this.  Don't worry if this doesn't make any sense, as we didn't cover the variational layer in any depth.  Just know that this is removing a random process from the network so that it is completely deterministic.  If we hadn't done this, we'd get slightly different results each time we used the network (which may even be desirable for your purposes).&lt;/p&gt;
&lt;p&gt;Now let's get the relevant parts of the network: &lt;code&gt;X&lt;/code&gt;, the input image to the network, &lt;code&gt;Z&lt;/code&gt;, the input image's encoding, and &lt;code&gt;G&lt;/code&gt;, the decoded image.  In many ways, this is just like the Autoencoders we learned about above, except instead of &lt;code&gt;Y&lt;/code&gt; being the output, we have &lt;code&gt;G&lt;/code&gt; from our generator!  And the way we train it is very different: we use an adversarial process between the generator and discriminator, and use the discriminator's own distance measure to help train the network, rather than pixel-to-pixel differences.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;X = g.get_tensor_by_name(&amp;#39;net/x:0&amp;#39;)
Z = g.get_tensor_by_name(&amp;#39;net/encoder/variational/z:0&amp;#39;)
G = g.get_tensor_by_name(&amp;#39;net/generator/x_tilde:0&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's get some data to play with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;files = sorted(datasets.CELEB())
img_i = 20
img = plt.imread(files[img_i])
plt.imshow(img)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Exploring the Celeb Net Attributes¶
Let's now try and explore the attributes of our dataset. We didn't train the network with any supervised labels, but the Celeb Net dataset has 40 attributes for each of its 200k images. These are already parsed and stored for you in the net dictionary:&lt;/p&gt;
&lt;p&gt;Find the Latent Encoding for an Attribute
The Celeb Dataset includes attributes for each of its 200k+ images. This allows us to feed into the encoder some images that we know have a specific attribute, e.g. "smiling". We store what their encoding is and retain this distribution of encoded values. We can then look at any other image and see how it is encoded, and slightly change the encoding by adding the encoded of our smiling images to it! The result should be our image but with more smiling. That is just insane and we're going to see how to do it. First lets inspect our latent space:
Latent Feature Arithmetic
Let's now try to write a general function for performing everything we've just done so that we can do this with many different features. We'll then try to combine them and synthesize people with the features we want them to have...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def get_features_for(label=&amp;#39;Bald&amp;#39;, has_label=True, n_imgs=50):
    # Helper function to obtain labels and then preprocessing and returning
    # a vector for the seeding function for GAN
    # basically figures out the embedding for a particular attribute
    label_i = net[&amp;#39;labels&amp;#39;].index(label)
    label_idxs = np.where(net[&amp;#39;attributes&amp;#39;][:, label_i] == has_label)[0]
    label_idxs = np.random.permutation(label_idxs)[:n_imgs]
    imgs = [plt.imread(files[img_i])[..., :3]
            for img_i in label_idxs]
    preprocessed = np.array([CV.preprocess(img_i) for img_i in imgs])
    zs = sess.run(Z, feed_dict={X: preprocessed})
    return np.mean(zs, 0)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we use the code to create an interpolation between "Male" and "Not Male" (Female) images. Because we are only using the two endpoints, we get two images: a 100% Man and 100% Woman (please note that we can also get anything in between by doing a weighed average of the two seeding vectors).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def gan_generate_data(num_iter=20000,imgs=15):
    # generates 2*(number of iter) images 
    # adding random number of pictures for each synthesis (to increase variation)
    # returns list of [Male, Female] * num_iter images
    generated_images = []

    for i in range(num_iter):

        n_imgs = random.choice(range(imgs-10, imgs+10))

        z1 = get_features_for(&amp;#39;Male&amp;#39;, True, n_imgs=n_imgs)
        z2 = get_features_for(&amp;#39;Male&amp;#39;, False, n_imgs=n_imgs)

        notmale_vector = z2 - z1
        amt = np.linspace(0, 1, 2)
        zs = np.array([z1 + notmale_vector*amt_i for amt_i in amt])
        g = sess.run(G, feed_dict={Z: zs})

        generated_images.append(g[0])
        generated_images.append(g[1])

        if i%1000==0:
            print(&amp;#39;Iteration number : {}&amp;#39;.format(i))

    return generated_images

generated_data = gan_generate_data()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Okay good, we have the data to play around with and it's saved in a pickle file so we don't have to re-create it. Now, let's just add one hot encoded labels (we have done this in predictable manner -- i.e. male (0) is always first). We can just sense-check it and get the shape of the overall sample.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;labels = [0,1] * 20000
generated_data = np.array(generated_data)
generated_data.shape
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a name="extensions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;Now let's get to the transfer learning part. First we have to get out network, &lt;code&gt;vgg16&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;libs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;vgg16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2v&lt;/span&gt;

&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vgg16&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_vgg_face_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Transfer Learning&lt;/h2&gt;
&lt;p&gt;Here we get the &lt;code&gt;vgg16&lt;/code&gt; network, which we have loaded up earlier and use it to generate the predictions for one of its own pre-trained classes. However, since we want to predict a different task, we then use the &lt;code&gt;transferred_predictions&lt;/code&gt; function to get the predictions for the 2623 different classes and then use that as an input to the next classifier to train it on recognizing gender. &lt;/p&gt;
&lt;p&gt;In order to do this effectively we must first do some image processing, which we do in &lt;code&gt;transferred_df&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transferred_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;gets&lt;/span&gt; &lt;span class="s s-Atom"&gt;an&lt;/span&gt; &lt;span class="nf"&gt;image&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;array&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;as&lt;/span&gt; &lt;span class="s s-Atom"&gt;an&lt;/span&gt; &lt;span class="s s-Atom"&gt;input&lt;/span&gt; &lt;span class="s s-Atom"&gt;outputs&lt;/span&gt; &lt;span class="s s-Atom"&gt;net&amp;#39;s final layer predictions &lt;/span&gt;
&lt;span class="s s-Atom"&gt;    results = []&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    # Grab the tensor defining the input to the network&lt;/span&gt;
&lt;span class="s s-Atom"&gt;    x = g.get_tensor_by_name(names[0] + &amp;quot;:0&amp;quot;)&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    # And grab the tensor defining the softmax layer of the network&lt;/span&gt;
&lt;span class="s s-Atom"&gt;    softmax = g.get_tensor_by_name(names[-2] + &amp;quot;:0&amp;quot;)&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    with tf.Session(graph=g) as sess, g.device(&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nn"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="sc"&gt;0&amp;#39;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="nv"&gt;Remember&lt;/span&gt; &lt;span class="s s-Atom"&gt;from&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;lecture&lt;/span&gt; &lt;span class="s s-Atom"&gt;that&lt;/span&gt; &lt;span class="s s-Atom"&gt;we&lt;/span&gt; &lt;span class="s s-Atom"&gt;have&lt;/span&gt; &lt;span class="s s-Atom"&gt;to&lt;/span&gt; &lt;span class="s s-Atom"&gt;set&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;dropout&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;keep probability&amp;quot;&lt;/span&gt; &lt;span class="s s-Atom"&gt;to&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;feed_dict=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nn"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;Not&lt;/span&gt; &lt;span class="s s-Atom"&gt;using&lt;/span&gt; &lt;span class="s s-Atom"&gt;droput&lt;/span&gt; &lt;span class="s s-Atom"&gt;here&lt;/span&gt;
                    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;&amp;#39;net/dropout_1/random_uniform:0&amp;#39;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;&amp;#39;net/dropout/random_uniform:0&amp;#39;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;test_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s s-Atom"&gt;::-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nf"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s s-Atom"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; 
                &lt;span class="s s-Atom"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
               &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;idx&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="s s-Atom"&gt;test_array&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s s-Atom"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;columns=&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;

&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;does&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;preprocessing&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;list&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt; &lt;span class="s s-Atom"&gt;and&lt;/span&gt; &lt;span class="s s-Atom"&gt;outputs&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;list&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;predictions&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;i&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;imresize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;size=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;preprocess&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;transferred_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;if&lt;/span&gt; &lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="c1"&gt;%1000==0:&lt;/span&gt;
            &lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Current image id {}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;


&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parallel_transfer_eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;returns&lt;/span&gt; &lt;span class="s s-Atom"&gt;parallely&lt;/span&gt; &lt;span class="s s-Atom"&gt;executed&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;using&lt;/span&gt; &lt;span class="s s-Atom"&gt;first&lt;/span&gt; &lt;span class="nf"&gt;split&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nf"&gt;second&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;and&lt;/span&gt; &lt;span class="nf"&gt;third&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;as&lt;/span&gt; &lt;span class="s s-Atom"&gt;divisors&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;multiprocessing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;ss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;:fs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s s-Atom"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s s-Atom"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;target&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nf"&gt;delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Leveraging transfer learning&lt;/h2&gt;
&lt;p&gt;Now we use the predictions made by &lt;code&gt;vgg16&lt;/code&gt; in a typical &lt;a href="http://cs231n.github.io/transfer-learning/"&gt;Transfer Learning&lt;/a&gt; paradigm. Here we just take the last layer of predictions, reshape the features and feed it to a next layer classifier (sometimes also done by removing the last (few) Fully Connected Layers) and putting training the whole network. Here we just create a new one just on the last layer. The practice supports both approaches. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="c1"&gt;# train-test for proper evaluation&lt;/span&gt;
&lt;span class="n"&gt;train_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_cores&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gpu_memory_fraction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# set up the network&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;categorical_crossentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_set&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LabelEncoder&lt;/span&gt;

&lt;span class="c1"&gt;# reshape labels so that they match what the network expects&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LabelEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np_utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

&lt;span class="n"&gt;test_imgs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preprocess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;test_imgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we're done with this bit as we have scores for both generated and hand-labelled images (test)! This only is the first step, however, in our journey, as now we have to transfer the &lt;code&gt;vgg16&lt;/code&gt; generated scores onto the new classifier (the last bit in transfer learning, which is typically simplified by cutting off the last layer and just re-running the network with a new final layer, but here done explicitly for training purposes.)&lt;/p&gt;
&lt;h2&gt;Training and evaluating a new classifier&lt;/h2&gt;
&lt;p&gt;For simplicity, we will just use the &lt;code&gt;tflearn&lt;/code&gt; classifier so that we have an easier job using transfer learning given the complexity of all the previous work:
1. we train (based on the synthetic data and the therefore completely predictable labels)
2. we evalute on the handlablled examples (by me) &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;absolute_import&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;


&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;

&lt;span class="n"&gt;feature_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real_valued_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dimension&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;


&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;gradient_clip_norm&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                                            &lt;span class="c1"&gt;# model_dir=&amp;#39;./model&amp;#39;)&lt;/span&gt;

&lt;span class="c1"&gt;# Fit model.&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Evaluate accuracy.&lt;/span&gt;
&lt;span class="n"&gt;test_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# test_array = np.array([ [res[0] for res in result] for result in test_array ])&lt;/span&gt;

&lt;span class="n"&gt;accuracy_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_labels&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;accuracy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Accuracy: {0:f}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;General discussion&lt;/h1&gt;
&lt;p&gt;The results were not that stellar, however, I think this is a fascinating research area and quite likely it is going to be one of the biggest areas for the future of AI: but we still got better than random (consistently) and might get better if I spent more time on this.&lt;/p&gt;
&lt;p&gt;Moreover this code can probably fine-tuned and re-used with only minor modifications in many industry applications:
(a) 3D object generation
(b) &lt;a href="https://www.youtube.com/watch?v=u7kQ5lNfUfg"&gt;Pix2Pix applications&lt;/a&gt; that manages to create new images based on style or just a generation of maps from satelite images. The possibilities here are &lt;em&gt;literally endless&lt;/em&gt;.
(c) Remastering Old Movies.
Just to name a few.&lt;/p&gt;
&lt;p&gt;Thank you for reading and if any of this was of interest, explore this website for more!&lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="semi-supervised learning"></category><category term="GANs"></category><category term="Generative"></category><category term="Adverserial"></category><category term="Neural Networks"></category><category term="code"></category></entry><entry><title>GANs &amp; Semi-Supervised Learning</title><link href="jakublangr.com/gans-tutorial.html" rel="alternate"></link><published>2017-04-08T21:00:00+01:00</published><updated>2017-04-08T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-04-08:jakublangr.com/gans-tutorial.html</id><summary type="html">&lt;p&gt;I know what you're thinking: can there be anything more exciting? Rest assured, the wild party don't stop there. So hold on to your glasses nerds, we're going to do some A-friggin-I. Or if you're into ML and think this minus the sarcasm. But actually, this is super cool.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="http://jakublangr.com/ai-2016-review"&gt;last blog post&lt;/a&gt; we looked what are some of the promising areas in AI and one of the areas that was mentioned many, many times by researchers and my friends as likely future directions of AI, was Generative Adverserial Learning/Networks (GANs). The business appeal is clear: GANs can train from less data, can create fascinating applications (such as 3D model generation) and has lots of future research potential. For more on the practical applications &lt;a href="http://jakublangr.com/ai-2016-review"&gt;go back to that post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post is intended to be somewhat technical and will feature some high-level pseudo-code, but I will try to make it as accessible and hopefully not too boring.&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img src='https://cdn.meme.am/instances/500x/76522645/get-in-loser-get-in-loser-were-going-to-do-some-ai.jpg' style="width: 40%" align="center"&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
&lt;div align="center"&gt; 
&lt;strong&gt; So you want to get on the hype train? &lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Note that all of this is quite cutting edge and some of the things showcased here were only invented and published in academic journals about a year ago, so this is something that unless you did a post-Doc in some discipline can feel a bit unusual (it did for me) to read things this new. But it also means that there's lots of unmapped theory around this as well as super strange bugs that you have to deal with. But because I &lt;a href="https://www.kadenze.com/certificates/verified/8TKKM10L"&gt;recently finished&lt;/a&gt; (I just really like taking MOOCs, okay?) the amazing course on &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv"&gt;Creative Applications of Deep Learning with TensorFlow&lt;/a&gt; by the one and the only &lt;a href="http://pkmital.com/home/"&gt;Parag Mital&lt;/a&gt;, I decided to share some of what I have learned.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Semi-supervised_learning"&gt;Semi-supervised learning&lt;/a&gt; basically means using labelled (supervised) as well as unlabelled (unsupervised) examples during training and as a concept is quite old. The core idea makes a lot of sense: we have lots of data that in a typical supervised setting lies unused. For example think linear regression on a house price (label) data. We all understand how &lt;a href="https://en.wikipedia.org/wiki/Linear_Regression"&gt;linear regression&lt;/a&gt; can generate &lt;a href="https://www.coursera.org/learn/ml-foundations/lecture/2HrHv/learning-a-simple-regression-model-to-predict-house-prices-from-house-size"&gt;house prices&lt;/a&gt;, but most houses are not sold, but perhaps we can get the data about them anyways, perhaps from the city planning. This data can give us much better picture, for example about how do different areas compare to each other, where is there a relative shortage of houses and where the biggest houses tend to be. It would be foolish then not to use this data in some way, but traditional algorithms do not allow it.&lt;/p&gt;
&lt;p&gt;So Semi-Supervised Learning (SSL) then means using different techniques to somehow add this data to the training of the machine learning (ML) model. But even this is not trivial: if you think of training ML as creating a decision tree and then you can check how good your decision tree was by checking if it got to the correct answer. Unfortunately with the unlabelled data, there's no answer (because the house was not sold during the time the data was gathered), so no learning happens, because the ML algorithm cannot attach correct answer (and therefore loss) to it. I want to focus on one of the techniques in SSL called Generative Adverserial Networks (GANs), which if you read my blog, understand why there's a lot of promise. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"&gt;GANs work&lt;/a&gt; by first having one network create an internal vision of the world (i.e. what do houses look like &lt;em&gt;in general&lt;/em&gt;): this is the generative model (G) and basically learns from all the data, because it does not need labels, just all of the features of a typical house in the dataset. The second network, the 'Discriminator' (D), which is the adversary in this case, takes in the examples both from the real dataset and the examples of houses generated by the generator and decides whether this data looks real, the generator has done a good job and gets a smaller loss or...&lt;/p&gt;
&lt;div align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/qc7jblpaPdM" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br &gt;&lt;/p&gt;
&lt;p&gt;In other words, imagine now we are trying to label cats or dogs, in this case G will learn how to generate images at first and subsequently get better at making the images more like cats or dogs. &lt;/p&gt;
&lt;p&gt;Then we put the G and D to basically compete against each other to produce the best results: hopefully every time G gets better D has to get better to match (though we have to make sure one of them is not too much better than the other). This was one of the core driving principles behind &lt;a href="https://en.wikipedia.org/wiki/AlphaGo#Algorithm"&gt;AlphaGo&lt;/a&gt; as well. Basically, we get the G to produce images and D to critique them. So G would send an array of images to D and D would output 0 or 1--real or fake back to G. G would then try to come up with better examples based on which fooled D and vice versa. Schematically, it may look something like this:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src='http://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png' style='width: 60%'&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;Credit KDNuggets.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So hopefully it is now clear that we can take lots of unlabelled data, construct a generator and make it learn some of the structure of the data (i.e. what does a typical example look like) then make it compete in making the data that it generates as close to the real data. After this process, we may end up with some pretty decent looking synthetic data and pretty much close to unlimited amount of it. I've skipped lots of caveats, but I will say this: the generator will only be able to generate things alike what it has seen in the data before. Even though it may be easy to forget, this is not magic.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src='https://i.imgflip.com/1mzsbp.jpg'&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;(Those who actually speak Latin forgive me.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the highest levels of abstraction this may look something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# get data
real_data = pd.read(&amp;#39;real_data.csv&amp;#39;) # shape (n_examples, n_features+label) 
unlabelled_data = pd.read(&amp;#39;unlabelled_data.csv&amp;#39;) # shape (n_examples, n_features)

# construct the two objects
generator = GeneratorClass()
discriminator = DiscriminatorClass()

# pre-train generator
generator.train(unlabelled_data)
# get synthetic data 
synthetic_data = make_compete(generator, discriminator, real_data) 
# shape (any_number, n_features)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;div align="center"&gt;
&lt;strong&gt; (Machine-Learning) WAR! What is it good for? &lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Okay so a keen reader might observe that we have not described a method for labeling the generated examples. Ideally, what we would like to have is a way of generating examples (e.g. houses with the price attached or pictures of objects with the object description attached). Thankfully for lots of instances, there is a way. If you go back to the diagram, you can see that there's a mention of something called 'the latent space'. Latent space is a way to control what kind of images get generated. If we trained the generator on cats and dogs, one of the dimensions will control how 'catty' or 'doggy' the image will be. It also allows for any interpolation between the two, so you can have a dog-cat or 70% cat, 30% dog. In other words, latent space can be thought of as some seeding factor--you give some initial input to G just so that it does not always generate the same thing, but it turns out this seeding factor has consistent latent ('hidden') properties that certain dimensions can be assigned meaning. &lt;/p&gt;
&lt;p&gt;So we can easily modify the pseudocode from above to make this clearer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# shape (any_number, n_features)
synthetic_cats = make_compete(generator, discriminator, 
                    real_data, input_noise=latent_feature_space.cat) 
# shape (any_number, n_features)
synthetic_dogs = make_compete(generator, discriminator, 
                    real_data, input_noise=latent_feature_space.dog) 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The amazing thing about this is that in theory, we don't even need to have the data labelled for that task to generate those examples (though it would help a lot). So we could have labelled training data for whether something is a being a good or a bad boy (for both cats and dogs) and we can train the G to create new examples of cats or dogs (based on one parameter of the latent space), both good or bad (based on another parameter of the latent space). Let's say that good or badness of a dog is something we can see from the picture (e.g. it is a bad boy every time it is destroying property, good otherwise). We can then discover parameters in the latent space for both of these features and generate even examples of cat-dog or dog-cat by interpolating between these two values.&lt;/p&gt;
&lt;p&gt;Another example is that we can download loads of unlabelled data of faces of celebrities and make the G generate faces and manipulate the latent space so that we get clear examples of male or female and then use it to train another classifier to detect male or female images (with no labelled data of any kind!), which is exactly what I have done. One question that might be still going through your head is 'how do we get this latent space representations of these different attributes?': that, however, is probably beyond the scope of this article unfortunately.&lt;/p&gt;
&lt;p&gt;Phew, look at the time: I was hoping to present some actual code so that people can try this on their own, but unfortunately, I felt that this blog post is already long enough so I will leave the code till next time. If there's interest or you would love to see something urgently, drop me a line.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In case you are interested in the actual code, check out the &lt;a href="http://jakublangr.com/gans-code.html"&gt;second part of this tutorial&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="semi-supervised learning"></category><category term="GANs"></category><category term="Generative"></category><category term="Adverserial"></category><category term="Neural Networks"></category></entry><entry><title>Artificially Intelligent Business</title><link href="jakublangr.com/ai-2016-review.html" rel="alternate"></link><published>2017-03-14T22:00:00+00:00</published><updated>2017-03-14T22:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-03-14:jakublangr.com/ai-2016-review.html</id><summary type="html">&lt;p&gt;AI is moving forward at an amazing rate but we should take the time to appreciate all the amazing potential of the recent advancements. I want to take the time to get some technical appreciation for the business implications the recent AI may have.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lots of people are arguing about the theoretical nature of what AI means many years down the road, but I would like to focus on the AI commercial products in the next 2-5 years. I'd also bring all the incredible, but deeply technical advances back to the real world and business and focus on and focus on what businesses and consumers can practically expect of the future. First off I will talk about the most interesting advances in AI, second what the current applications of those advances are and wrap it up with what this means for business.&lt;/p&gt;
&lt;p&gt;At the same time, this is &lt;strong&gt;not&lt;/strong&gt; intended to be a discussion of the business theory behind AI-startups. This has been covered by more qualified people, for &lt;a href="http://venturebeat.com/2017/03/13/10000-ai-startups-need-to-learn-these-lessons/" target="_blank"&gt;example here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 1. Breakthroughs in AI &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last couple of months were excellent for AI: everything from &lt;a href="https://en.wikipedia.org/wiki/One-shot_learning" target="_blank"&gt;One-Shot-Learning&lt;/a&gt;, advances in Generative Adversarial Networks (GANs) and Memory-Augmented Networks to practical applications such as AlphaGo, &lt;a href="http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players" target="_blank"&gt;Liberatus&lt;/a&gt; winning the world Poker championship and a neural net creating images that match textual input. &lt;/p&gt;
&lt;p&gt;There have been way too many practical applications to list, but all of these are a results of more theoretical work from prior years. Therefore, in order to look at what future is likely to hold, we should look at the theoretical advancements and extrapolate what kind of practical tasks these novel techniques can achieve in production. One such area is &lt;a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank"&gt;reasoning AI&lt;/a&gt;: memory augmented and differentiable networks are likely to bring in a new era of higher level reasoning: as recent &lt;a href="https://www.youtube.com/watch?v=eyovmAtoUx0&amp;amp;t=4s" target="_blank"&gt;Stanford papers&lt;/a&gt; and &lt;a href="https://code.facebook.com/posts/457605107772545/under-the-hood-building-accessibility-tools-for-the-visually-impaired-on-facebook/" target="_blank"&gt;recent Facebook tools to help visually impaired understand content of images&lt;/a&gt; demonstrate. So reasoning and Q&amp;amp;A AI is just around the corner.
In terms of future theoretical breakthroughs, the most fruitful areas are likely to be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further improvements on GANs, as they already are being tested in the field of &lt;a href="https://www.youtube.com/watch?v=kf-KViOuktc" target="_blank"&gt;3D object generation&lt;/a&gt;, &lt;a href="https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503" target="_blank"&gt;remastering old movies&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=WovbLx8C0yA"&gt;image enhancement&lt;/a&gt;. Even one of the fathers of deep learning, &lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/" target="_blank"&gt;Yann LeCunn, called it&lt;/a&gt; “the most important [recent advancement in deep learning]”.&lt;/p&gt;
&lt;p&gt;Further improvements on memory/attention as demonstrated for instance by a &lt;a href="https://deepmind.com/blog/differentiable-neural-computers"&gt;recent study by DeepMind published in Nature&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--It does not really matter what the names of these advances are, though if you are interested, feel free to read more about them.
--&gt;

&lt;p&gt;&lt;strong&gt; 2. Summary of state of the art, future improvements and likely future paths &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The biggest breakthrough, &lt;a href="(http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/ ){:target=&amp;quot;_blank&amp;quot;}, I think will come from Generative Adversarial Networks (GANs), so I would like to focus on them as they are especially impressive in the case of [Adversarial and Semi-Supervised Learning](https://ishmaelbelghazi.github.io/ALI){:target=&amp;quot;_blank&amp;quot;}. In essence, this approach tries to get around the problem of not having enough labelled data. This is a problem that everyone maybe except the likes of Google faces. Hence this breakthrough can kick-start a new wave of AI companies in any sufficiently technically savvy ecosystem, such as London. The results are already extremely promising based on research from one group in Montreal (I highly recommend reading [this link](https://ishmaelbelghazi.github.io/ALI){:target=&amp;quot;_blank&amp;quot;}.) as well as OpenAI"&gt;in line with LeCunn&lt;/a&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“This approach [GANs for semi-supervised learning] allows us to obtain state of the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network — a result that’s very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is very promising because labeled examples can be quite expensive to obtain in practice.” &lt;a href="https://openai.com/blog/generative-models/" target="_blank"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To summarize: this means that people can get performance as good as the best networks in the field trained with the full dataset. &lt;/p&gt;
&lt;p&gt;I foresee that the next improvements will apply similar techniques to more complex datasets and using fewer and fewer labelled examples to &lt;a href="http://rodrigob.github.io/are_we_there_yet/build" target="_blank"&gt;match the state-of-the art performance&lt;/a&gt;. This can probably be realized by not only using transfer learning as the “training agent” more effectively (or using &lt;a href="https://en.wikipedia.org/wiki/Multi-task_learning" target="_blank"&gt;multi-task learning&lt;/a&gt;), but also by using advanced GANs themselves, as hinted to above. Or maybe even to move closer to optimal &lt;a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank"&gt;Active Learning&lt;/a&gt; by generating exactly the hardest (real) examples for the network to classify at that point in time to improve the fastest. (This does &lt;strong&gt;not&lt;/strong&gt; depend on advances in other areas, such as &lt;a href="https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients" target="_blank"&gt;Synthetic Gradients&lt;/a&gt;, better optimizers etc.)&lt;/p&gt;
&lt;p&gt;In other words, now we are advancing, in a sense, in the "theory of knowledge" of Machine Learning algorithms so that they can learn faster. Akin to a child in middle-school, deep learning is now discovering "learning aids" or "hacks" to learn even faster and generalize better. This means that we can move AI closer to production and usefulness. This means that plugging into a new (business) vertical will be easier than ever. You will still need to develop vertical-specific expertise and architecture, but technology like generic object detection, &lt;a href="https://www.technologyreview.com/s/603811/baidus-artificial-intelligence-lab-unveils-synthetic-speech-system/" target="_blank"&gt;speech synthesis&lt;/a&gt; or sentiment analysis will be commoditized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 3. Commodtizied ML is here, so what? &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These advancements along with better tools for AI researchers and production, will favor companies who have (i) unique business partnerships or (ii) overcome some unique regulatory hurdle. Huge opportunities are still in &lt;a href="https://www.bloomberg.com/news/articles/2017-02-28/jpmorgan-marshals-an-army-of-developers-to-automate-high-finance" target="_blank"&gt;FinTech&lt;/a&gt; and other regulation-heavy industries.&lt;/p&gt;
&lt;p&gt;However, the key point is to realize that the AI of the future will likely just serve as a preprocessing or feature engineering step feeding into the next, traditional data science, layer of algorithms. But make no mistake: this is huge. Computers, for the first time in history, will have the ability to see and create their own representations of objects. To take a practical example: I have recently &lt;a href="http://joinef.com" target="_blank"&gt;been through an accelerator&lt;/a&gt; where &lt;a href="http://observe.tech/" target="_blank"&gt;one of the companies, Observe, is working on automating fish farming&lt;/a&gt;. They have a standard object recognition algorithm that passes on its output to further systems managing feeding of fish etc., so that they maximize the fish stock and minimize waste. Their key selling proposition in this case is the second, fish feeding, algorithm, but until couple of years ago there was no easy way to explain the images of fish to the second (control) algorithm. &lt;/p&gt;
&lt;p&gt;To take a traditional computer science analogy, we can think of the case of &lt;a href="https://en.wikipedia.org/wiki/Chinese_room" target="_blank"&gt;John Searle's Chinese room experiment&lt;/a&gt;: so far the computer was just manipulating symbols with no conception of what those symbols mean. I.e. the computer could not be thought of as "understanding" something or being "conscious". I do not want to delve into the philosophical implications of this statement, but the core dilemma is: because the computer has no context, it cannot be thought of as operating on anything else than a (computationally) straightforward set of instructions (no matter how complex in human terms). Now, computers are getting more context, so, in an odd sense, we are closer to understanding. &lt;/p&gt;
&lt;p&gt;&lt;img src='https://upload.wikimedia.org/wikipedia/commons/d/de/ChineseRoom2009_CRset.jpg' style='width: 90%'&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From Wikipedia under CC license.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Keeping in line just with the practical sense, computers will now be able to auto-extract features. In fact, &lt;a href="http://eyn.vision/" target="_blank"&gt;for instance Eyn's&lt;/a&gt; core tech is already a facial detection algorithm fed into spoofing detection. This is only possible because facial detection is so close to perfect we can rely on it almost as deterministic. Andrew Ng talked about the importance of moving from 95% to 99%, I think there is another, perhaps even more important milestone in moving from 99 to 99.9% where we can use commoditized machine learning almost as a framework for our core tech.&lt;/p&gt;
&lt;p&gt;This is especially important in parts of the world where databases about the real world are not as common as in the EU. Because algorithms can now integrate into physical space and get data from it directly, we can build directly onto the physical space: i.e. we can provide value to all of the ~6.1 billion people currently underserved by technology and apply technology directly to areas they find most useful, like agriculture: &lt;a href="http://optimal.ag/" target="_blank"&gt;Optimal Labs&lt;/a&gt; is already doing that. Moreover, with all this computer vision, it will be much easier to jump-start projects like city planning: in fact, this is already happening in India.&lt;/p&gt;
&lt;p&gt;The last thing I would like to talk about is a direct implication of the previous paragraph, but has far reaching consequences. For the first time, IT systems may make sense even in situations in which there is no other IT system to connect to or IT support. Algorithms now do not need supporting database--as they can create their own or rely on pre-trained models--so they can be rolled out to a completely new company without the usual enterprise sales &amp;amp; integration cycle. In theory, you can have an entire IT system managed by external companies as it is quite expensive to have AI talent or ML maintenance on site (e.g. in agriculture) beyond simple mechanics. Now, this is wildly speculative, but if true, this absence of in-house IT people means that external companies can deploy to completely novel areas (and there'll be more of those opportunities because in-house IT is not there to pitch obvious technology solutions) and capture much more of the market. But it also means that there are no longer IT people to guide and support you so the business contacts are now more important than ever. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Conclusion &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So from the recent advancements and meta-learning theory it is clear that we are in an age of vertical AI and that the future direction is likely to heavily rely on understanding the business use-cases of particular AI technology. This seems to be further accentuated by the fact that AI can now be deployed on its own without external support or databases to connect to. This means that the business side of AI will be more important than ever, but also the opportunities are becoming more plentiful, because even the very obvious usecases might not be covered by in-house teams.  &lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="reviews"></category><category term="StartUps"></category></entry><entry><title>Kaggle Struggle</title><link href="jakublangr.com/kaggle-2016.html" rel="alternate"></link><published>2016-05-03T21:00:00+01:00</published><updated>2016-05-03T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2016-05-03:jakublangr.com/kaggle-2016.html</id><summary type="html">&lt;p&gt;My trip to the &amp;lt;1% of data scientists. Or in the the world of competitive data science and back again. Aka: three (relative) successes and a fail. Also how not to spend a lot of money on your data problem. I need to start writing better summaries.&lt;/p&gt;</summary><content type="html">&lt;p&gt;For those of you that are not compete data nerds, I occasionally venture into the world of &lt;a href="http://www.kaggle.com"&gt;Kaggle&lt;/a&gt;: a competitive data science website where everyone from the top Data Science experts to the people who are just starting out compete against each other in the hopes of getting money (statistically unlikely), fame and massive amounts of fake nerd points (very important).&lt;/p&gt;
&lt;h1&gt;My life on Kaggle&lt;/h1&gt;
&lt;p&gt;&lt;div align="center"&gt;
&lt;img src="http://s3.amazonaws.com/theoatmeal-img/comics/working_home/6.png" alt='my life on Kaggle' style="width: 95%; height: auto "&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;I have so far participated in four different competitions: &lt;a href="https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction"&gt;Liberty Property Inspection Prediction&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management"&gt;BNP Paribas Claims Management&lt;/a&gt; and &lt;a href="https://www.kaggle.com/c/santander-customer-satisfaction"&gt;Santander Customer Satisfaction&lt;/a&gt;. All of these were fairly different competitions and I have learned loads of different things. &lt;/p&gt;
&lt;p&gt;In many cases, Kaggle is great: no more data archeology like in most commercial datasets. But this is not a rant about how when you bring an extra team on a project you should document the changes they made. Or about how when you start/stop sharding your database and you have variable names that indicate this, these should be updated as well. Or indeed that columns have swapped names or names that have nothing to do with what they contain for historical reasons. Not that I am bitter or anything.&lt;/p&gt;
&lt;h1&gt;Okay, okay... but I still don't get it.&lt;/h1&gt;
&lt;p&gt;So the idea behind Kaggle is that companies that have a data problem put their data (anonymised, well-structured and standardised) on the site provide some minimal description of the problem (e.g. we want to be able to predict which marketing campaign will be successful based on some historical data). &lt;/p&gt;
&lt;p&gt;Then about half a million data scientists form around the world can download this dataset and play around with it and even submit their predictions to be scored against the known (historical) truth. The best ones get all of the perks above, the others get nothing. Capish?&lt;/p&gt;
&lt;h1&gt;Why did I decide to join?&lt;/h1&gt;
&lt;p&gt;This website is first and foremost a great way to learn: you get to know what people much better than you are doing and learn from them: usually mostly after the competition is over. &lt;/p&gt;
&lt;p&gt;I also think that working on these kinds of competitions is great especially for benchmarking your own skills and generally what lift (percentage increase) can we expect by massaging the data a bit more: be it &lt;a href="https://en.wikipedia.org/wiki/Feature_engineering"&gt;feature engineering&lt;/a&gt; or by &lt;a href="http://mlwave.com/kaggle-ensembling-guide/"&gt;ensambling&lt;/a&gt;. So Kaggle is a great way to test out some of these things in practice and see how others use them to some extent. &lt;/p&gt;
&lt;p&gt;I think that Kaggle is a great way to connect with other data scientist and figure out how they do things and maybe even make friends along the way. Coincidentally, this is also a great way how to spend a lot of money (more on that later).&lt;/p&gt;
&lt;h1&gt;How did I do?&lt;/h1&gt;
&lt;p&gt;There is a lot to do Kaggle: you earn ranking within each competition: I earned the Top 25% badge by finishing in the top 23.6% (property inspection), 12.9% (marketing) and 20.9% (claims management). Once I missed it by a 31.3% finish (customer satisfaction). I think that overall I have learned loads during these competitions: people frequently share code and model descriptions of the best performing models (the best performers though usually hold out till the end of competition). (You thought there will be an article about data science without numbers? Ha.)&lt;/p&gt;
&lt;h1&gt;Dude, you have a serious data problem.&lt;/h1&gt;
&lt;p&gt;Hi, I'm Jakub and I am a data addict. I have spent two weeks off AWS Console... but in all seriousness, Amazon Web Services is both a gift and a curse to every techie. Gift because you can have data centres' worth of computational power at your fingertips within minutes. Curse because of what it is going to cost you:&lt;/p&gt;
&lt;table align="center"&gt; 
&lt;tr&gt;
    &lt;td&gt;
    &lt;img src='images/april_aws.png' alt="April" style="width: 95%; height: auto "align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
    &lt;td&gt;
    &lt;img src="images/may_aws.png" height="38%" alt='May' style="width: 95%; height: auto " align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1 style="text-align: center"&gt; &lt;b&gt; &lt;a href='https://youtu.be/CduA0TULnow?t=1m28s'&gt; Ooops. &lt;/a&gt;&lt;/b&gt; &lt;/h1&gt;

&lt;p&gt;I don't feel like I am using AWS excessively even. But perhaps it was all worth it: &lt;/p&gt;
&lt;div align="center"&gt;
&lt;a href="https://www.kaggle.com/jakublangr"&gt;
&lt;img src="images/kaggle_status.png" style="align-content: center; width: 65%; height: auto"&gt; &lt;br&gt; 
&lt;/a&gt; Top .929%, aww yeah! 
&lt;/div&gt;

&lt;h1&gt;What to make of it&lt;/h1&gt;
&lt;p&gt;But the model of competitive data science is greatly interesting to many companies: not only does Kaggle have a track record of blowing the existing industry experts out of the water with their accuracy. So it is clearly doing well in other respects: like supporting the community and spreading information. There's loads of free public datasets to explore and easily share your findings. Also, the spread of efficient techniques and algorithms is thanks to Kaggle greatly accelerated: for instance, random forest (probably one of the top algorithm) was written in 2001 and rose to prominence around 2009, while eXtreme Gradient Boosting (XGB) was written in 2014 and blew other algorithms away by late 2015. So massive acceleration. Even I personally have to say that the community is amazing and there's always loads to learn.&lt;/p&gt;
&lt;p&gt;So that's it from now. As always, if you have a comment/question, I'd love to hear from you!&lt;/p&gt;</content><category term="datascience ML technical"></category></entry><entry><title>On MOOCs: Projects, Practice and Perspective</title><link href="jakublangr.com/moocs-part1.html" rel="alternate"></link><published>2015-01-23T21:00:00+00:00</published><updated>2015-01-23T21:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2015-01-23:jakublangr.com/moocs-part1.html</id><summary type="html">&lt;p&gt;I said I will stop at finishing 14 online courses... and I am doing three MOOCs again. Review of my learning journey, projects I have done, reviews of great and good courses, my (brief) take on practices. &lt;strong&gt;UPDATED&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been quite a while since I started my first MOOC at Coursera. I think now is the time to reflect on the courses I have finished, what I have learned as well as what to recommend to my fellow MOOCers. I will try to broadly classify (and score, because if you are anything like me, you occasionally dislike the imprecision of natural language) the courses I have finished and then I will try to give my take on MOOCs in general.&lt;/p&gt;
&lt;h1&gt;1. Awesome Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/sna"&gt;Social Network Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course by &lt;a href="http://www.ladamic.com/"&gt;Lada Adamic&lt;/a&gt; was definitely one of the most awesome courses regardless of platform, country or institution that I have been involved in. Not only was Lada willing to have Google Hangout sessions with the course participants, but the course was excellently done: we have worked with &lt;a href="http://gephi.github.io/"&gt;Gephi&lt;/a&gt;, programmed in &lt;a href="https://ccl.northwestern.edu/netlogo/"&gt;NetLogo&lt;/a&gt; and used the &lt;a href="http://cran.r-project.org/web/packages/sna/sna.pdf"&gt;R SNA package&lt;/a&gt;(or at least I did in my projects). The course was a great balance of (social) network analysis, problem sets and optional programming assignments. You can view mine &lt;a href="Public/US_Contributions.pdf"&gt;here&lt;/a&gt; (though bear in mind, I have done almost 2 years ago so it is not something I am particularly proud of). &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; R, Gephi, NetLogo, different metrics characterizing a graph&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/dataanalysis"&gt;Data Analysis&lt;/a&gt; and &lt;a href="https://www.coursera.org/course/compdata"&gt;Computing for Data Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long before Coursera introduced the 'Data Science specialization' (that in my opinion seems way too superficial), Coursera was offering these two amazing Courses that together were actually undiluted first year graduate bioinformatics courses for students at Johns Hopkins University. The problem with a lot of the later MOOCs is that they were starting to get too simple to actually teach me anything and so the pool of people who was actually taking these courses was a lot worse. I think the latter is also an issue, because half of the learning experience is the community. Like the people putting interesting thoughts on the forums or the moderators (or in this case, the community itself) trying to prevent cheating (mostly unintentional, I hope, why else would you do a MOOC?) and so on. I have been taking this courses with statistics PhDs and other people with amazing insights and experience and I think that is probably why the learning experience was so amazing and all the four projects including &lt;a href="Public/blog/Cellphone_data_Prediction.pdf"&gt;the final one&lt;/a&gt; were something that gave me a solid grounding for the Social Network Analysis MOOC to use R as well as for my &lt;a href="http://goo.gl/sEUFMa"&gt;internship in the summer of 2013&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 9/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Through exploration of R basics, statistics, basic machine learning, data preparation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-0#.VMPxF4ptN5Q"&gt;Science of Uncertainty, Introduction to Probability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was amazing, but also &lt;strong&gt;very&lt;/strong&gt; hard. This is the undiluted MIT 6.041 course. The commitment from the teaching staff of MIT (including the professor &lt;a href="http://www.mit.edu/~jnt/home.html"&gt;John Tsitsiklis&lt;/a&gt;) was incredible. Generally, I got a great answer for any of my questions in a shorter period of time then most of my professors at my home institution. The treatment was very rigorous that keeping up with the Oxford workload while doing this 12 week course proved quite difficult. But it was so much more rewarding to see the certificate of completion afterwards.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Probability theory, Statistics, Inference &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/ml"&gt;Machine Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ah yes, this is where it all started. I had to, of course, eventually finish this course. Andrew Ng (my personal hero) created this course back in April 2012 and I believe it might be now in its 10th iteration. This course will give you a taster of Support Vector Machines, Recommender Systems, Artificial Neural Networks (basis for &lt;a href="http://en.wikipedia.org/wiki/Deep_learning"&gt;Deep Learning&lt;/a&gt;) and many others. The awesome thing about this course was that it abstracts &lt;em&gt;everything else&lt;/em&gt; apart from the ML algorithms to a package that you download so all you need to write is the machine learning algorithm. This means that this course is a mere introduction to machine learning and this alone cannot be considered to give you enough knowledge to actually go out and program some of them in production. This course was just so much fun, because of all the cool stuff that you write that I still have to give it 10/10. Plus, in the right data science tradition, this course has been continuously improved for each iteration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, MATLAB, Statistics&lt;/p&gt;
&lt;h1&gt;2. Great Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-0"&gt;Computational Thinking and Data Science&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was a great application of Object-Oriented Programming and simulations. I have to say I have gotten so much support on the forums it was incredible. (I do not recall, however, whether this was mostly staff or other students. Either way, great answers.) I think that the course, however, was too short and might have been more rigorous. (Though I do understand this was only a half an MIT course and probably simplified.) But I just admire professor Guttag as a person for being one of the first to get on board with MOOCs. This course was also a very different take on data science from the usual approach, as here you focus on simulations--e.g. behavior of different virus populations based on applications of different drugs/medicines and the genetically inherited (hence the OOP design) resistance to different active components of these drugs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; OOP, Simulations, Stochastic Algorithm Testing&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/cs215"&gt;Udacity Algorithms&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I finished this course before Udacity changed their business model from offering free courses with no deadlines to one where they only offer paid courses (though the material is still free, but you cannot get a certificate). This course was quite hard and the support was sparse (for a MOOC). But this meant that I just had to spend a bit more time thinking about the problem, which was probably helpful in the end to my general CS skills. I think that all the concepts were quite well explained and overall I think that there was sufficient practice to really understand the material. Though there was not so much of a 'course feel', which there was in both Coursera and EdX. But despite this, they recorded &lt;a href="https://www.youtube.com/watch?v=stdG3BGmhqo"&gt;probably the most awesome CS theory song in existence&lt;/a&gt;, so I cannot really question anything about this course. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Algorithms, Data Structures, Social Network Computation &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/ud032"&gt;Udacity Data Wrangling with MongoDB&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course is a bit different from all the others: you can either take it for free (but no certificate at the end) or you can pay $200 for each month and have a personal coach, human-verified project at the end and get that certificate. Partially, because I wanted to evaluate Udacity's new business model and partially because I knew that data wrangling is a huge part of data science, I finished this course in about month and a half. Surprisingly there is very little about MongoDB -- it is only introduced in the last 20% of the course, but that does not mean that the course does not teach you helpful skills. It does! Web scraping and XML parsing are definitely useful skills and I think that the course is appropriate in difficulty, though I think that it could be a bit harder at times to really force the students to organize their code well and debug more complex data cleaning parsers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned&lt;/strong&gt;: Website Scraping, Data Wrangling with Python and MongoDB&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/networksonline"&gt;Social and Economic Networks: Models and Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Normally economists (especially the really famous ones) are not too keen to join MOOCs, but that just makes &lt;a href="https://web.stanford.edu/~jacksonm"&gt;Matthew Jackson&lt;/a&gt; that much more awesome. This class was fun and allegedly a graduate level, though it must have been very diluted, because I found the material (although going to greater depth than the Michigan Course) quite easy. There was again some work with Gephi and this time even Pajek, though sadly no R. But I should note that there were more advanced versions of the course that one could participate in (sadly, with no recognition) that I wanted to participate in, but unfortunately I did not have the time. So I think potentially this could have been an 8/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; More computational and mathematical methods for social network analysis&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://online.stanford.edu/course/statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course covers an explanation of most of a recent book by Trevor Hastie, Robert Tibshirani and others: &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning, with Applications in R&lt;/a&gt; and some of their more advanced book, &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;. This course is fun to take, though I mentioned 'explanation' for a reason. This course will provide you with a thorough explanation of fairly advanced statistical concepts, but definitely will not test you rigorously enough. Usually, especially if you buy the discounted versions of one of the books, you will get enough knowledge and training to apply some of these concepts sensibly. But you will still need to work hard to test yourself and to really probe the ideas with no assistance. Regardless of whether you do, the lectures are really fun to listen to and explain concepts very well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, Intermediate Statistics, Computation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/initprogjava"&gt;Introduction à la programmation orientée objet (en Java)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I think I pretty much took this course, because programming in English is too mainstream. But it was really nice to learn a language from 0 in a very organized environment. What I genuinely appreciate is the walk-throughs on how to set up the Java environment in Windows, Ubuntu and Mac. But what I think was by far the best thing about this course was that the actual instructors were &lt;em&gt;so commonly&lt;/em&gt; involved on the forums, helping people. Even I once received some help from one of the instructors, which was great. Coding-wise, the MOOC was quite simple, but I assume approximately on the same level every other intro course for non-CS majors would be. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Java, French, Basic Data Structures&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/bigdata"&gt;IIT's Web Intelligence and Big Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was quite fun and relatively easy to grasp. The concepts revolved around the mathematics (and some programming) of the Big Data technologies, as well as their (recent) history. The problem sets were well done, though sometimes seemed bit too shallow and I would appreciate a bit more programming. The programming assignments were great: tested a whole array of important skills and technologies, but were only three through the entire course. This course was allegedly supposed to be taken as a half-course by actual IIT engineers. I, however, seem quite skeptical of that, as it seemed a bit too easy if true. But generally quite fun and insightful. Though the MOOC seemed to be a bit mass produced with little involvement from the IIT staff after a while.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Map-Reduce and other "big data algorithms" (locality sensitive hashing, page rank etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/introfinance"&gt;Introduction to Finance&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was my first MOOC I finished so I still remember quite a lot the odd details, as well as some of the catch-phrases the professor was using ("It's not about finance, it's about love.") as well as the fact that people (probably still stuck in the school mentality), were trying to ease their way through the course. I actually learned a lot of interesting concepts I did not know. With this said, I started this course in June 2012, which is just two or three months after Coursera launched. But that meant that some aspects of the course were quite not ready. The platform was great, but I feel like courses tended to be a bit better structured in the later courses. But the fact that I was trying out the first iteration also meant that right now the course could be a 10/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Basic accounting, basic financial modeling (but going beyond NPV etc.), Excel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/getdata"&gt;Getting and Cleaning Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was by the same Jeffrey Leek and Roger Peng that I admired so much and it was part of the data science specialization. But it was just not nearly as good as the previous two. And because by this point I knew that data science is 90% data cleaning and 10% complaining about data cleaning (I think I used this as a joke somewhere, but it &lt;em&gt;might&lt;/em&gt; not be my line, but I could not find the original source.), I wanted to learn more about how to do it efficiently. I wanted to try out the new specialization that the company that I so love and admire was spearheading. But the course was quite short, and although overall well-done, it was just not a very transformative experience. I think that I also really knew about 95% of this course, so I did not even watch most the lectures. Next week I always dived in in hopes of learning something new. I think for a beginner it could be a decent course, but I am still not sure if I would pay $49 for it. Though I understand the tradeoff Coursera was facing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; (potentially) Data wrangling, RegEx, &lt;code&gt;reshape&lt;/code&gt; and other useful packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 6/10&lt;/p&gt;
&lt;h1&gt;3. Good Courses&lt;/h1&gt;
&lt;p&gt;There good thing about the MOOCs is that if I don't like the course, the instruction seems crappy or I have hard time understanding the professors or I just do not like something, I can drop it. Moreover, there is so many great and awesome courses that I could not even do those that seemed 'good'--i.e. I would most likely like to take them if they were one part of the curriculum at my home institution. &lt;/p&gt;
&lt;h1&gt;Best practices&lt;/h1&gt;
&lt;p&gt;I think there are 3 key strategies to successfully finishing the MOOC. But the key insight is that most people who do not finish a MOOC usually do not finish it because of lack of time, not because of lack of aptitude. So lack of persistence is your key enemy. To crush your enemy, you have to:&lt;/p&gt;
&lt;h2&gt;1. Assess quickly the time commitment of a course.&lt;/h2&gt;
&lt;p&gt;I still remember that &lt;a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/about"&gt;convex optimization course&lt;/a&gt; by Stanford that I really wanted to take. But, alas, I could not. I did not have the time as I had a busy schedule &lt;em&gt;and&lt;/em&gt; I was doing another MOOC (or two?) at the time. I realized that this was a rigorous class that required substantial time commitment. But I needed to drop it, because if you do not take each commitment seriously, you will probably not finish any of them.&lt;/p&gt;
&lt;h2&gt;2. Honor your commitments&lt;/h2&gt;
&lt;p&gt;Meaning also that you should immediately unenroll from the courses you know you cannot finish. This will hopefully also give you the respect towards your commitment that you will also finish the ones you have elected to keep.&lt;/p&gt;
&lt;h2&gt;3. Commit to deadlines, minimum benchmarks&lt;/h2&gt;
&lt;p&gt;There is a reason why I only finished one free course on Udacity: because there are no deadlines (well, until they introduced the payment by the month). These help you get stuff done, because now you have to or all (or at least something) is lost. All major platforms have it now, so please use it.&lt;/p&gt;
&lt;p&gt;Benchmarks are also very helpful: there were courses (like both of the SNA ones) where I knew I wanted a distinction, because I wanted to know the stuff &lt;em&gt;well&lt;/em&gt;. Then there were courses, where I just wanted a brief overview so I only watched lectures (not listed, because I technically, haven't finished them). But decide quickly what your goals are and then follow them.&lt;/p&gt;
&lt;p&gt;I would also love to do a general post on MOOCs, but I think things are best kept separate. &lt;!-- Generally, I think MOOCs will fundamentally change the nature of education (for reasons that deserve a separate post), _provided_ that they manage to meaningfully integrate themselves to people's lives. We could already see many interesting initiatives like the one by &lt;a href="https://www.edsurge.com/n/2014-11-19-free-coursera-pd-thanks-to-the-president"&gt;Obama for teachers for their professional development&lt;/a&gt;. But currently MOOCs are mostly a domain of people who are happy to sacrifice some degree of social life for learning (totally worth it). Though I do not doubt there are exceptions, it seems that most people I know (and online statistics confirm this) are too busy to actually finish the courses.  --&gt;&lt;/p&gt;
&lt;p&gt;Anyway, hope this was helpful. As always, I would welcome any reaction. Get in touch!&lt;/p&gt;</content><category term="R"></category><category term="education"></category><category term="python"></category><category term="reviews"></category></entry><entry><title>On Randomness in (Social) Science</title><link href="jakublangr.com/underspecification-socsci.html" rel="alternate"></link><published>2014-12-06T04:00:00+00:00</published><updated>2014-12-06T04:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-12-06:jakublangr.com/underspecification-socsci.html</id><summary type="html">&lt;p&gt;Slightly technical, but relatively simple introduction to randomness and why it has to be everywhere around us. (Also check out the cool new LaTeX plug-in.)&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while back I was mentioning &lt;a href="/hello-world.html"&gt;how I was confused by the randomness as an emergent property of human systems&lt;/a&gt;, and so I have been giving this some thought, which I thought I might share with the world. The underlying problem always has been why is &lt;a href="http://plato.stanford.edu/entries/properties-emergent/"&gt;emergent&lt;/a&gt; randomness a good model of human behavior. At some level, we understand that our actions are &lt;a href="http://en.wikipedia.org/wiki/Determinism"&gt;deterministic&lt;/a&gt; at least on above-quantum level and I really dare not make any statements about the quantum nature of the universe. &lt;/p&gt;
&lt;p&gt;Before I start the discussion it is worth pointing out that I will try to make this as intuitive as possible, but some math will have to be involved, though I tried to make it so that it is not necessary to understand everything to really understand the point of this article. It was included to make the reasoning a bit easier. At the same time I am making some imprecise statements for the sake of understanding, but if you feel I might be leading people astray, feel free to drop me a line.&lt;/p&gt;
&lt;p&gt;The very definition of randomness is lack of predictability. That seems almost circular, but first thing to note here is that does not mean that all outcomes are equally likely. Think about a pair of dice, and our random variable ("what we measure") as the sum of the two rolls. Here there are seven has six possible combinations (6+1,5+2,3+4,4+3,2+5,1+6), giving us approximately normal distribution centered at seven:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;COUNT           NUMBER OF       PROBABILITY
                COMBINATIONS    
2               1               2.78%
3               2               5.56%
4               3               8.33%
5               4               11.11%
6               5               13.89%
7               6               16.67%
8               5               13.89%
9               4               11.11%
10              3               8.33%
11              2               5.56%
12              1               2.78%
Total           36               100%
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A better definition of randomness is that given an infinite amount of trials of random variable &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, we get the true distribution where the outcome of the next realization of said random variable is still unpredictable. So say you are throwing some dice in a casino and you are trying to figure out what is the bet that makes sense where in this odd casino you are meant to bet on the sum of the two dice. The number actually rolled will be a realization  (what you actually rolled) of &lt;span class="math"&gt;\(Y\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;. Any given roll will be unpredictable -- otherwise you could easily bet on it -- but it is deterministic -- i.e. if at the point of the throw you had a lot of time and all the laws of physics you could say with 100% certainty what the roll it is going to be. Yet right now, you do not know. However, that does not mean that it does not follow a certain distribution even though it is unpredictable. &lt;/p&gt;
&lt;p&gt;Now there is no doubt that if you come across any given pair of dice and you are asked what do you think the next number will be 7 should be your top choice. (Because it minimizes likely how far off the true number you are likely to be, both because it is in the middle and because it is most likely.) That is what can be thought of as expectation. However, generally the expectation lies in the middle of the distribution, but need not be the most common value. This becomes more obvious if we look at the formula for expectation. It is basically a weighted average of the value (&lt;span class="math"&gt;\(y\)&lt;/span&gt;) and the frequency (&lt;span class="math"&gt;\(f(y)\)&lt;/span&gt;). (Where &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are the boundary points.)&lt;/p&gt;
&lt;div class="math"&gt;$$
E[Y] = \sum_{a=2}^{b=12} y f(y) \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,  \,\,\,\,\,\,\,\,\,
\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,
$$&lt;/div&gt;
&lt;p&gt;In theory, we can even have a distribution where the expected value is impossible, such as the one below: (for a concrete example imagine student's ratings of a &lt;em&gt;really&lt;/em&gt; controversial class on a scale from -15 to +15, where the two groups are equally split and the average rating is about zero)&lt;/p&gt;
&lt;p&gt;&lt;img src="Public/blog/download.png" alt="Distribution" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;If you look at the formula for expectation above it is all very nice and mathematical. We have our best guess that makes sense, but is far from guaranteeing that we are actually walk back home with a lot of money. But what if you could observe rolls of two dice would they be perfectly distributed according to the distribution that we have specified? Well, no. Dicemakers probably tried to make them as balanced as possible, but probably every pair of dice has certain numbers that will come less or more commonly than simple combinatorics would suggest. This deviations might be due to tiny imperfections on the table, the dice themselves or many other factors. If you find statistically significant relationships, we might be able to derive a marginally better model. But it is only going to do as well as the strength of relationships. Moreover, if we find these associations and do not really understand why -- e.g. we just observe, they might backfire, because if maybe we found some irregularities on the table and then we start throwing on other parts of the table, we will start losing more. Our model did not generalize "out of sample" (on what we did not observe) well.&lt;/p&gt;
&lt;p&gt;Now bring this back to social science. Some people choose to think of the economy as some sort of machine, which suggests determinism. Let's accept that for a second. &lt;/p&gt;
&lt;p&gt;Even if we do we have two problems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The underlying nature of the system keeps on changing. Sure, economy &lt;em&gt;can&lt;/em&gt; be a machine, but it will be a different machine in March 2014 and different in April. &lt;/li&gt;
&lt;li&gt;It does not actually explain why there is any randomness. A machine is deterministic, so shouldn't our predictions be too? It turns out that it is the emergent property (or rather the fact that we have incomplete information) that makes it random. So yes, the world may be deterministic but we need a lot more information that we ever have. Hence we are trying to abstract models that capture the key information. So in our dice example we can notice that the side with one is heavier than the others and hence might come up marginally less often and even though the dice roll is always deterministic, the best heuristic, if we cannot exactly calculate everything, is to use the base rate (how often does 7 naturally occur) plus whatever extra information we may have (heavier sides?).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But randomness only really makes sense if we are actually trying to predict something. We are essentially saying that there is a part that our model (the base rate and extra information together) can predict [1] and a part that our model cannot explain -- i.e. the error term [2]. Basically whenever we don't get our prediction exactly right we get an error -- the difference between actual and expected value. So what we are basically saying is that at this level we can only explain so much and the predicted value is our best guess. That is, after all, what we are doing intuitively with the dice as well: we all intuitively know that predicting always that the outcome is going to be 3 is not a good strategy, unless we know that the dice are biased towards them.&lt;/p&gt;
&lt;p&gt;So really, this problem is not unique to social science, but rather inference in general. Because all of the extra information we have probably included in our prediction. Think about predicting rain: to have rain you need clouds, hence if there is a huge black cloud over your head your prediction with this extra information will already be say 90%, but that is only because we have already built the model (we know) that if there's a could over our heads it it is likely to rain. The fact that it does was not a trivial insight at some point. This is what we are doing in science, except with much more difficult concepts. Hence, randomness has to be an emergent property of &lt;em&gt;any&lt;/em&gt; predictive model with incomplete information. And since we make predictions so often in every science and the error term has to be random (otherwise we could easily figure out a better model), which is why all the good models of complex behavior have to have a random part. But always keep in mind that the distribution of errors can vary, but there is always going to be a random part by definition unless we know the true model (i.e. we have complete information).&lt;/p&gt;
&lt;p&gt;I hope that this was helpful and that you know at least sort of understand why randomness is not a property of human systems, but of prediction with incomplete information and that there are different forms that randomness can put on. But I would love to hear your thoughts and feedback, let me know!&lt;/p&gt;
&lt;p&gt;[1] usually denoted as &lt;span class="math"&gt;\(E(Y | {\bf X } )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[2] usually denoted as &lt;span class="math"&gt;\( \epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt;, though it can be distributed according to a wide variety of distributions.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="technical"></category><category term="social science"></category></entry><entry><title>Tracking the CrISIS in Syria</title><link href="jakublangr.com/tracking-crisis.html" rel="alternate"></link><published>2014-09-22T21:00:00+01:00</published><updated>2014-09-22T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-09-22:jakublangr.com/tracking-crisis.html</id><summary type="html">&lt;p&gt;Exploring the power of Big Data approach (GDELT) applied to the civil war in Syria to get unconventional insights into the Syrian civil war and the terror tactics of ISIS.&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are interested more in the analysis of the conflict from a quantitative persective and not so much in the methodology, you &lt;a href='#explore'&gt; can jump straight there.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As I was working as a contractor this summer, I came across a very interesting project called &lt;a href="http://www.gdeltproject.org/"&gt;GDELT&lt;/a&gt; -- Global Database of Events, Language and Tone. This project I think is best described by the official website statement: "GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world."&lt;/p&gt;
&lt;p&gt;In addition to that, as a part of &lt;a href="../hello-world.html"&gt;Good Judgement Project&lt;/a&gt;, I was made to forecast on Syria and ISIS' actions. As I was going through the articles describing what is happening in Syria, I became to wonder: what is the bigger picture? To what extent is this hype all just because &lt;strong&gt;now&lt;/strong&gt; ISIS is the headline-grabber in the West? (But of course, no one cares until it becomes a story in the public consciousness.) That's when GDELT comes into the picture. I wanted to try it as I came across it, but since I was finishing my data science project, I did not quite have the time. I was awe-struck by the complexity and potential of it so I knew I wanted to give a go. I have not really used R all that extensively in about a year since my internship, but I am not afraid of large analytics, so I decided to refresh those skills.&lt;/p&gt;
&lt;p&gt;Fortunately, Google was so nice as to host this almost 100 GB dataset on &lt;a href="http://googlecloudplatform.blogspot.cz/2014/05/worlds-largest-event-dataset-now-publicly-available-in-google-bigquery.html"&gt;its cloud for free&lt;/a&gt;, for which I love Google. Not only that, but Google also loaded this dataset so that it can be queried using &lt;a href="http://en.wikipedia.org/wiki/BigQuertracky"&gt;BigQuery&lt;/a&gt; using public-facing APIs or a web interface. We can then query GDELT using SQL-like language as shown below and subsequently export the dataset as a CSV:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gdelt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bq&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;full&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; 
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MonthYear&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;201303&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;2015&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;NumSources&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;GoldsteinScale&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt;
&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Actor1Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(For those interested, I include a smaller version of the dataset I was working with after exporting from GDELT &lt;a href="Public/blog/sample.csv"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The assumption of this endeavor is that if I include non-English media (and perhaps the less main-stream ones too) by using GDELT's information rather than Google, which will always give me traditional sources, I can get a better sense of what is going on. Thanks to the the law of large numbers, I should not get too many false positives from spurious sources, so my overall analysis should remain robust and even more objective than relying on sources such as CNN or BBC alone. One additional advantage GDELT has is that it is updated every 24 hours. In theory, this analysis could be automated so that it updates itself every 24 hours. (Actually, it should not be &lt;em&gt;that&lt;/em&gt; much more work ... but given the size of GDELT, I would have to start paying for using the BigQuery service that often.)&lt;/p&gt;
&lt;p&gt;As with any more substantive project, there was a lot of data cleaning and obstacles. Namely: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ISIS not recognized by GDELT&lt;/strong&gt;:
There's lots of International Militarized Groups (IMGs) recognized by GDELT, but ISIS seems to be too new in the headlines to be recognized by GDELT coding yet. Hence I had to look for substitutes:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMG (International Militarized Group) tag to the rescue&lt;/strong&gt;:
There is a column specifying actor &lt;em&gt;type&lt;/em&gt; (not actor name!) for each event. There is an actor type IMG, which was fortunate. However, there is no way of fully knowing whether said group is ISIS or not from this dataset. (At least with this level of granularity, but I am sure it would be technically possible, if GDELT team so chose.) I would also imagine that the fighters on the ground themselves are often not quite sure, given the number of splinter groups these organizations harbor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;But sometimes "REB"&lt;/strong&gt;:
When individually inspecting the events (just some of them, I do not have the time to go through the entire data dump), I quickly realized that the data pertaining to clearly ISIS attacks are sometimes labeled as "REBEL" actor type. So I needed to include both in the analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cleaning the data&lt;/strong&gt;:
For some events it is hard, if not impossible, to correctly specify precise coordinates. As a result, GDELT merely inputs the values of the approximate center of Syria. These records obviously needed to be removed from the dataset. Obviously, there was couple of more steps that I needed to do and a couple that tricked me for a little bit, for instance, when I found that the state code of one of the events was &lt;em&gt;IS&lt;/em&gt;, which, however, stood for Israel, as I later found out.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GDELT project classifies each event using &lt;a href="http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt"&gt;one of their internal codes&lt;/a&gt;, each of these codes then gets then a &lt;a href="http://web.pdx.edu/~kinsella/jgscale.html"&gt;Goldstein score&lt;/a&gt;, which basically expresses whether this event whether people are being nice to each other (positive score) or mean/evil towards each other (negative score). Where +10.0 is some massive humanitarian/developmental aid project, +1.0 is probably visit of some minor national politician. All the positive things were filtered out, because I wanted to focus on what the problems were. There -1.1 is, for instance, "Deny an attributed policy, action, role or position" and -10.0 is "Military attack; clash; assault".&lt;/p&gt;
&lt;p&gt;As the more SQL-knowledgeable of you might have noticed, I only considered events that came from 2 independent sources and happened on or after March 2013. &lt;/p&gt;
&lt;h1 id='explore'&gt; Let's explore &lt;/h1&gt;

&lt;p&gt;With this dataset in hand, I made several maps where I plotted each event as a point sized according to its Goldstein score and set relatively low transparency so that all hotspots appear in bright red. I plotted this dataset by month, and here's March 2013 till July 2013. We see that at the end of this period, there's was violence spearing from its initial hotspots. &lt;/p&gt;
&lt;h3&gt;March and April 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201303.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201304.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;May and July 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201305.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201307.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Now, if we look at the period August till November, we see that there's a lot more violence then. But to some extent we have to bear in mind that this dataset includes &lt;em&gt;all&lt;/em&gt; rebels and not just ISIS. &lt;/p&gt;
&lt;h3&gt;August 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="Public/blog/201308.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;September 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="Public/blog/201309.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;October 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="Public/blog/201310.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;November 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="Public/blog/201311.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we have to consider the connected events: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=Syria,+Chemical+Weapons&amp;date=1/2013+12m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Here, I am using Google Trends as a proxy for "Western interest" in the matter. We can see that the use of chemical drew Western attention to Syria, however, this interest was very short-lived.&lt;/p&gt;
&lt;p&gt;Clearly, August and September are easily explained by the usage of chemical weapons. But the violence following this, especially in October and later, cannot be explained just by looking at chemical weapons, because that is when the hype ended and has not picked up until ISIS was the next important thing.&lt;/p&gt;
&lt;p&gt;The following months were similarly violent and few in the West seemed to care. &lt;/p&gt;
&lt;h3&gt;December 2013 and January 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201312.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201401.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;February and March 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201402.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201403.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;April and May 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201404.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201405.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;If we plot the Google trend graph again, since the start of Syrian Civil War in March 2011, we see that the attention Syria got around the times of the use of chemical weapons is unparalleled. This is hardly surprising, but what is notable is that there a &lt;a href="http://www.cbsnews.com/news/syria-chemical-weapons-attack-blamed-on-assad-but-wheres-the-evidence/"&gt;disagreement regarding who actually used them&lt;/a&gt;, though I think that at least some of these attacks were sure to be made by the government. &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=Syria&amp;date=1/2009+43m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_1&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;But despite the limited interested by the West, violence was actually similar to the months during which the chemical weapons were used. &lt;/p&gt;
&lt;p&gt;What is interesting that ISIS was not picked up by Western media much until recent months, despite the fact that ISIS can be traced back to 2004: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=%22Islamic+State%22,+ISIS,+ISIL&amp;date=today+12-m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Still, violence was hovering around the same level:&lt;/p&gt;
&lt;h3&gt;June and July 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201406.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201407.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;August and September 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="Public/blog/201408.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="Public/blog/201409.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;It is interesting that it seems that despite the limited budget that ISIS has for its media operations, its terror tactics are largely successful: Syria is not, at least by according to this analysis, by and large, more violent than it was in the few months prior to ISIS' inclusion in the Western media's limelight. &lt;em&gt;Yet&lt;/em&gt;, ISIS get substantial amount of attention, so much so that it prodded United States and &lt;em&gt;five Arab&lt;/em&gt; states (no European states, however, at the time of writing) to launch a military operation starting today. This article used a lot of proxies so I do not want to make grandiose claims that cannot be supported by this analysis.&lt;/p&gt;
&lt;p&gt;Nevertheless, I think what this article shows that ISIS first and foremost is unparalleled by its usage of social media compared to other terrorist organiztations and splinter groups. If anything, it seems that the number of violent occurrences has decreased. But the terror-tactics and the headline-grabbing strategy of ISIS is already working well for them. West pays attention. &lt;/p&gt;
&lt;p&gt;At the same time, the brutality and the casualities is hard to really compare, because the information about the previous comparable systems (e.g. &lt;a href="http://en.wikipedia.org/wiki/Islamic_Emirate_of_Afghanistan"&gt;Islamic Emirate of Afghanistan&lt;/a&gt;) is very limited and we might just never get as complete picture as we have now. But now, largely thanks to GDELT, we have a much more complete dataset about the conditions within the Islamic State and so it might prove easier to benchmark the level of violence and discord in this society against all other -- be it present or future -- states. &lt;/p&gt;
&lt;p&gt;I am currently working on an interactive R-Shiny app for this visualization/analysis so stay tuned!&lt;/p&gt;
&lt;p&gt;Want the R script? Want to give feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</content><category term="modeling"></category><category term="GDELT"></category><category term="R"></category><category term="Syria"></category></entry><entry><title>Predictions R hard</title><link href="jakublangr.com/hello-world.html" rel="alternate"></link><published>2014-09-14T21:00:00+01:00</published><updated>2014-09-14T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-09-14:jakublangr.com/hello-world.html</id><summary type="html">&lt;p&gt;Quick and dirty statistical modeling in R for social scientists, decision makers and enthusiasts. Insights from my involvement in a US intelligence project.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;... or quick and dirty statistical modeling in R&lt;/h1&gt;
&lt;p&gt;Recently, I decided to start a blog and my ongoing involvement in the &lt;a href="http://goodjudgmentproject.com/"&gt;Good Judgment Project (GJP)&lt;/a&gt;, which is part of US government's &lt;a href="http://www.iarpa.gov"&gt;Intelligence Advanced Research Projects Activity (IARPA)&lt;/a&gt;, more specifically the ACE Project, proved to be a decent excuse to do so. &lt;/p&gt;
&lt;p&gt;I have joined GJP mid-Season 3 (hence I haven't participated fully) and then I tested into season 4 (combination of general intelligence test and political knowledge). This test alone was a treat, because it followed all the guidelines about best practices of hiring (to my knowledge) and I knew that if I get in, this will be a lot of fun. What I did not know at the time was that only one in ten will actually be accepted into GJP, otherwise I would probably put in a lot more work into the test.&lt;/p&gt;
&lt;p&gt;Despite this mess-up, I joined a project and met my amazing fellow forecasters in my team. At the time of joining, I already knew that I will be putting numeric probabilistic values on how the world might soon turn out to be (e.g. will there be a direct Russia-Ukraine confrontation in Crimea). All of these questions are well-defined using a very sophisticated set of definitions, but making the evaluation criteria very flexible and as to allow GJP Team to 'resolve' each question in the spirit rather than the letter of the question.&lt;/p&gt;
&lt;p&gt;Number of these questions have allowed for use of publicly available datasets and so I would like to offer some code that I used to ease up my work as a forecaster. This particular example is relates to a question that resolved yesterday: If on a certain date the area of ice on the Arctic sea will be more than what it was last year.&lt;/p&gt;
&lt;p&gt;The following R code downloads, cleans and plots some time-series representation of the record ed data of the ice for the past couple of years. It is &lt;em&gt;super-simple&lt;/em&gt;, but I was able to do it from question to quantitative probabilities in about 30 minutes, so this is just to demonstrate that even such simple code can help you make a much more solid prediction. This was necessary, because my team's forecasts were all over the place (among 9 forecasters we ranged 5-80%) and statistical modeling is a great way how to deal with data that is normally quite counter-intuitive and complex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;TTR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reshape&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tseries&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;forecast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Loading, cleaning (missing values are by default assigned value of -9999)&lt;/span&gt;
&lt;span class="nf"&gt;read.csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ijis.iarc.uaf.edu/seaice/extent/plot_v2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="nf"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-9999&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="nf"&gt;colSums&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Melt changes the columns (there is a column for each year) into just one column &amp;#39;value&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;data_ts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;melt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data[&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="n"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_ts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deltat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data_ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;na.remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ts[&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="n"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Decomposes the time-series plots the decomposition and the forecast (as well as prints it)&lt;/span&gt;
&lt;span class="n"&gt;decomposed_ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;decompose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_ts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decomposed_ts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ts_forecast&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;HoltWinters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_ts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;forecasts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;forecast.HoltWinters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ts_forecast&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;forecasts&lt;/span&gt;
&lt;span class="nf"&gt;plot.forecast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;forecasts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data[250&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;260&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For GJP purposes, the confidence intervals produced by this model are especially useful, because they give you an idea of how confident you can be in your predictions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;forecasts&lt;/span&gt; 
&lt;span class="n"&gt;Point&lt;/span&gt; &lt;span class="n"&gt;Forecast&lt;/span&gt; &lt;span class="n"&gt;Lo&lt;/span&gt; &lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;Hi&lt;/span&gt; &lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;Lo&lt;/span&gt; &lt;span class="m"&gt;95&lt;/span&gt; &lt;span class="n"&gt;Hi&lt;/span&gt; &lt;span class="m"&gt;95&lt;/span&gt; 
&lt;span class="m"&gt;16.73710&lt;/span&gt; &lt;span class="m"&gt;4981786&lt;/span&gt; &lt;span class="m"&gt;4920648&lt;/span&gt; &lt;span class="m"&gt;5042924&lt;/span&gt; &lt;span class="m"&gt;4888284&lt;/span&gt; &lt;span class="m"&gt;5075288&lt;/span&gt; 
&lt;span class="m"&gt;16.73995&lt;/span&gt; &lt;span class="m"&gt;4963324&lt;/span&gt; &lt;span class="m"&gt;4867154&lt;/span&gt; &lt;span class="m"&gt;5059494&lt;/span&gt; &lt;span class="m"&gt;4816245&lt;/span&gt; &lt;span class="m"&gt;5110403&lt;/span&gt; 
&lt;span class="m"&gt;16.74279&lt;/span&gt; &lt;span class="m"&gt;4944862&lt;/span&gt; &lt;span class="m"&gt;4814956&lt;/span&gt; &lt;span class="m"&gt;5074768&lt;/span&gt; &lt;span class="m"&gt;4746188&lt;/span&gt; &lt;span class="m"&gt;5143536&lt;/span&gt; 
&lt;span class="m"&gt;16.74564&lt;/span&gt; &lt;span class="m"&gt;4926399&lt;/span&gt; &lt;span class="m"&gt;4762199&lt;/span&gt; &lt;span class="m"&gt;5090600&lt;/span&gt; &lt;span class="m"&gt;4675277&lt;/span&gt; &lt;span class="m"&gt;5177522&lt;/span&gt; 
&lt;span class="m"&gt;16.74849&lt;/span&gt; &lt;span class="m"&gt;4907937&lt;/span&gt; &lt;span class="m"&gt;4708313&lt;/span&gt; &lt;span class="m"&gt;5107562&lt;/span&gt; &lt;span class="m"&gt;4602638&lt;/span&gt; &lt;span class="m"&gt;5213237&lt;/span&gt; 
&lt;span class="m"&gt;16.75134&lt;/span&gt; &lt;span class="m"&gt;4889475&lt;/span&gt; &lt;span class="m"&gt;4653078&lt;/span&gt; &lt;span class="m"&gt;5125872&lt;/span&gt; &lt;span class="m"&gt;4527937&lt;/span&gt; &lt;span class="m"&gt;5251014&lt;/span&gt; 
&lt;span class="m"&gt;16.75419&lt;/span&gt; &lt;span class="m"&gt;4871013&lt;/span&gt; &lt;span class="m"&gt;4596412&lt;/span&gt; &lt;span class="m"&gt;5145614&lt;/span&gt; &lt;span class="m"&gt;4451047&lt;/span&gt; &lt;span class="m"&gt;5290979&lt;/span&gt; 
&lt;span class="m"&gt;16.75704&lt;/span&gt; &lt;span class="m"&gt;4852551&lt;/span&gt; &lt;span class="m"&gt;4538291&lt;/span&gt; &lt;span class="m"&gt;5166811&lt;/span&gt; &lt;span class="m"&gt;4371932&lt;/span&gt; &lt;span class="m"&gt;5333170&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the plots here can help elucidate why my team might have been all over the place: there are many factors influencing how much ice there will be on a given day: sure, there is the seasonal (summer-winter) cycle, which matters a lot, but there is also some random element as well as measurement error. I fear that lots of my fellow forecasters (perhaps including myself for a long time) fell victim to &lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;over-fitting&lt;/a&gt; to the random element. Sure, we could observe the pattern from the previous year, but no one from the team--to my knowledge--plotted the data to see what (approximate) influence each of the two factors have. This is why decomposition is so great:&lt;/p&gt;
&lt;h2&gt;Decomposed time-series plot&lt;/h2&gt;
&lt;p&gt;&lt;img src="Public/blog/gjp_modelling_ts_decomposition.svg" alt="Timeseries decomposition of ice-area" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;One interesting element of the decomposition is the trend, which I was expecting to see fairly clear evidence of global warming, but instead I really found out "just" evidence of climate change. Though, of course, I am no climatologist, so perhaps that interesting dip in the year 2000 had some other (spurious?) reasons. &lt;/p&gt;
&lt;p&gt;The decomposition, although helpful, was not what I considered the main advantage of this little exercise: I was then able to use a time-series statistical model called HoltWinters to predict the area of ice (no pun intended?), which uses exponential smoothing that is quite fitting in these conditions (I wonder whether these statistical puns significantly affect my readership?). This model is appropriate mainly because there is a lot of historical data and the data is always very constrained to a certain range (e.g. a 3 sigma change is &lt;em&gt;actually&lt;/em&gt; extremely unlikely, which cannot be said of all financial data this model is applied to). &lt;/p&gt;
&lt;h1&gt;Thoughts about GJP itself&lt;/h1&gt;
&lt;p&gt;GJP is what I think social science should be: quantifiable, based on best-practices and aiming to get as close to the truth as possible, not tell the best story. This was, interestingly enough, the conclusion (&lt;em&gt;simplifying tremendously&lt;/em&gt;) of Professor Philip Teltock in &lt;a href="http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715"&gt;his book Expert Political Judgment&lt;/a&gt;. Namely that the political forecasters that tell the best story are usually favored by the media, because it sells well, but they tend to do worse than most others on accuracy. Professor Tetlock is now one of the main organizers of GJP and so it is a great honor to work as his lab rat, because it helps to do cutting-edge research and it helps me to learn a ton in the process.&lt;/p&gt;
&lt;p&gt;One additional gift every forecaster got from the GJP was a set of presentations of best practices of forecasting, which I think were fascinating, albeit oftentimes simple guidelines to stick to. But as with everything, it is much more a matter of putting these into practice rather that makes a great forecaster. &lt;/p&gt;
&lt;p&gt;Couple of interesting things that I noticed about myself looking at old predictions: &lt;/p&gt;
&lt;h2&gt;1. I am overly cautious&lt;/h2&gt;
&lt;p&gt;Reading loads of literature about randomness and its role in human society (Kahneman, Taleb, Tversky, Ariely, Mlodinow etc.) I was overly careful about putting down anything close to 1% or 99%, even though some questions call for it (likelihood of a massive reform of international institutions in a short timespan). Good Judgment, after all, is not only very well calibrated, but it is also very discriminating. &lt;/p&gt;
&lt;h2&gt;2. I struggle with randomness&lt;/h2&gt;
&lt;p&gt;Interesting point about randomness--to some extent related to the previous one--how do we square the fact that randomness is just a model and yet we keep on invoking the properties of randomness when talking about the world. But, at least on the level of humans, each action is supposedly deterministic. Yes, some might call on the &lt;a href="http://www.quantumdiaries.org/2014/07/04/wrong/"&gt;famous George Box quote&lt;/a&gt;, but that does not really answer why in a situation of imperfect information &lt;em&gt;random&lt;/em&gt; is the &lt;em&gt;best&lt;/em&gt; model. This is especially confusing given the amount of strategic interaction and social influence in the world. &lt;/p&gt;
&lt;p&gt;This turned out to be a problem with with some of the GJP predictions I make, because I want to stick to randomness as a baseline (because it seems to be best practice), but then struggle to always justify this to myself and square it with the rest of information. This especially bothered me outside of GJP, however. In Mlodinow's book &lt;a href="http://www.amazon.com/The-Drunkards-Walk-Randomness-Rules/dp/0307275175"&gt;The Drunkard's Walk&lt;/a&gt;, he specifically used the example of production studio's CEOs as someone who is faced with great randomness in the movies. But each action in this immensely complex chain of interactions is deterministic, strategic and influenced by one's surroundings. Why is randomness an emergent property of human systems? Seems non-obvious.&lt;/p&gt;
&lt;h2&gt;3. Conditional interaction is hard&lt;/h2&gt;
&lt;h3&gt;(especially cross-culture)&lt;/h3&gt;
&lt;p&gt;There was a question on what would DPRK do, should the United States take a bold, but merely supportive, military action in aid of South Korea. Our team struggled to agree even if the bold action would increase chance of state DPRK being more aggressive towards South Korea or less so. Would the desire to prove something to US (or more generally, the West) outweigh the potential risks, because of the importance of the precedent or would the bold action scare off DPRK? Both ways of reasoning sounded equally plausible to me at the time so I just followed what my initial prediction was. But really, there was no reason for it. The question whether I got it right or wrong is irrelevant in this case; the question is: would I be able to make a good prediction next time? I fear that so far the answer is no and so I never swayed too far off 50%. I will probably need to devise a way of breaking these ties.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</content><category term="modeling"></category><category term="R"></category><category term="statistics"></category><category term="GJP"></category></entry></feed>