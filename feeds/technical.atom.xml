<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Langr's Blog</title><link href="jakublangr.com/" rel="alternate"></link><link href="http://www.jakublangr.com/feeds/technical.atom.xml" rel="self"></link><id>jakublangr.com/</id><updated>2016-05-03T21:00:00+01:00</updated><entry><title>Kaggle Struggle</title><link href="jakublangr.com/kaggle-2016.html" rel="alternate"></link><published>2016-05-03T21:00:00+01:00</published><author><name>Jakub Langr</name></author><id>tag:,2016-05-03:jakublangr.com/kaggle-2016.html</id><summary type="html">&lt;p&gt;For those of you that are not compete data nerds, I occasionally venture into the world of &lt;a href="http://www.kaggle.com"&gt;Kaggle&lt;/a&gt;: a competitive data science website where everyone from the top Data Science experts to the people who are just starting out compete against each other in the hopes of getting money (statistically unlikely), fame and massive amounts of fake nerd points (very important).&lt;/p&gt;
&lt;h1&gt;My life on Kaggle&lt;/h1&gt;
&lt;p&gt;&lt;div align="center"&gt;
&lt;img src="http://s3.amazonaws.com/theoatmeal-img/comics/working_home/6.png" alt='my life on Kaggle' style="width: 95%; height: auto "&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;I have so far participated in four different competitions: &lt;a href="https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction"&gt;Liberty Property Inspection Prediction&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management"&gt;BNP Paribas Claims Management&lt;/a&gt; and &lt;a href="https://www.kaggle.com/c/santander-customer-satisfaction"&gt;Santander Customer Satisfaction&lt;/a&gt;. All of these were fairly different competitions and I have learned loads of different things. &lt;/p&gt;
&lt;p&gt;In many cases, Kaggle is great: no more data archeology like in most commercial datasets. But this is not a rant about how when you bring an extra team on a project you should document the changes they made. Or about how when you start/stop sharding your database and you have variable names that indicate this, these should be updated as well. Or indeed that columns have swapped names or names that have nothing to do with what they contain for historical reasons. Not that I am bitter or anything.&lt;/p&gt;
&lt;h1&gt;Okay, okay... but I still don't get it.&lt;/h1&gt;
&lt;p&gt;So the idea behind Kaggle is that companies that have a data problem put their data (anonymised, well-structured and standardised) on the site provide some minimal description of the problem (e.g. we want to be able to predict which marketing campaign will be successful based on some historical data). &lt;/p&gt;
&lt;p&gt;Then about half a million data scientists form around the world can download this dataset and play around with it and even submit their predictions to be scored against the known (historical) truth. The best ones get all of the perks above, the others get nothing. Capish?&lt;/p&gt;
&lt;h1&gt;Why did I decide to join?&lt;/h1&gt;
&lt;p&gt;This website is first and foremost a great way to learn: you get to know what people much better than you are doing and learn from them: usually mostly after the competition is over. &lt;/p&gt;
&lt;p&gt;I also think that working on these kinds of competitions is great especially for benchmarking your own skills and generally what lift (percentage increase) can we expect by massaging the data a bit more: be it &lt;a href="https://en.wikipedia.org/wiki/Feature_engineering"&gt;feature engineering&lt;/a&gt; or by &lt;a href="http://mlwave.com/kaggle-ensembling-guide/"&gt;ensambling&lt;/a&gt;. So Kaggle is a great way to test out some of these things in practice and see how others use them to some extent. &lt;/p&gt;
&lt;p&gt;I think that Kaggle is a great way to connect with other data scientist and figure out how they do things and maybe even make friends along the way. Coincidentally, this is also a great way how to spend a lot of money (more on that later).&lt;/p&gt;
&lt;h1&gt;How did I do?&lt;/h1&gt;
&lt;p&gt;There is a lot to do Kaggle: you earn ranking within each competition: I earned the Top 25% badge by finishing in the top 23.6% (property inspection), 12.9% (marketing) and 20.9% (claims management). Once I missed it by a 31.3% finish (customer satisfaction). I think that overall I have learned loads during these competitions: people frequently share code and model descriptions of the best performing models (the best performers though usually hold out till the end of competition). (You thought there will be an article about data science without numbers? Ha.)&lt;/p&gt;
&lt;h1&gt;Dude, you have a serious data problem.&lt;/h1&gt;
&lt;p&gt;Hi, I'm Jakub and I am a data addict. I have spent two weeks off AWS Console... but in all seriousness, Amazon Web Services is both a gift and a curse to every techie. Gift because you can have data centres' worth of computational power at your fingertips within minutes. Curse because of what it is going to cost you:&lt;/p&gt;
&lt;table align="center"&gt; 
&lt;tr&gt;
    &lt;td&gt;
    &lt;img src='images/april_aws.png' alt="April" style="width: 95%; height: auto "align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
    &lt;td&gt;
    &lt;img src="images/may_aws.png" height="38%" alt='May' style="width: 95%; height: auto " align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1 style="text-align: center"&gt; &lt;b&gt; &lt;a href='https://youtu.be/CduA0TULnow?t=1m28s'&gt; Ooops. &lt;/a&gt;&lt;/b&gt; &lt;/h1&gt;

&lt;p&gt;I don't feel like I am using AWS excessively even. But perhaps it was all worth it: &lt;/p&gt;
&lt;div align="center"&gt;
&lt;a href="https://www.kaggle.com/jakublangr"&gt;
&lt;img src="images/kaggle_status.png" style="align-content: center; width: 65%; height: auto"&gt; &lt;br&gt; 
&lt;/a&gt; Top .929%, aww yeah! 
&lt;/div&gt;

&lt;h1&gt;What to make of it&lt;/h1&gt;
&lt;p&gt;But the model of competitive data science is greatly interesting to many companies: not only does Kaggle have a track record of blowing the existing industry experts out of the water with their accuracy. So it is clearly doing well in other respects: like supporting the community and spreading information. There's loads of free public datasets to explore and easily share your findings. Also, the spread of efficient techniques and algorithms is thanks to Kaggle greatly accelerated: for instance, random forest (probably one of the top algorithm) was written in 2001 and rose to prominence around 2009, while eXtreme Gradient Boosting (XGB) was written in 2014 and blew other algorithms away by late 2015. So massive acceleration. Even I personally have to say that the community is amazing and there's always loads to learn.&lt;/p&gt;
&lt;p&gt;So that's it from now. As always, if you have a comment/question, I'd love to hear from you!&lt;/p&gt;</summary><category term="datascience ML technical"></category></entry><entry><title>On MOOCs: Projects, Practice and Perspective</title><link href="jakublangr.com/moocs-part1.html" rel="alternate"></link><published>2015-01-23T21:00:00+00:00</published><author><name>Jakub Langr</name></author><id>tag:,2015-01-23:jakublangr.com/moocs-part1.html</id><summary type="html">&lt;p&gt;It has been quite a while since I started my first MOOC at Coursera. I think now is the time to reflect on the courses I have finished, what I have learned as well as what to recommend to my fellow MOOCers. I will try to broadly classify (and score, because if you are anything like me, you occasionally dislike the imprecision of natural language) the courses I have finished and then I will try to give my take on MOOCs in general.&lt;/p&gt;
&lt;h1&gt;1. Awesome Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/sna"&gt;Social Network Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course by &lt;a href="http://www.ladamic.com/"&gt;Lada Adamic&lt;/a&gt; was definitely one of the most awesome courses regardless of platform, country or institution that I have been involved in. Not only was Lada willing to have Google Hangout sessions with the course participants, but the course was excellently done: we have worked with &lt;a href="http://gephi.github.io/"&gt;Gephi&lt;/a&gt;, programmed in &lt;a href="https://ccl.northwestern.edu/netlogo/"&gt;NetLogo&lt;/a&gt; and used the &lt;a href="http://cran.r-project.org/web/packages/sna/sna.pdf"&gt;R SNA package&lt;/a&gt;(or at least I did in my projects). The course was a great balance of (social) network analysis, problem sets and optional programming assignments. You can view mine &lt;a href="https://dl.dropboxusercontent.com/u/30848031/US_Contributions.pdf"&gt;here&lt;/a&gt; (though bear in mind, I have done almost 2 years ago so it is not something I am particularly proud of). &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; R, Gephi, NetLogo, different metrics characterizing a graph&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/dataanalysis"&gt;Data Analysis&lt;/a&gt; and &lt;a href="https://www.coursera.org/course/compdata"&gt;Computing for Data Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long before Coursera introduced the 'Data Science specialization' (that in my opinion seems way too superficial), Coursera was offering these two amazing Courses that together were actually undiluted first year graduate bioinformatics courses for students at Johns Hopkins University. The problem with a lot of the later MOOCs is that they were starting to get too simple to actually teach me anything and so the pool of people who was actually taking these courses was a lot worse. I think the latter is also an issue, because half of the learning experience is the community. Like the people putting interesting thoughts on the forums or the moderators (or in this case, the community itself) trying to prevent cheating (mostly unintentional, I hope, why else would you do a MOOC?) and so on. I have been taking this courses with statistics PhDs and other people with amazing insights and experience and I think that is probably why the learning experience was so amazing and all the four projects including &lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/Cellphone_data_Prediction.pdf"&gt;the final one&lt;/a&gt; were something that gave me a solid grounding for the Social Network Analysis MOOC to use R as well as for my &lt;a href="http://goo.gl/sEUFMa"&gt;internship in the summer of 2013&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 9/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Through exploration of R basics, statistics, basic machine learning, data preparation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-0#.VMPxF4ptN5Q"&gt;Science of Uncertainty, Introduction to Probability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was amazing, but also &lt;strong&gt;very&lt;/strong&gt; hard. This is the undiluted MIT 6.041 course. The commitment from the teaching staff of MIT (including the professor &lt;a href="http://www.mit.edu/~jnt/home.html"&gt;John Tsitsiklis&lt;/a&gt;) was incredible. Generally, I got a great answer for any of my questions in a shorter period of time then most of my professors at my home institution. The treatment was very rigorous that keeping up with the Oxford workload while doing this 12 week course proved quite difficult. But it was so much more rewarding to see the certificate of completion afterwards.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Probability theory, Statistics, Inference &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/ml"&gt;Machine Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ah yes, this is where it all started. I had to, of course, eventually finish this course. Andrew Ng (my personal hero) created this course back in April 2012 and I believe it might be now in its 10th iteration. This course will give you a taster of Support Vector Machines, Recommender Systems, Artificial Neural Networks (basis for &lt;a href="http://en.wikipedia.org/wiki/Deep_learning"&gt;Deep Learning&lt;/a&gt;) and many others. The awesome thing about this course was that it abstracts &lt;em&gt;everything else&lt;/em&gt; apart from the ML algorithms to a package that you download so all you need to write is the machine learning algorithm. This means that this course is a mere introduction to machine learning and this alone cannot be considered to give you enough knowledge to actually go out and program some of them in production. This course was just so much fun, because of all the cool stuff that you write that I still have to give it 10/10. Plus, in the right data science tradition, this course has been continuously improved for each iteration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, MATLAB, Statistics&lt;/p&gt;
&lt;h1&gt;2. Great Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-0"&gt;Computational Thinking and Data Science&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was a great application of Object-Oriented Programming and simulations. I have to say I have gotten so much support on the forums it was incredible. (I do not recall, however, whether this was mostly staff or other students. Either way, great answers.) I think that the course, however, was too short and might have been more rigorous. (Though I do understand this was only a half an MIT course and probably simplified.) But I just admire professor Guttag as a person for being one of the first to get on board with MOOCs. This course was also a very different take on data science from the usual approach, as here you focus on simulations--e.g. behavior of different virus populations based on applications of different drugs/medicines and the genetically inherited (hence the OOP design) resistance to different active components of these drugs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; OOP, Simulations, Stochastic Algorithm Testing&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/cs215"&gt;Udacity Algorithms&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I finished this course before Udacity changed their business model from offering free courses with no deadlines to one where they only offer paid courses (though the material is still free, but you cannot get a certificate). This course was quite hard and the support was sparse (for a MOOC). But this meant that I just had to spend a bit more time thinking about the problem, which was probably helpful in the end to my general CS skills. I think that all the concepts were quite well explained and overall I think that there was sufficient practice to really understand the material. Though there was not so much of a 'course feel', which there was in both Coursera and EdX. But despite this, they recorded &lt;a href="https://www.youtube.com/watch?v=stdG3BGmhqo"&gt;probably the most awesome CS theory song in existence&lt;/a&gt;, so I cannot really question anything about this course. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Algorithms, Data Structures, Social Network Computation &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/ud032"&gt;Udacity Data Wrangling with MongoDB&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course is a bit different from all the others: you can either take it for free (but no certificate at the end) or you can pay $200 for each month and have a personal coach, human-verified project at the end and get that certificate. Partially, because I wanted to evaluate Udacity's new business model and partially because I knew that data wrangling is a huge part of data science, I finished this course in about month and a half. Surprisingly there is very little about MongoDB -- it is only introduced in the last 20% of the course, but that does not mean that the course does not teach you helpful skills. It does! Web scraping and XML parsing are definitely useful skills and I think that the course is appropriate in difficulty, though I think that it could be a bit harder at times to really force the students to organize their code well and debug more complex data cleaning parsers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned&lt;/strong&gt;: Website Scraping, Data Wrangling with Python and MongoDB&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/networksonline"&gt;Social and Economic Networks: Models and Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Normally economists (especially the really famous ones) are not too keen to join MOOCs, but that just makes &lt;a href="https://web.stanford.edu/~jacksonm"&gt;Matthew Jackson&lt;/a&gt; that much more awesome. This class was fun and allegedly a graduate level, though it must have been very diluted, because I found the material (although going to greater depth than the Michigan Course) quite easy. There was again some work with Gephi and this time even Pajek, though sadly no R. But I should note that there were more advanced versions of the course that one could participate in (sadly, with no recognition) that I wanted to participate in, but unfortunately I did not have the time. So I think potentially this could have been an 8/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; More computational and mathematical methods for social network analysis&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://online.stanford.edu/course/statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course covers an explanation of most of a recent book by Trevor Hastie, Robert Tibshirani and others: &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning, with Applications in R&lt;/a&gt; and some of their more advanced book, &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;. This course is fun to take, though I mentioned 'explanation' for a reason. This course will provide you with a thorough explanation of fairly advanced statistical concepts, but definitely will not test you rigorously enough. Usually, especially if you buy the discounted versions of one of the books, you will get enough knowledge and training to apply some of these concepts sensibly. But you will still need to work hard to test yourself and to really probe the ideas with no assistance. Regardless of whether you do, the lectures are really fun to listen to and explain concepts very well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, Intermediate Statistics, Computation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/initprogjava"&gt;Introduction à la programmation orientée objet (en Java)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I think I pretty much took this course, because programming in English is too mainstream. But it was really nice to learn a language from 0 in a very organized environment. What I genuinely appreciate is the walk-throughs on how to set up the Java environment in Windows, Ubuntu and Mac. But what I think was by far the best thing about this course was that the actual instructors were &lt;em&gt;so commonly&lt;/em&gt; involved on the forums, helping people. Even I once received some help from one of the instructors, which was great. Coding-wise, the MOOC was quite simple, but I assume approximately on the same level every other intro course for non-CS majors would be. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Java, French, Basic Data Structures&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/bigdata"&gt;IIT's Web Intelligence and Big Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was quite fun and relatively easy to grasp. The concepts revolved around the mathematics (and some programming) of the Big Data technologies, as well as their (recent) history. The problem sets were well done, though sometimes seemed bit too shallow and I would appreciate a bit more programming. The programming assignments were great: tested a whole array of important skills and technologies, but were only three through the entire course. This course was allegedly supposed to be taken as a half-course by actual IIT engineers. I, however, seem quite skeptical of that, as it seemed a bit too easy if true. But generally quite fun and insightful. Though the MOOC seemed to be a bit mass produced with little involvement from the IIT staff after a while.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Map-Reduce and other "big data algorithms" (locality sensitive hashing, page rank etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/introfinance"&gt;Introduction to Finance&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was my first MOOC I finished so I still remember quite a lot the odd details, as well as some of the catch-phrases the professor was using ("It's not about finance, it's about love.") as well as the fact that people (probably still stuck in the school mentality), were trying to ease their way through the course. I actually learned a lot of interesting concepts I did not know. With this said, I started this course in June 2012, which is just two or three months after Coursera launched. But that meant that some aspects of the course were quite not ready. The platform was great, but I feel like courses tended to be a bit better structured in the later courses. But the fact that I was trying out the first iteration also meant that right now the course could be a 10/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Basic accounting, basic financial modeling (but going beyond NPV etc.), Excel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/getdata"&gt;Getting and Cleaning Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was by the same Jeffrey Leek and Roger Peng that I admired so much and it was part of the data science specialization. But it was just not nearly as good as the previous two. And because by this point I knew that data science is 90% data cleaning and 10% complaining about data cleaning (I think I used this as a joke somewhere, but it &lt;em&gt;might&lt;/em&gt; not be my line, but I could not find the original source.), I wanted to learn more about how to do it efficiently. I wanted to try out the new specialization that the company that I so love and admire was spearheading. But the course was quite short, and although overall well-done, it was just not a very transformative experience. I think that I also really knew about 95% of this course, so I did not even watch most the lectures. Next week I always dived in in hopes of learning something new. I think for a beginner it could be a decent course, but I am still not sure if I would pay $49 for it. Though I understand the tradeoff Coursera was facing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; (potentially) Data wrangling, RegEx, &lt;code&gt;reshape&lt;/code&gt; and other useful packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 6/10&lt;/p&gt;
&lt;h1&gt;3. Good Courses&lt;/h1&gt;
&lt;p&gt;There good thing about the MOOCs is that if I don't like the course, the instruction seems crappy or I have hard time understanding the professors or I just do not like something, I can drop it. Moreover, there is so many great and awesome courses that I could not even do those that seemed 'good'--i.e. I would most likely like to take them if they were one part of the curriculum at my home institution. &lt;/p&gt;
&lt;h1&gt;Best practices&lt;/h1&gt;
&lt;p&gt;I think there are 3 key strategies to successfully finishing the MOOC. But the key insight is that most people who do not finish a MOOC usually do not finish it because of lack of time, not because of lack of aptitude. So lack of persistence is your key enemy. To crush your enemy, you have to:&lt;/p&gt;
&lt;h2&gt;1. Assess quickly the time commitment of a course.&lt;/h2&gt;
&lt;p&gt;I still remember that &lt;a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/about"&gt;convex optimization course&lt;/a&gt; by Stanford that I really wanted to take. But, alas, I could not. I did not have the time as I had a busy schedule &lt;em&gt;and&lt;/em&gt; I was doing another MOOC (or two?) at the time. I realized that this was a rigorous class that required substantial time commitment. But I needed to drop it, because if you do not take each commitment seriously, you will probably not finish any of them.&lt;/p&gt;
&lt;h2&gt;2. Honor your commitments&lt;/h2&gt;
&lt;p&gt;Meaning also that you should immediately unenroll from the courses you know you cannot finish. This will hopefully also give you the respect towards your commitment that you will also finish the ones you have elected to keep.&lt;/p&gt;
&lt;h2&gt;3. Commit to deadlines, minimum benchmarks&lt;/h2&gt;
&lt;p&gt;There is a reason why I only finished one free course on Udacity: because there are no deadlines (well, until they introduced the payment by the month). These help you get stuff done, because now you have to or all (or at least something) is lost. All major platforms have it now, so please use it.&lt;/p&gt;
&lt;p&gt;Benchmarks are also very helpful: there were courses (like both of the SNA ones) where I knew I wanted a distinction, because I wanted to know the stuff &lt;em&gt;well&lt;/em&gt;. Then there were courses, where I just wanted a brief overview so I only watched lectures (not listed, because I technically, haven't finished them). But decide quickly what your goals are and then follow them.&lt;/p&gt;
&lt;p&gt;I would also love to do a general post on MOOCs, but I think things are best kept separate. &lt;!-- Generally, I think MOOCs will fundamentally change the nature of education (for reasons that deserve a separate post), _provided_ that they manage to meaningfully integrate themselves to people's lives. We could already see many interesting initiatives like the one by &lt;a href="https://www.edsurge.com/n/2014-11-19-free-coursera-pd-thanks-to-the-president"&gt;Obama for teachers for their professional development&lt;/a&gt;. But currently MOOCs are mostly a domain of people who are happy to sacrifice some degree of social life for learning (totally worth it). Though I do not doubt there are exceptions, it seems that most people I know (and online statistics confirm this) are too busy to actually finish the courses.  --&gt;&lt;/p&gt;
&lt;p&gt;Anyway, hope this was helpful. As always, I would welcome any reaction. Get in touch!&lt;/p&gt;</summary><category term="R"></category><category term="education"></category><category term="python"></category><category term="reviews"></category></entry><entry><title>On Randomness in (Social) Science</title><link href="jakublangr.com/underspecification-socsci.html" rel="alternate"></link><published>2014-12-06T04:00:00+00:00</published><author><name>Jakub Langr</name></author><id>tag:,2014-12-06:jakublangr.com/underspecification-socsci.html</id><summary type="html">&lt;p&gt;A while back I was mentioning &lt;a href="/hello-world.html"&gt;how I was confused by the randomness as an emergent property of human systems&lt;/a&gt;, and so I have been giving this some thought, which I thought I might share with the world. The underlying problem always has been why is &lt;a href="http://plato.stanford.edu/entries/properties-emergent/"&gt;emergent&lt;/a&gt; randomness a good model of human behavior. At some level, we understand that our actions are &lt;a href="http://en.wikipedia.org/wiki/Determinism"&gt;deterministic&lt;/a&gt; at least on above-quantum level and I really dare not make any statements about the quantum nature of the universe. &lt;/p&gt;
&lt;p&gt;Before I start the discussion it is worth pointing out that I will try to make this as intuitive as possible, but some math will have to be involved, though I tried to make it so that it is not necessary to understand everything to really understand the point of this article. It was included to make the reasoning a bit easier. At the same time I am making some imprecise statements for the sake of understanding, but if you feel I might be leading people astray, feel free to drop me a line.&lt;/p&gt;
&lt;p&gt;The very definition of randomness is lack of predictability. That seems almost circular, but first thing to note here is that does not mean that all outcomes are equally likely. Think about a pair of dice, and our random variable ("what we measure") as the sum of the two rolls. Here there are seven has six possible combinations (6+1,5+2,3+4,4+3,2+5,1+6), giving us approximately normal distribution centered at seven:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;COUNT           NUMBER OF       PROBABILITY
                COMBINATIONS    
2               1               2.78%
3               2               5.56%
4               3               8.33%
5               4               11.11%
6               5               13.89%
7               6               16.67%
8               5               13.89%
9               4               11.11%
10              3               8.33%
11              2               5.56%
12              1               2.78%
Total           36               100%
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A better definition of randomness is that given an infinite amount of trials of random variable &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, we get the true distribution where the outcome of the next realization of said random variable is still unpredictable. So say you are throwing some dice in a casino and you are trying to figure out what is the bet that makes sense where in this odd casino you are meant to bet on the sum of the two dice. The number actually rolled will be a realization  (what you actually rolled) of &lt;span class="math"&gt;\(Y\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;. Any given roll will be unpredictable -- otherwise you could easily bet on it -- but it is deterministic -- i.e. if at the point of the throw you had a lot of time and all the laws of physics you could say with 100% certainty what the roll it is going to be. Yet right now, you do not know. However, that does not mean that it does not follow a certain distribution even though it is unpredictable. &lt;/p&gt;
&lt;p&gt;Now there is no doubt that if you come across any given pair of dice and you are asked what do you think the next number will be 7 should be your top choice. (Because it minimizes likely how far off the true number you are likely to be, both because it is in the middle and because it is most likely.) That is what can be thought of as expectation. However, generally the expectation lies in the middle of the distribution, but need not be the most common value. This becomes more obvious if we look at the formula for expectation. It is basically a weighted average of the value (&lt;span class="math"&gt;\(y\)&lt;/span&gt;) and the frequency (&lt;span class="math"&gt;\(f(y)\)&lt;/span&gt;). (Where &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; are the boundary points.)&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
E[Y] = \sum_{a=2}^{b=12} y f(y) \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,  \,\,\,\,\,\,\,\,\,
\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;In theory, we can even have a distribution where the expected value is impossible, such as the one below: (for a concrete example imagine student's ratings of a &lt;em&gt;really&lt;/em&gt; controversial class on a scale from -15 to +15, where the two groups are equally split and the average rating is about zero)&lt;/p&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/download.png" alt="Distribution" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;If you look at the formula for expectation above it is all very nice and mathematical. We have our best guess that makes sense, but is far from guaranteeing that we are actually walk back home with a lot of money. But what if you could observe rolls of two dice would they be perfectly distributed according to the distribution that we have specified? Well, no. Dicemakers probably tried to make them as balanced as possible, but probably every pair of dice has certain numbers that will come less or more commonly than simple combinatorics would suggest. This deviations might be due to tiny imperfections on the table, the dice themselves or many other factors. If you find statistically significant relationships, we might be able to derive a marginally better model. But it is only going to do as well as the strength of relationships. Moreover, if we find these associations and do not really understand why -- e.g. we just observe, they might backfire, because if maybe we found some irregularities on the table and then we start throwing on other parts of the table, we will start losing more. Our model did not generalize "out of sample" (on what we did not observe) well.&lt;/p&gt;
&lt;p&gt;Now bring this back to social science. Some people choose to think of the economy as some sort of machine, which suggests determinism. Let's accept that for a second. &lt;/p&gt;
&lt;p&gt;Even if we do we have two problems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The underlying nature of the system keeps on changing. Sure, economy &lt;em&gt;can&lt;/em&gt; be a machine, but it will be a different machine in March 2014 and different in April. &lt;/li&gt;
&lt;li&gt;It does not actually explain why there is any randomness. A machine is deterministic, so shouldn't our predictions be too? It turns out that it is the emergent property (or rather the fact that we have incomplete information) that makes it random. So yes, the world may be deterministic but we need a lot more information that we ever have. Hence we are trying to abstract models that capture the key information. So in our dice example we can notice that the side with one is heavier than the others and hence might come up marginally less often and even though the dice roll is always deterministic, the best heuristic, if we cannot exactly calculate everything, is to use the base rate (how often does 7 naturally occur) plus whatever extra information we may have (heavier sides?).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But randomness only really makes sense if we are actually trying to predict something. We are essentially saying that there is a part that our model (the base rate and extra information together) can predict [1] and a part that our model cannot explain -- i.e. the error term [2]. Basically whenever we don't get our prediction exactly right we get an error -- the difference between actual and expected value. So what we are basically saying is that at this level we can only explain so much and the predicted value is our best guess. That is, after all, what we are doing intuitively with the dice as well: we all intuitively know that predicting always that the outcome is going to be 3 is not a good strategy, unless we know that the dice are biased towards them.&lt;/p&gt;
&lt;p&gt;So really, this problem is not unique to social science, but rather inference in general. Because all of the extra information we have probably included in our prediction. Think about predicting rain: to have rain you need clouds, hence if there is a huge black cloud over your head your prediction with this extra information will already be say 90%, but that is only because we have already built the model (we know) that if there's a could over our heads it it is likely to rain. The fact that it does was not a trivial insight at some point. This is what we are doing in science, except with much more difficult concepts. Hence, randomness has to be an emergent property of &lt;em&gt;any&lt;/em&gt; predictive model with incomplete information. And since we make predictions so often in every science and the error term has to be random (otherwise we could easily figure out a better model), which is why all the good models of complex behavior have to have a random part. But always keep in mind that the distribution of errors can vary, but there is always going to be a random part by definition unless we know the true model (i.e. we have complete information).&lt;/p&gt;
&lt;p&gt;I hope that this was helpful and that you know at least sort of understand why randomness is not a property of human systems, but of prediction with incomplete information and that there are different forms that randomness can put on. But I would love to hear your thoughts and feedback, let me know!&lt;/p&gt;
&lt;p&gt;[1] usually denoted as &lt;span class="math"&gt;\(E(Y | {\bf X } )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[2] usually denoted as &lt;span class="math"&gt;\( \epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt;, though it can be distributed according to a wide variety of distributions.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="technical"></category><category term="social science"></category></entry><entry><title>Tracking the CrISIS in Syria</title><link href="jakublangr.com/tracking-crisis.html" rel="alternate"></link><published>2014-09-22T21:00:00+01:00</published><author><name>Jakub Langr</name></author><id>tag:,2014-09-22:jakublangr.com/tracking-crisis.html</id><summary type="html">&lt;p&gt;If you are interested more in the analysis of the conflict from a quantitative persective and not so much in the methodology, you &lt;a href='#explore'&gt; can jump straight there.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As I was working as a contractor this summer, I came across a very interesting project called &lt;a href="http://www.gdeltproject.org/"&gt;GDELT&lt;/a&gt; -- Global Database of Events, Language and Tone. This project I think is best described by the official website statement: "GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world."&lt;/p&gt;
&lt;p&gt;In addition to that, as a part of &lt;a href="../hello-world.html"&gt;Good Judgement Project&lt;/a&gt;, I was made to forecast on Syria and ISIS' actions. As I was going through the articles describing what is happening in Syria, I became to wonder: what is the bigger picture? To what extent is this hype all just because &lt;strong&gt;now&lt;/strong&gt; ISIS is the headline-grabber in the West? (But of course, no one cares until it becomes a story in the public consciousness.) That's when GDELT comes into the picture. I wanted to try it as I came across it, but since I was finishing my data science project, I did not quite have the time. I was awe-struck by the complexity and potential of it so I knew I wanted to give a go. I have not really used R all that extensively in about a year since my internship, but I am not afraid of large analytics, so I decided to refresh those skills.&lt;/p&gt;
&lt;p&gt;Fortunately, Google was so nice as to host this almost 100 GB dataset on &lt;a href="http://googlecloudplatform.blogspot.cz/2014/05/worlds-largest-event-dataset-now-publicly-available-in-google-bigquery.html"&gt;its cloud for free&lt;/a&gt;, for which I love Google. Not only that, but Google also loaded this dataset so that it can be queried using &lt;a href="http://en.wikipedia.org/wiki/BigQuertracky"&gt;BigQuery&lt;/a&gt; using public-facing APIs or a web interface. We can then query GDELT using SQL-like language as shown below and subsequently export the dataset as a CSV:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gdelt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bq&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;full&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; 
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MonthYear&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;201303&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;2015&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;NumSources&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;GoldsteinScale&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt;
&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Actor1Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(For those interested, I include a smaller version of the dataset I was working with after exporting from GDELT &lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/sample.csv"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The assumption of this endeavor is that if I include non-English media (and perhaps the less main-stream ones too) by using GDELT's information rather than Google, which will always give me traditional sources, I can get a better sense of what is going on. Thanks to the the law of large numbers, I should not get too many false positives from spurious sources, so my overall analysis should remain robust and even more objective than relying on sources such as CNN or BBC alone. One additional advantage GDELT has is that it is updated every 24 hours. In theory, this analysis could be automated so that it updates itself every 24 hours. (Actually, it should not be &lt;em&gt;that&lt;/em&gt; much more work ... but given the size of GDELT, I would have to start paying for using the BigQuery service that often.)&lt;/p&gt;
&lt;p&gt;As with any more substantive project, there was a lot of data cleaning and obstacles. Namely: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ISIS not recognized by GDELT&lt;/strong&gt;:
There's lots of International Militarized Groups (IMGs) recognized by GDELT, but ISIS seems to be too new in the headlines to be recognized by GDELT coding yet. Hence I had to look for substitutes:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMG (International Militarized Group) tag to the rescue&lt;/strong&gt;:
There is a column specifying actor &lt;em&gt;type&lt;/em&gt; (not actor name!) for each event. There is an actor type IMG, which was fortunate. However, there is no way of fully knowing whether said group is ISIS or not from this dataset. (At least with this level of granularity, but I am sure it would be technically possible, if GDELT team so chose.) I would also imagine that the fighters on the ground themselves are often not quite sure, given the number of splinter groups these organizations harbor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;But sometimes "REB"&lt;/strong&gt;:
When individually inspecting the events (just some of them, I do not have the time to go through the entire data dump), I quickly realized that the data pertaining to clearly ISIS attacks are sometimes labeled as "REBEL" actor type. So I needed to include both in the analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cleaning the data&lt;/strong&gt;:
For some events it is hard, if not impossible, to correctly specify precise coordinates. As a result, GDELT merely inputs the values of the approximate center of Syria. These records obviously needed to be removed from the dataset. Obviously, there was couple of more steps that I needed to do and a couple that tricked me for a little bit, for instance, when I found that the state code of one of the events was &lt;em&gt;IS&lt;/em&gt;, which, however, stood for Israel, as I later found out.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GDELT project classifies each event using &lt;a href="http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt"&gt;one of their internal codes&lt;/a&gt;, each of these codes then gets then a &lt;a href="http://web.pdx.edu/~kinsella/jgscale.html"&gt;Goldstein score&lt;/a&gt;, which basically expresses whether this event whether people are being nice to each other (positive score) or mean/evil towards each other (negative score). Where +10.0 is some massive humanitarian/developmental aid project, +1.0 is probably visit of some minor national politician. All the positive things were filtered out, because I wanted to focus on what the problems were. There -1.1 is, for instance, "Deny an attributed policy, action, role or position" and -10.0 is "Military attack; clash; assault".&lt;/p&gt;
&lt;p&gt;As the more SQL-knowledgeable of you might have noticed, I only considered events that came from 2 independent sources and happened on or after March 2013. &lt;/p&gt;
&lt;h1 id='explore'&gt; Let's explore &lt;/h1&gt;

&lt;p&gt;With this dataset in hand, I made several maps where I plotted each event as a point sized according to its Goldstein score and set relatively low transparency so that all hotspots appear in bright red. I plotted this dataset by month, and here's March 2013 till July 2013. We see that at the end of this period, there's was violence spearing from its initial hotspots. &lt;/p&gt;
&lt;h3&gt;March and April 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201303.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201304.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;May and July 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201305.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201307.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Now, if we look at the period August till November, we see that there's a lot more violence then. But to some extent we have to bear in mind that this dataset includes &lt;em&gt;all&lt;/em&gt; rebels and not just ISIS. &lt;/p&gt;
&lt;h3&gt;August 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201308.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;September 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201309.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;October 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201310.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;November 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201311.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we have to consider the connected events: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=Syria,+Chemical+Weapons&amp;date=1/2013+12m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Here, I am using Google Trends as a proxy for "Western interest" in the matter. We can see that the use of chemical drew Western attention to Syria, however, this interest was very short-lived.&lt;/p&gt;
&lt;p&gt;Clearly, August and September are easily explained by the usage of chemical weapons. But the violence following this, especially in October and later, cannot be explained just by looking at chemical weapons, because that is when the hype ended and has not picked up until ISIS was the next important thing.&lt;/p&gt;
&lt;p&gt;The following months were similarly violent and few in the West seemed to care. &lt;/p&gt;
&lt;h3&gt;December 2013 and January 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201312.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201401.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;February and March 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201402.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201403.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;April and May 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201404.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201405.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;If we plot the Google trend graph again, since the start of Syrian Civil War in March 2011, we see that the attention Syria got around the times of the use of chemical weapons is unparalleled. This is hardly surprising, but what is notable is that there a &lt;a href="http://www.cbsnews.com/news/syria-chemical-weapons-attack-blamed-on-assad-but-wheres-the-evidence/"&gt;disagreement regarding who actually used them&lt;/a&gt;, though I think that at least some of these attacks were sure to be made by the government. &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=Syria&amp;date=1/2009+43m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_1&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;But despite the limited interested by the West, violence was actually similar to the months during which the chemical weapons were used. &lt;/p&gt;
&lt;p&gt;What is interesting that ISIS was not picked up by Western media much until recent months, despite the fact that ISIS can be traced back to 2004: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=%22Islamic+State%22,+ISIS,+ISIL&amp;date=today+12-m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Still, violence was hovering around the same level:&lt;/p&gt;
&lt;h3&gt;June and July 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201406.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201407.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;August and September 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201408.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201409.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;It is interesting that it seems that despite the limited budget that ISIS has for its media operations, its terror tactics are largely successful: Syria is not, at least by according to this analysis, by and large, more violent than it was in the few months prior to ISIS' inclusion in the Western media's limelight. &lt;em&gt;Yet&lt;/em&gt;, ISIS get substantial amount of attention, so much so that it prodded United States and &lt;em&gt;five Arab&lt;/em&gt; states (no European states, however, at the time of writing) to launch a military operation starting today. This article used a lot of proxies so I do not want to make grandiose claims that cannot be supported by this analysis.&lt;/p&gt;
&lt;p&gt;Nevertheless, I think what this article shows that ISIS first and foremost is unparalleled by its usage of social media compared to other terrorist organiztations and splinter groups. If anything, it seems that the number of violent occurrences has decreased. But the terror-tactics and the headline-grabbing strategy of ISIS is already working well for them. West pays attention. &lt;/p&gt;
&lt;p&gt;At the same time, the brutality and the casualities is hard to really compare, because the information about the previous comparable systems (e.g. &lt;a href="http://en.wikipedia.org/wiki/Islamic_Emirate_of_Afghanistan"&gt;Islamic Emirate of Afghanistan&lt;/a&gt;) is very limited and we might just never get as complete picture as we have now. But now, largely thanks to GDELT, we have a much more complete dataset about the conditions within the Islamic State and so it might prove easier to benchmark the level of violence and discord in this society against all other -- be it present or future -- states. &lt;/p&gt;
&lt;p&gt;I am currently working on an interactive R-Shiny app for this visualization/analysis so stay tuned!&lt;/p&gt;
&lt;p&gt;Want the R script? Want to give feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</summary><category term="modeling"></category><category term="GDELT"></category><category term="R"></category><category term="Syria"></category></entry><entry><title>Predictions R hard</title><link href="jakublangr.com/hello-world.html" rel="alternate"></link><published>2014-09-14T21:00:00+01:00</published><author><name>Jakub Langr</name></author><id>tag:,2014-09-14:jakublangr.com/hello-world.html</id><summary type="html">&lt;h1&gt;... or quick and dirty statistical modeling in R&lt;/h1&gt;
&lt;p&gt;Recently, I decided to start a blog and my ongoing involvement in the &lt;a href="http://goodjudgmentproject.com/"&gt;Good Judgment Project (GJP)&lt;/a&gt;, which is part of US government's &lt;a href="http://www.iarpa.gov"&gt;Intelligence Advanced Research Projects Activity (IARPA)&lt;/a&gt;, more specifically the ACE Project, proved to be a decent excuse to do so. &lt;/p&gt;
&lt;p&gt;I have joined GJP mid-Season 3 (hence I haven't participated fully) and then I tested into season 4 (combination of general intelligence test and political knowledge). This test alone was a treat, because it followed all the guidelines about best practices of hiring (to my knowledge) and I knew that if I get in, this will be a lot of fun. What I did not know at the time was that only one in ten will actually be accepted into GJP, otherwise I would probably put in a lot more work into the test.&lt;/p&gt;
&lt;p&gt;Despite this mess-up, I joined a project and met my amazing fellow forecasters in my team. At the time of joining, I already knew that I will be putting numeric probabilistic values on how the world might soon turn out to be (e.g. will there be a direct Russia-Ukraine confrontation in Crimea). All of these questions are well-defined using a very sophisticated set of definitions, but making the evaluation criteria very flexible and as to allow GJP Team to 'resolve' each question in the spirit rather than the letter of the question.&lt;/p&gt;
&lt;p&gt;Number of these questions have allowed for use of publicly available datasets and so I would like to offer some code that I used to ease up my work as a forecaster. This particular example is relates to a question that resolved yesterday: If on a certain date the area of ice on the Arctic sea will be more than what it was last year.&lt;/p&gt;
&lt;p&gt;The following R code downloads, cleans and plots some time-series representation of the record ed data of the ice for the past couple of years. It is &lt;em&gt;super-simple&lt;/em&gt;, but I was able to do it from question to quantitative probabilities in about 30 minutes, so this is just to demonstrate that even such simple code can help you make a much more solid prediction. This was necessary, because my team's forecasts were all over the place (among 9 forecasters we ranged 5-80%) and statistical modeling is a great way how to deal with data that is normally quite counter-intuitive and complex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;TTR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reshape&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tseries&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;forecast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Loading, cleaning (missing values are by default assigned value of -9999)&lt;/span&gt;
read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ijis.iarc.uaf.edu/seaice/extent/plot_v2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
&lt;span class="kp"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-9999&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
&lt;span class="kp"&gt;colSums&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Melt changes the columns (there is a column for each year) into just one column &amp;#39;value&amp;#39;&lt;/span&gt;
data_ts &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; melt&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
ts &lt;span class="o"&gt;=&lt;/span&gt; ts&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt; deltat&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data_ts &lt;span class="o"&gt;=&lt;/span&gt; na.remove&lt;span class="p"&gt;(&lt;/span&gt;ts&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Decomposes the time-series plots the decomposition and the forecast (as well as prints it)&lt;/span&gt;
decomposed_ts &lt;span class="o"&gt;=&lt;/span&gt; decompose&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;decomposed_ts&lt;span class="p"&gt;)&lt;/span&gt;
ts_forecast &lt;span class="o"&gt;=&lt;/span&gt; HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt;gamma&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts &lt;span class="o"&gt;=&lt;/span&gt; forecast.HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;ts_forecast&lt;span class="p"&gt;,&lt;/span&gt;h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts
plot.forecast&lt;span class="p"&gt;(&lt;/span&gt;forecasts&lt;span class="p"&gt;)&lt;/span&gt;
data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;250&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;260&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For GJP purposes, the confidence intervals produced by this model are especially useful, because they give you an idea of how confident you can be in your predictions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; forecasts 
Point Forecast Lo &lt;span class="m"&gt;80&lt;/span&gt; Hi &lt;span class="m"&gt;80&lt;/span&gt; Lo &lt;span class="m"&gt;95&lt;/span&gt; Hi &lt;span class="m"&gt;95&lt;/span&gt; 
&lt;span class="m"&gt;16.73710&lt;/span&gt; &lt;span class="m"&gt;4981786&lt;/span&gt; &lt;span class="m"&gt;4920648&lt;/span&gt; &lt;span class="m"&gt;5042924&lt;/span&gt; &lt;span class="m"&gt;4888284&lt;/span&gt; &lt;span class="m"&gt;5075288&lt;/span&gt; 
&lt;span class="m"&gt;16.73995&lt;/span&gt; &lt;span class="m"&gt;4963324&lt;/span&gt; &lt;span class="m"&gt;4867154&lt;/span&gt; &lt;span class="m"&gt;5059494&lt;/span&gt; &lt;span class="m"&gt;4816245&lt;/span&gt; &lt;span class="m"&gt;5110403&lt;/span&gt; 
&lt;span class="m"&gt;16.74279&lt;/span&gt; &lt;span class="m"&gt;4944862&lt;/span&gt; &lt;span class="m"&gt;4814956&lt;/span&gt; &lt;span class="m"&gt;5074768&lt;/span&gt; &lt;span class="m"&gt;4746188&lt;/span&gt; &lt;span class="m"&gt;5143536&lt;/span&gt; 
&lt;span class="m"&gt;16.74564&lt;/span&gt; &lt;span class="m"&gt;4926399&lt;/span&gt; &lt;span class="m"&gt;4762199&lt;/span&gt; &lt;span class="m"&gt;5090600&lt;/span&gt; &lt;span class="m"&gt;4675277&lt;/span&gt; &lt;span class="m"&gt;5177522&lt;/span&gt; 
&lt;span class="m"&gt;16.74849&lt;/span&gt; &lt;span class="m"&gt;4907937&lt;/span&gt; &lt;span class="m"&gt;4708313&lt;/span&gt; &lt;span class="m"&gt;5107562&lt;/span&gt; &lt;span class="m"&gt;4602638&lt;/span&gt; &lt;span class="m"&gt;5213237&lt;/span&gt; 
&lt;span class="m"&gt;16.75134&lt;/span&gt; &lt;span class="m"&gt;4889475&lt;/span&gt; &lt;span class="m"&gt;4653078&lt;/span&gt; &lt;span class="m"&gt;5125872&lt;/span&gt; &lt;span class="m"&gt;4527937&lt;/span&gt; &lt;span class="m"&gt;5251014&lt;/span&gt; 
&lt;span class="m"&gt;16.75419&lt;/span&gt; &lt;span class="m"&gt;4871013&lt;/span&gt; &lt;span class="m"&gt;4596412&lt;/span&gt; &lt;span class="m"&gt;5145614&lt;/span&gt; &lt;span class="m"&gt;4451047&lt;/span&gt; &lt;span class="m"&gt;5290979&lt;/span&gt; 
&lt;span class="m"&gt;16.75704&lt;/span&gt; &lt;span class="m"&gt;4852551&lt;/span&gt; &lt;span class="m"&gt;4538291&lt;/span&gt; &lt;span class="m"&gt;5166811&lt;/span&gt; &lt;span class="m"&gt;4371932&lt;/span&gt; &lt;span class="m"&gt;5333170&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the plots here can help elucidate why my team might have been all over the place: there are many factors influencing how much ice there will be on a given day: sure, there is the seasonal (summer-winter) cycle, which matters a lot, but there is also some random element as well as measurement error. I fear that lots of my fellow forecasters (perhaps including myself for a long time) fell victim to &lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;over-fitting&lt;/a&gt; to the random element. Sure, we could observe the pattern from the previous year, but no one from the team--to my knowledge--plotted the data to see what (approximate) influence each of the two factors have. This is why decomposition is so great:&lt;/p&gt;
&lt;h2&gt;Decomposed time-series plot&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/gjp_modelling_ts_decomposition.svg" alt="Timeseries decomposition of ice-area" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;One interesting element of the decomposition is the trend, which I was expecting to see fairly clear evidence of global warming, but instead I really found out "just" evidence of climate change. Though, of course, I am no climatologist, so perhaps that interesting dip in the year 2000 had some other (spurious?) reasons. &lt;/p&gt;
&lt;p&gt;The decomposition, although helpful, was not what I considered the main advantage of this little exercise: I was then able to use a time-series statistical model called HoltWinters to predict the area of ice (no pun intended?), which uses exponential smoothing that is quite fitting in these conditions (I wonder whether these statistical puns significantly affect my readership?). This model is appropriate mainly because there is a lot of historical data and the data is always very constrained to a certain range (e.g. a 3 sigma change is &lt;em&gt;actually&lt;/em&gt; extremely unlikely, which cannot be said of all financial data this model is applied to). &lt;/p&gt;
&lt;h1&gt;Thoughts about GJP itself&lt;/h1&gt;
&lt;p&gt;GJP is what I think social science should be: quantifiable, based on best-practices and aiming to get as close to the truth as possible, not tell the best story. This was, interestingly enough, the conclusion (&lt;em&gt;simplifying tremendously&lt;/em&gt;) of Professor Philip Teltock in &lt;a href="http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715"&gt;his book Expert Political Judgment&lt;/a&gt;. Namely that the political forecasters that tell the best story are usually favored by the media, because it sells well, but they tend to do worse than most others on accuracy. Professor Tetlock is now one of the main organizers of GJP and so it is a great honor to work as his lab rat, because it helps to do cutting-edge research and it helps me to learn a ton in the process.&lt;/p&gt;
&lt;p&gt;One additional gift every forecaster got from the GJP was a set of presentations of best practices of forecasting, which I think were fascinating, albeit oftentimes simple guidelines to stick to. But as with everything, it is much more a matter of putting these into practice rather that makes a great forecaster. &lt;/p&gt;
&lt;p&gt;Couple of interesting things that I noticed about myself looking at old predictions: &lt;/p&gt;
&lt;h2&gt;1. I am overly cautious&lt;/h2&gt;
&lt;p&gt;Reading loads of literature about randomness and its role in human society (Kahneman, Taleb, Tversky, Ariely, Mlodinow etc.) I was overly careful about putting down anything close to 1% or 99%, even though some questions call for it (likelihood of a massive reform of international institutions in a short timespan). Good Judgment, after all, is not only very well calibrated, but it is also very discriminating. &lt;/p&gt;
&lt;h2&gt;2. I struggle with randomness&lt;/h2&gt;
&lt;p&gt;Interesting point about randomness--to some extent related to the previous one--how do we square the fact that randomness is just a model and yet we keep on invoking the properties of randomness when talking about the world. But, at least on the level of humans, each action is supposedly deterministic. Yes, some might call on the &lt;a href="http://www.quantumdiaries.org/2014/07/04/wrong/"&gt;famous George Box quote&lt;/a&gt;, but that does not really answer why in a situation of imperfect information &lt;em&gt;random&lt;/em&gt; is the &lt;em&gt;best&lt;/em&gt; model. This is especially confusing given the amount of strategic interaction and social influence in the world. &lt;/p&gt;
&lt;p&gt;This turned out to be a problem with with some of the GJP predictions I make, because I want to stick to randomness as a baseline (because it seems to be best practice), but then struggle to always justify this to myself and square it with the rest of information. This especially bothered me outside of GJP, however. In Mlodinow's book &lt;a href="http://www.amazon.com/The-Drunkards-Walk-Randomness-Rules/dp/0307275175"&gt;The Drunkard's Walk&lt;/a&gt;, he specifically used the example of production studio's CEOs as someone who is faced with great randomness in the movies. But each action in this immensely complex chain of interactions is deterministic, strategic and influenced by one's surroundings. Why is randomness an emergent property of human systems? Seems non-obvious.&lt;/p&gt;
&lt;h2&gt;3. Conditional interaction is hard&lt;/h2&gt;
&lt;h3&gt;(especially cross-culture)&lt;/h3&gt;
&lt;p&gt;There was a question on what would DPRK do, should the United States take a bold, but merely supportive, military action in aid of South Korea. Our team struggled to agree even if the bold action would increase chance of state DPRK being more aggressive towards South Korea or less so. Would the desire to prove something to US (or more generally, the West) outweigh the potential risks, because of the importance of the precedent or would the bold action scare off DPRK? Both ways of reasoning sounded equally plausible to me at the time so I just followed what my initial prediction was. But really, there was no reason for it. The question whether I got it right or wrong is irrelevant in this case; the question is: would I be able to make a good prediction next time? I fear that so far the answer is no and so I never swayed too far off 50%. I will probably need to devise a way of breaking these ties.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</summary><category term="modeling"></category><category term="R"></category><category term="statistics"></category><category term="GJP"></category></entry></feed>