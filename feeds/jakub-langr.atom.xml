<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Langr's Blog - Jakub Langr</title><link href="jakublangr.com/" rel="alternate"></link><link href="http://www.jakublangr.com/feeds/jakub-langr.atom.xml" rel="self"></link><id>jakublangr.com/</id><updated>2017-09-10T14:00:00+01:00</updated><entry><title>Coding for GANS &amp; (Semi)-Learning</title><link href="jakublangr.com/gans-code.html" rel="alternate"></link><published>2017-09-10T14:00:00+01:00</published><updated>2017-09-10T14:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-09-10:jakublangr.com/gans-code.html</id><summary type="html">&lt;p&gt;As it often happens, I get busy at work and forget to publish something I should have really done months ago. Well, this code is that thing. But at least it's now nice and shiny.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;I'll jump straight into what we have explained on a high-level &lt;a href="http://jakublangr.com/gans-tutorial.html"&gt;last time&lt;/a&gt;. The code is also available on &lt;a href="https://github.com/jakubLangr/Gans-Semi-Supervised/blob/master/gans_semi_supervised_learning.ipynb"&gt;GitHub&lt;/a&gt; and on &lt;a href="https://medium.com/@james.langr"&gt;Medium&lt;/a&gt;. This part is identical to the Jupyter notebook, except it is lacking the code output.&lt;/p&gt;
&lt;h2&gt;Generative Adverserial Networks &amp;amp; Semi-Supervised Learning&lt;/h2&gt;
&lt;h1&gt;By Jakub Langr (originally written March 2017)&lt;/h1&gt;
&lt;p&gt;This code was written for me to experiment with some of the recent advancements in AI. I chose specificially semi-supervised learning and Generative Adverserial Networks (GANs) to push myself. Some of the code was done as a homework for the &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv"&gt;Creative Applications of Deep Learning Course&lt;/a&gt;, which was extremely helpful in helping me learn about modern AI. Some of the broad framework came as pre-coded set-up and explanations for the last part of the course by &lt;a href="https://www.linkedin.com/in/pkmital"&gt;Parag Mital&lt;/a&gt;, but this usage of his code is completely novel and took a lot of engineering, stitching together and abstractions.
In this Jupyter Notebook I do the following things:
1. Import all the necessary dependencies (as well as some that I just used during development but not in final version)
2. Use a GAN approach to generate synethic images. 
    + More specifically, &lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"&gt;this recently extremely popular unsupervised technique&lt;/a&gt; can learn the higher representations of what constitutes a human face (along with many attributes in the latent space) on the &lt;a href="mmlab.ie.cuhk.edu.hk/projects/CelebA.html"&gt;Celeb Dataset&lt;/a&gt; by competing another network to fool each other (explained later)
    + Alternatively, one can think of this approach as using an auto-encoder-style gene generative model that tries to generate new examples based on a seeding factor.
3. This seeding factor or 'latent feature space' invariably encode some aspects of the generative models and once understood, can be used to predictiably manipulate the nature of generated images--e.g. baldness, gender or smile. 
4. We can therefore generate an almost infinite supply of new examples and because we know how we manipulate the latent space, we can know their labels. In this example, we created 40,000 of Men and Women faces that can now be used for further training
5. Then we train the next layer classifier on the synthetic data for a binary classification of men or women faces.  Instead of training a new classifier from scratch, however, we use a &lt;code&gt;transfer learning&lt;/code&gt; approach using Oxford's &lt;code&gt;Visual Geometry Group&lt;/code&gt; or &lt;code&gt;vgg16&lt;/code&gt; pre-trained network to get higher accuracy without having to training for days on a massive cluster.
6. We use the different &lt;code&gt;vgg16&lt;/code&gt; Celebrity face predictions (&lt;code&gt;2623&lt;/code&gt; to be exact) and train a simple fully connected two-layer neural network on the synthetic examples with the labels. (This is in stead of the typical transfer learning approach that cuts off the last layer and trains on those. Here we simply split that into 2 steps)
7. Use the 100 hand-labelled (by me) examples to evalute the accuracy of the new classifier.&lt;/p&gt;
&lt;h3&gt;Motivation&lt;/h3&gt;
&lt;p&gt;This is really exciting because it allow us to train classifier with having virtually no labelled data as long as we have lots of unlabelled data, &lt;a href="http://jakublangr.com/ai-2016-review.html"&gt;which is a tremendously promising strategy especially for smaller companies with smaller datasets&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Brief definition of terms:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised learning&lt;/strong&gt;: is basically using unlabelled data in addition labelled data during the traing process&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generative Adverserial Netwokrs&lt;/strong&gt;: explained in detail below&lt;/p&gt;
&lt;p&gt;The code was done in &lt;code&gt;Tensorflow 1.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# First check the Python version&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version_info&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;You are running an older version of Python!&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;You should consider updating to Python 3.4.0 or&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;higher as the libraries built for this course&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;have only been tested in Python 3.4 and higher.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Try installing the Python 3.5 version of anaconda&amp;#39;&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;and then restart `jupyter notebook`:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;https://www.continuum.io/downloads&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now get necessary libraries&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tflearn&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;joblib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delayed&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage.transform&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="kp"&gt;resize&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skimage&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.misc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;imresize&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.ndimage.filters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gaussian_filter&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ipyd&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;libs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset_utils&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nb_utils&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Make sure you have started notebook in the same directory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;as the provided zip file which includes the &amp;#39;libs&amp;#39; folder&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;and the file &amp;#39;utils.py&amp;#39; inside of it.  You will NOT be able&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;to complete this assignment unless you restart jupyter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;notebook inside the directory created by extracting&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;the zip file or cloning the github repo.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We&amp;#39;ll tell matplotlib to inline any drawn figures like so:&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ggplot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a name="part-1---generative-adversarial-networks-gan--deep-convolutional-gan-dcgan"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Generative Adversarial Networks (GAN) / Deep Convolutional GAN (DCGAN)&lt;/h1&gt;
&lt;p&gt;&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recall that a Generative Adversarial Network is two networks, a generator and a discriminator.  The "generator" takes a feature vector and decodes this feature vector to become an image. The discriminator is exactly like the encoder of the Autoencoder, except it can only have 1 value in the final layer.  We use a sigmoid to squash this value between 0 and 1, and then interpret the meaning of it as: 1, the image you gave me was real, or 0, the image you gave me was generated by the generator, it's a FAKE! So the discriminator is like an encoder which takes an image and then perfoms lie detection.  Are you feeding me lies?  Or is the image real?  &lt;/p&gt;
&lt;p&gt;Consider the AutoEncoders for instance.  The loss function operated partly on the input space.  It said, per pixel, what is the difference between my reconstruction and the input image?  The l2-loss per pixel.  Recall at that time we suggested that this wasn't the best idea because per-pixel differences aren't representative of our own perception of the image.  One way to consider this is if we had the same image, and translated it by a few pixels.  We would not be able to tell the difference, but the per-pixel difference between the two images could be enormously high.&lt;/p&gt;
&lt;p&gt;The GAN does not use per-pixel difference.  Instead, it trains a distance function: the discriminator.  The discriminator takes in two images, the real image and the generated one, and learns what a similar image should look like!  That is really the amazing part of this network and has opened up some very exciting potential future directions for unsupervised learning.  Another network that also learns a distance function is known as the siamese network.  We didn't get into this network in this course, but it is commonly used in facial verification, or asserting whether two faces are the same or not.&lt;/p&gt;
&lt;p&gt;The GAN network is notoriously a huge pain to train!  For that reason, we won't actually be training it.  Instead, we'll discuss an extension to this basic network called the VAEGAN (Variational Auto Encoder GAN). For now, let's stick with creating the GAN.&lt;/p&gt;
&lt;p&gt;Let's first create the two networks: the discriminator and the generator.  We'll first begin by building a general purpose encoder which we'll use for our discriminator.  What we want is for the input placeholder to be encoded using a list of dimensions for each of our encoder's layers.  In the case of a convolutional network, our list of dimensions should correspond to the number of output filters.  We also need to specify the kernel heights and widths for each layer's convolutional network.&lt;/p&gt;
&lt;p&gt;We'll first need a placeholder.  This will be the "real" image input to the discriminator and the discrimintator will encode this image into a single value, 0 or 1, saying, yes this is real, or no, this is not real.&lt;/p&gt;
&lt;p&gt;This description was kindly provided by Parag under &lt;a href="http://jakublangr.com/ai-2016-review.html"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;net = CV.get_celeb_vaegan_model()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We'll load the graph_def contained inside this dictionary.  It follows the same idea as the &lt;code&gt;inception&lt;/code&gt;, &lt;code&gt;vgg16&lt;/code&gt;, and &lt;code&gt;i2v&lt;/code&gt; pretrained networks.  It is a dictionary with the key &lt;code&gt;graph_def&lt;/code&gt; defined, with the graph's pretrained network.  It also includes &lt;code&gt;labels&lt;/code&gt; and a &lt;code&gt;preprocess&lt;/code&gt; key.  We'll have to do one additional thing which is to turn off the random sampling from variational layer.  This isn't really necessary but will ensure we get the same results each time we use the network.  We'll use the &lt;code&gt;input_map&lt;/code&gt; argument to do this.  Don't worry if this doesn't make any sense, as we didn't cover the variational layer in any depth.  Just know that this is removing a random process from the network so that it is completely deterministic.  If we hadn't done this, we'd get slightly different results each time we used the network (which may even be desirable for your purposes).&lt;/p&gt;
&lt;p&gt;Now let's get the relevant parts of the network: &lt;code&gt;X&lt;/code&gt;, the input image to the network, &lt;code&gt;Z&lt;/code&gt;, the input image's encoding, and &lt;code&gt;G&lt;/code&gt;, the decoded image.  In many ways, this is just like the Autoencoders we learned about above, except instead of &lt;code&gt;Y&lt;/code&gt; being the output, we have &lt;code&gt;G&lt;/code&gt; from our generator!  And the way we train it is very different: we use an adversarial process between the generator and discriminator, and use the discriminator's own distance measure to help train the network, rather than pixel-to-pixel differences.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;X = g.get_tensor_by_name(&amp;#39;net/x:0&amp;#39;)
Z = g.get_tensor_by_name(&amp;#39;net/encoder/variational/z:0&amp;#39;)
G = g.get_tensor_by_name(&amp;#39;net/generator/x_tilde:0&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's get some data to play with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;files = sorted(datasets.CELEB())
img_i = 20
img = plt.imread(files[img_i])
plt.imshow(img)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Exploring the Celeb Net AttributesÂ¶
Let's now try and explore the attributes of our dataset. We didn't train the network with any supervised labels, but the Celeb Net dataset has 40 attributes for each of its 200k images. These are already parsed and stored for you in the net dictionary:&lt;/p&gt;
&lt;p&gt;Find the Latent Encoding for an Attribute
The Celeb Dataset includes attributes for each of its 200k+ images. This allows us to feed into the encoder some images that we know have a specific attribute, e.g. "smiling". We store what their encoding is and retain this distribution of encoded values. We can then look at any other image and see how it is encoded, and slightly change the encoding by adding the encoded of our smiling images to it! The result should be our image but with more smiling. That is just insane and we're going to see how to do it. First lets inspect our latent space:
Latent Feature Arithmetic
Let's now try to write a general function for performing everything we've just done so that we can do this with many different features. We'll then try to combine them and synthesize people with the features we want them to have...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def get_features_for(label=&amp;#39;Bald&amp;#39;, has_label=True, n_imgs=50):
    # Helper function to obtain labels and then preprocessing and returning
    # a vector for the seeding function for GAN
    # basically figures out the embedding for a particular attribute
    label_i = net[&amp;#39;labels&amp;#39;].index(label)
    label_idxs = np.where(net[&amp;#39;attributes&amp;#39;][:, label_i] == has_label)[0]
    label_idxs = np.random.permutation(label_idxs)[:n_imgs]
    imgs = [plt.imread(files[img_i])[..., :3]
            for img_i in label_idxs]
    preprocessed = np.array([CV.preprocess(img_i) for img_i in imgs])
    zs = sess.run(Z, feed_dict={X: preprocessed})
    return np.mean(zs, 0)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we use the code to create an interpolation between "Male" and "Not Male" (Female) images. Because we are only using the two endpoints, we get two images: a 100% Man and 100% Woman (please note that we can also get anything in between by doing a weighed average of the two seeding vectors).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def gan_generate_data(num_iter=20000,imgs=15):
    # generates 2*(number of iter) images 
    # adding random number of pictures for each synthesis (to increase variation)
    # returns list of [Male, Female] * num_iter images
    generated_images = []

    for i in range(num_iter):

        n_imgs = random.choice(range(imgs-10, imgs+10))

        z1 = get_features_for(&amp;#39;Male&amp;#39;, True, n_imgs=n_imgs)
        z2 = get_features_for(&amp;#39;Male&amp;#39;, False, n_imgs=n_imgs)

        notmale_vector = z2 - z1
        amt = np.linspace(0, 1, 2)
        zs = np.array([z1 + notmale_vector*amt_i for amt_i in amt])
        g = sess.run(G, feed_dict={Z: zs})

        generated_images.append(g[0])
        generated_images.append(g[1])

        if i%1000==0:
            print(&amp;#39;Iteration number : {}&amp;#39;.format(i))

    return generated_images

generated_data = gan_generate_data()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Okay good, we have the data to play around with and it's saved in a pickle file so we don't have to re-create it. Now, let's just add one hot encoded labels (we have done this in predictable manner -- i.e. male (0) is always first). We can just sense-check it and get the shape of the overall sample.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;labels = [0,1] * 20000
generated_data = np.array(generated_data)
generated_data.shape
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a name="extensions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;Now let's get to the transfer learning part. First we have to get out network, &lt;code&gt;vgg16&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;libs&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;vgg16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2v&lt;/span&gt;

&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vgg16&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_vgg_face_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Transfer Learning&lt;/h2&gt;
&lt;p&gt;Here we get the &lt;code&gt;vgg16&lt;/code&gt; network, which we have loaded up earlier and use it to generate the predictions for one of its own pre-trained classes. However, since we want to predict a different task, we then use the &lt;code&gt;transferred_predictions&lt;/code&gt; function to get the predictions for the 2623 different classes and then use that as an input to the next classifier to train it on recognizing gender. &lt;/p&gt;
&lt;p&gt;In order to do this effectively we must first do some image processing, which we do in &lt;code&gt;transferred_df&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transferred_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;gets&lt;/span&gt; &lt;span class="s s-Atom"&gt;an&lt;/span&gt; &lt;span class="nf"&gt;image&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;array&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;as&lt;/span&gt; &lt;span class="s s-Atom"&gt;an&lt;/span&gt; &lt;span class="s s-Atom"&gt;input&lt;/span&gt; &lt;span class="s s-Atom"&gt;outputs&lt;/span&gt; &lt;span class="s s-Atom"&gt;net&amp;#39;s final layer predictions &lt;/span&gt;
&lt;span class="s s-Atom"&gt;    results = []&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    # Grab the tensor defining the input to the network&lt;/span&gt;
&lt;span class="s s-Atom"&gt;    x = g.get_tensor_by_name(names[0] + &amp;quot;:0&amp;quot;)&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    # And grab the tensor defining the softmax layer of the network&lt;/span&gt;
&lt;span class="s s-Atom"&gt;    softmax = g.get_tensor_by_name(names[-2] + &amp;quot;:0&amp;quot;)&lt;/span&gt;

&lt;span class="s s-Atom"&gt;    with tf.Session(graph=g) as sess, g.device(&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nn"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="sc"&gt;0&amp;#39;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="nv"&gt;Remember&lt;/span&gt; &lt;span class="s s-Atom"&gt;from&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;lecture&lt;/span&gt; &lt;span class="s s-Atom"&gt;that&lt;/span&gt; &lt;span class="s s-Atom"&gt;we&lt;/span&gt; &lt;span class="s s-Atom"&gt;have&lt;/span&gt; &lt;span class="s s-Atom"&gt;to&lt;/span&gt; &lt;span class="s s-Atom"&gt;set&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;dropout&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;keep probability&amp;quot;&lt;/span&gt; &lt;span class="s s-Atom"&gt;to&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;feed_dict=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nn"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;Not&lt;/span&gt; &lt;span class="s s-Atom"&gt;using&lt;/span&gt; &lt;span class="s s-Atom"&gt;droput&lt;/span&gt; &lt;span class="s s-Atom"&gt;here&lt;/span&gt;
                    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;&amp;#39;net/dropout_1/random_uniform:0&amp;#39;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;&amp;#39;net/dropout/random_uniform:0&amp;#39;:&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;test_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s s-Atom"&gt;::-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nf"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s s-Atom"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; 
                &lt;span class="s s-Atom"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
               &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;idx&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="s s-Atom"&gt;test_array&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s s-Atom"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;columns=&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;

&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;does&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="s s-Atom"&gt;preprocessing&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;the&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;list&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt; &lt;span class="s s-Atom"&gt;and&lt;/span&gt; &lt;span class="s s-Atom"&gt;outputs&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;list&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;of&lt;/span&gt; &lt;span class="s s-Atom"&gt;predictions&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;i&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="nf"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;imresize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;size=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;&amp;#39;preprocess&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="s s-Atom"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;transferred_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="s s-Atom"&gt;results&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="s s-Atom"&gt;if&lt;/span&gt; &lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="c1"&gt;%1000==0:&lt;/span&gt;
            &lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Current image id {}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;


&lt;span class="s s-Atom"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parallel_transfer_eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="s s-Atom"&gt;:&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;returns&lt;/span&gt; &lt;span class="s s-Atom"&gt;parallely&lt;/span&gt; &lt;span class="s s-Atom"&gt;executed&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s s-Atom"&gt;using&lt;/span&gt; &lt;span class="s s-Atom"&gt;first&lt;/span&gt; &lt;span class="nf"&gt;split&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nf"&gt;second&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;and&lt;/span&gt; &lt;span class="nf"&gt;third&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;as&lt;/span&gt; &lt;span class="s s-Atom"&gt;divisors&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;multiprocessing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;ss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s s-Atom"&gt;:fs&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s s-Atom"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;ss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s s-Atom"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nn"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s s-Atom"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;target&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="s s-Atom"&gt;#&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nf"&gt;delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s s-Atom"&gt;transferred_df&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="s s-Atom"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s s-Atom"&gt;for&lt;/span&gt; &lt;span class="s s-Atom"&gt;img&lt;/span&gt; &lt;span class="s s-Atom"&gt;in&lt;/span&gt; &lt;span class="s s-Atom"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="s s-Atom"&gt;return&lt;/span&gt; &lt;span class="s s-Atom"&gt;results&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Leveraging transfer learning&lt;/h2&gt;
&lt;p&gt;Now we use the predictions made by &lt;code&gt;vgg16&lt;/code&gt; in a typical &lt;a href="http://cs231n.github.io/transfer-learning/"&gt;Transfer Learning&lt;/a&gt; paradigm. Here we just take the last layer of predictions, reshape the features and feed it to a next layer classifier (sometimes also done by removing the last (few) Fully Connected Layers) and putting training the whole network. Here we just create a new one just on the last layer. The practice supports both approaches. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="c1"&gt;# train-test for proper evaluation&lt;/span&gt;
&lt;span class="n"&gt;train_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_cores&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gpu_memory_fraction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# set up the network&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;categorical_crossentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tflearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;generated_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_set&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LabelEncoder&lt;/span&gt;

&lt;span class="c1"&gt;# reshape labels so that they match what the network expects&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LabelEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np_utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

&lt;span class="n"&gt;test_imgs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preprocess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;test_imgs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we're done with this bit as we have scores for both generated and hand-labelled images (test)! This only is the first step, however, in our journey, as now we have to transfer the &lt;code&gt;vgg16&lt;/code&gt; generated scores onto the new classifier (the last bit in transfer learning, which is typically simplified by cutting off the last layer and just re-running the network with a new final layer, but here done explicitly for training purposes.)&lt;/p&gt;
&lt;h2&gt;Training and evaluating a new classifier&lt;/h2&gt;
&lt;p&gt;For simplicity, we will just use the &lt;code&gt;tflearn&lt;/code&gt; classifier so that we have an easier job using transfer learning given the complexity of all the previous work:
1. we train (based on the synthetic data and the therefore completely predictable labels)
2. we evalute on the handlablled examples (by me) &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;absolute_import&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;


&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;

&lt;span class="n"&gt;feature_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real_valued_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dimension&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;


&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DNNClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2623&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;gradient_clip_norm&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                            &lt;span class="n"&gt;n_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                                            &lt;span class="c1"&gt;# model_dir=&amp;#39;./model&amp;#39;)&lt;/span&gt;

&lt;span class="c1"&gt;# Fit model.&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Evaluate accuracy.&lt;/span&gt;
&lt;span class="n"&gt;test_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# test_array = np.array([ [res[0] for res in result] for result in test_array ])&lt;/span&gt;

&lt;span class="n"&gt;accuracy_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_labels&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;accuracy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Accuracy: {0:f}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;General discussion&lt;/h1&gt;
&lt;p&gt;The results were not that stellar, however, I think this is a fascinating research area and quite likely it is going to be one of the biggest areas for the future of AI: but we still got better than random (consistently) and might get better if I spent more time on this.&lt;/p&gt;
&lt;p&gt;Moreover this code can probably fine-tuned and re-used with only minor modifications in many industry applications:
(a) 3D object generation
(b) &lt;a href="https://www.youtube.com/watch?v=u7kQ5lNfUfg"&gt;Pix2Pix applications&lt;/a&gt; that manages to create new images based on style or just a generation of maps from satelite images. The possibilities here are &lt;em&gt;literally endless&lt;/em&gt;.
(c) Remastering Old Movies.
Just to name a few.&lt;/p&gt;
&lt;p&gt;Thank you for reading and if any of this was of interest, explore this website for more!&lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="semi-supervised learning"></category><category term="GANs"></category><category term="Generative"></category><category term="Adverserial"></category><category term="Neural Networks"></category><category term="code"></category></entry><entry><title>GANs &amp; Semi-Supervised Learning</title><link href="jakublangr.com/gans-tutorial.html" rel="alternate"></link><published>2017-04-08T21:00:00+01:00</published><updated>2017-04-08T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-04-08:jakublangr.com/gans-tutorial.html</id><summary type="html">&lt;p&gt;I know what you're thinking: can there be anything more exciting? Rest assured, the wild party don't stop there. So hold on to your glasses nerds, we're going to do some A-friggin-I. Or if you're into ML and think this minus the sarcasm. But actually, this is super cool.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="http://jakublangr.com/ai-2016-review"&gt;last blog post&lt;/a&gt; we looked what are some of the promising areas in AI and one of the areas that was mentioned many, many times by researchers and my friends as likely future directions of AI, was Generative Adverserial Learning/Networks (GANs). The business appeal is clear: GANs can train from less data, can create fascinating applications (such as 3D model generation) and has lots of future research potential. For more on the practical applications &lt;a href="http://jakublangr.com/ai-2016-review"&gt;go back to that post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post is intended to be somewhat technical and will feature some high-level pseudo-code, but I will try to make it as accessible and hopefully not too boring.&lt;/p&gt;
&lt;div align='center'&gt;
&lt;img src='https://cdn.meme.am/instances/500x/76522645/get-in-loser-get-in-loser-were-going-to-do-some-ai.jpg' style="width: 40%" align="center"&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
&lt;div align="center"&gt; 
&lt;strong&gt; So you want to get on the hype train? &lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Note that all of this is quite cutting edge and some of the things showcased here were only invented and published in academic journals about a year ago, so this is something that unless you did a post-Doc in some discipline can feel a bit unusual (it did for me) to read things this new. But it also means that there's lots of unmapped theory around this as well as super strange bugs that you have to deal with. But because I &lt;a href="https://www.kadenze.com/certificates/verified/8TKKM10L"&gt;recently finished&lt;/a&gt; (I just really like taking MOOCs, okay?) the amazing course on &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-iv"&gt;Creative Applications of Deep Learning with TensorFlow&lt;/a&gt; by the one and the only &lt;a href="http://pkmital.com/home/"&gt;Parag Mital&lt;/a&gt;, I decided to share some of what I have learned.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Semi-supervised_learning"&gt;Semi-supervised learning&lt;/a&gt; basically means using labelled (supervised) as well as unlabelled (unsupervised) examples during training and as a concept is quite old. The core idea makes a lot of sense: we have lots of data that in a typical supervised setting lies unused. For example think linear regression on a house price (label) data. We all understand how &lt;a href="https://en.wikipedia.org/wiki/Linear_Regression"&gt;linear regression&lt;/a&gt; can generate &lt;a href="https://www.coursera.org/learn/ml-foundations/lecture/2HrHv/learning-a-simple-regression-model-to-predict-house-prices-from-house-size"&gt;house prices&lt;/a&gt;, but most houses are not sold, but perhaps we can get the data about them anyways, perhaps from the city planning. This data can give us much better picture, for example about how do different areas compare to each other, where is there a relative shortage of houses and where the biggest houses tend to be. It would be foolish then not to use this data in some way, but traditional algorithms do not allow it.&lt;/p&gt;
&lt;p&gt;So Semi-Supervised Learning (SSL) then means using different techniques to somehow add this data to the training of the machine learning (ML) model. But even this is not trivial: if you think of training ML as creating a decision tree and then you can check how good your decision tree was by checking if it got to the correct answer. Unfortunately with the unlabelled data, there's no answer (because the house was not sold during the time the data was gathered), so no learning happens, because the ML algorithm cannot attach correct answer (and therefore loss) to it. I want to focus on one of the techniques in SSL called Generative Adverserial Networks (GANs), which if you read my blog, understand why there's a lot of promise. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"&gt;GANs work&lt;/a&gt; by first having one network create an internal vision of the world (i.e. what do houses look like &lt;em&gt;in general&lt;/em&gt;): this is the generative model (G) and basically learns from all the data, because it does not need labels, just all of the features of a typical house in the dataset. The second network, the 'Discriminator' (D), which is the adversary in this case, takes in the examples both from the real dataset and the examples of houses generated by the generator and decides whether this data looks real, the generator has done a good job and gets a smaller loss or...&lt;/p&gt;
&lt;div align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/qc7jblpaPdM" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br &gt;&lt;/p&gt;
&lt;p&gt;In other words, imagine now we are trying to label cats or dogs, in this case G will learn how to generate images at first and subsequently get better at making the images more like cats or dogs. &lt;/p&gt;
&lt;p&gt;Then we put the G and D to basically compete against each other to produce the best results: hopefully every time G gets better D has to get better to match (though we have to make sure one of them is not too much better than the other). This was one of the core driving principles behind &lt;a href="https://en.wikipedia.org/wiki/AlphaGo#Algorithm"&gt;AlphaGo&lt;/a&gt; as well. Basically, we get the D to produce images and G to critique them. So G would send an array of images to D and D would output 0 or 1--real or fake back to G. G would then try to come up with better examples based on which fooled D and vice versa. Schematically, it may look something like this:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src='http://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png' style='width: 60%'&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;Credit KDNuggets.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So hopefully it is now clear that we can take lots of unlabelled data, construct a generator and make it learn some of the structure of the data (i.e. what does a typical example look like) then make it compete in making the data that it generates as close to the real data. After this process, we may end up with some pretty decent looking synthetic data and pretty much close to unlimited amount of it. I've skipped lots of caveats, but I will say this: the generator will only be able to generate things alike what it has seen in the data before. Even though it may be easy to forget, this is not magic.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src='https://i.imgflip.com/1mzsbp.jpg'&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;(Those who actually speak Latin forgive me.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the highest levels of abstraction this may look something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# get data
real_data = pd.read(&amp;#39;real_data.csv&amp;#39;) # shape (n_examples, n_features+label) 
unlabelled_data = pd.read(&amp;#39;unlabelled_data.csv&amp;#39;) # shape (n_examples, n_features)

# construct the two objects
generator = GeneratorClass()
discriminator = DiscriminatorClass()

# pre-train generator
generator.train(unlabelled_data)
# get synthetic data 
synthetic_data = make_compete(generator, discriminator, real_data) 
# shape (any_number, n_features)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;div align="center"&gt;
&lt;strong&gt; (Machine-Learning) WAR! What is it good for? &lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Okay so a keen reader might observe that we have not described a method for labeling the generated examples. Ideally, what we would like to have is a way of generating examples (e.g. houses with the price attached or pictures of objects with the object description attached). Thankfully for lots of instances, there is a way. If you go back to the diagram, you can see that there's a mention of something called 'the latent space'. Latent space is a way to control what kind of images get generated. If we trained the generator on cats and dogs, one of the dimensions will control how 'catty' or 'doggy' the image will be. It also allows for any interpolation between the two, so you can have a dog-cat or 70% cat, 30% dog. In other words, latent space can be thought of as some seeding factor--you give some initial input to G just so that it does not always generate the same thing, but it turns out this seeding factor has consistent latent ('hidden') properties that certain dimensions can be assigned meaning. &lt;/p&gt;
&lt;p&gt;So we can easily modify the pseudocode from above to make this clearer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# shape (any_number, n_features)
synthetic_cats = make_compete(generator, discriminator, 
                    real_data, input_noise=latent_feature_space.cat) 
# shape (any_number, n_features)
synthetic_dogs = make_compete(generator, discriminator, 
                    real_data, input_noise=latent_feature_space.dog) 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The amazing thing about this is that in theory, we don't even need to have the data labelled for that task to generate those examples (though it would help a lot). So we could have labelled training data for whether something is a being a good or a bad boy (for both cats and dogs) and we can train the G to create new examples of cats or dogs (based on one parameter of the latent space), both good or bad (based on another parameter of the latent space). Let's say that good or badness of a dog is something we can see from the picture (e.g. it is a bad boy every time it is destroying property, good otherwise). We can then discover parameters in the latent space for both of these features and generate even examples of cat-dog or dog-cat by interpolating between these two values.&lt;/p&gt;
&lt;p&gt;Another example is that we can download loads of unlabelled data of faces of celebrities and make the G generate faces and manipulate the latent space so that we get clear examples of male or female and then use it to train another classifier to detect male or female images (with no labelled data of any kind!), which is exactly what I have done. One question that might be still going through your head is 'how do we get this latent space representations of these different attributes?': that, however, is probably beyond the scope of this article unfortunately.&lt;/p&gt;
&lt;p&gt;Phew, look at the time: I was hoping to present some actual code so that people can try this on their own, but unfortunately, I felt that this blog post is already long enough so I will leave the code till next time. If there's interest or you would love to see something urgently, drop me a line.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In case you are interested in the actual code, check out the &lt;a href="http://jakublangr.com/gans-code.html"&gt;second part of this tutorial&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="semi-supervised learning"></category><category term="GANs"></category><category term="Generative"></category><category term="Adverserial"></category><category term="Neural Networks"></category></entry><entry><title>Artificially Intelligent Business</title><link href="jakublangr.com/ai-2016-review.html" rel="alternate"></link><published>2017-03-14T22:00:00+00:00</published><updated>2017-03-14T22:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2017-03-14:jakublangr.com/ai-2016-review.html</id><summary type="html">&lt;p&gt;AI is moving forward at an amazing rate but we should take the time to appreciate all the amazing potential of the recent advancements. I want to take the time to get some technical appreciation for the business implications the recent AI may have.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lots of people are arguing about the theoretical nature of what AI means many years down the road, but I would like to focus on the AI commercial products in the next 2-5 years. I'd also bring all the incredible, but deeply technical advances back to the real world and business and focus on and focus on what businesses and consumers can practically expect of the future. First off I will talk about the most interesting advances in AI, second what the current applications of those advances are and wrap it up with what this means for business.&lt;/p&gt;
&lt;p&gt;At the same time, this is &lt;strong&gt;not&lt;/strong&gt; intended to be a discussion of the business theory behind AI-startups. This has been covered by more qualified people, for &lt;a href="http://venturebeat.com/2017/03/13/10000-ai-startups-need-to-learn-these-lessons/" target="_blank"&gt;example here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 1. Breakthroughs in AI &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last couple of months were excellent for AI: everything from &lt;a href="https://en.wikipedia.org/wiki/One-shot_learning" target="_blank"&gt;One-Shot-Learning&lt;/a&gt;, advances in Generative Adversarial Networks (GANs) and Memory-Augmented Networks to practical applications such as AlphaGo, &lt;a href="http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players" target="_blank"&gt;Liberatus&lt;/a&gt; winning the world Poker championship and a neural net creating images that match textual input. &lt;/p&gt;
&lt;p&gt;There have been way too many practical applications to list, but all of these are a results of more theoretical work from prior years. Therefore, in order to look at what future is likely to hold, we should look at the theoretical advancements and extrapolate what kind of practical tasks these novel techniques can achieve in production. One such area is &lt;a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank"&gt;reasoning AI&lt;/a&gt;: memory augmented and differentiable networks are likely to bring in a new era of higher level reasoning: as recent &lt;a href="https://www.youtube.com/watch?v=eyovmAtoUx0&amp;amp;t=4s" target="_blank"&gt;Stanford papers&lt;/a&gt; and &lt;a href="https://code.facebook.com/posts/457605107772545/under-the-hood-building-accessibility-tools-for-the-visually-impaired-on-facebook/" target="_blank"&gt;recent Facebook tools to help visually impaired understand content of images&lt;/a&gt; demonstrate. So reasoning and Q&amp;amp;A AI is just around the corner.
In terms of future theoretical breakthroughs, the most fruitful areas are likely to be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further improvements on GANs, as they already are being tested in the field of &lt;a href="https://www.youtube.com/watch?v=kf-KViOuktc" target="_blank"&gt;3D object generation&lt;/a&gt;, &lt;a href="https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503" target="_blank"&gt;remastering old movies&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=WovbLx8C0yA"&gt;image enhancement&lt;/a&gt;. Even one of the fathers of deep learning, &lt;a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/" target="_blank"&gt;Yann LeCunn, called it&lt;/a&gt; âthe most important [recent advancement in deep learning]â.&lt;/p&gt;
&lt;p&gt;Further improvements on memory/attention as demonstrated for instance by a &lt;a href="https://deepmind.com/blog/differentiable-neural-computers"&gt;recent study by DeepMind published in Nature&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--It does not really matter what the names of these advances are, though if you are interested, feel free to read more about them.
--&gt;

&lt;p&gt;&lt;strong&gt; 2. Summary of state of the art, future improvements and likely future paths &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The biggest breakthrough, &lt;a href="(http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/ ){:target=&amp;quot;_blank&amp;quot;}, I think will come from Generative Adversarial Networks (GANs), so I would like to focus on them as they are especially impressive in the case of [Adversarial and Semi-Supervised Learning](https://ishmaelbelghazi.github.io/ALI){:target=&amp;quot;_blank&amp;quot;}. In essence, this approach tries to get around the problem of not having enough labelled data. This is a problem that everyone maybe except the likes of Google faces. Hence this breakthrough can kick-start a new wave of AI companies in any sufficiently technically savvy ecosystem, such as London. The results are already extremely promising based on research from one group in Montreal (I highly recommend reading [this link](https://ishmaelbelghazi.github.io/ALI){:target=&amp;quot;_blank&amp;quot;}."&gt;in line with LeCunn&lt;/a&gt; as well as OpenAI: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;âThis approach [GANs for semi-supervised learning] allows us to obtain state of the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network â a result thatâs very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is very promising because labeled examples can be quite expensive to obtain in practice.â &lt;a href="https://openai.com/blog/generative-models/" target="_blank"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To summarize: this means that people can get performance as good as the best networks in the field trained with the full dataset. &lt;/p&gt;
&lt;p&gt;I foresee that the next improvements will apply similar techniques to more complex datasets and using fewer and fewer labelled examples to &lt;a href="http://rodrigob.github.io/are_we_there_yet/build" target="_blank"&gt;match the state-of-the art performance&lt;/a&gt;. This can probably be realized by not only using transfer learning as the âtraining agentâ more effectively (or using &lt;a href="https://en.wikipedia.org/wiki/Multi-task_learning" target="_blank"&gt;multi-task learning&lt;/a&gt;), but also by using advanced GANs themselves, as hinted to above. Or maybe even to move closer to optimal &lt;a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank"&gt;Active Learning&lt;/a&gt; by generating exactly the hardest (real) examples for the network to classify at that point in time to improve the fastest. (This does &lt;strong&gt;not&lt;/strong&gt; depend on advances in other areas, such as &lt;a href="https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients" target="_blank"&gt;Synthetic Gradients&lt;/a&gt;, better optimizers etc.)&lt;/p&gt;
&lt;p&gt;In other words, now we are advancing, in a sense, in the "theory of knowledge" of Machine Learning algorithms so that they can learn faster. Akin to a child in middle-school, deep learning is now discovering "learning aids" or "hacks" to learn even faster and generalize better. This means that we can move AI closer to production and usefulness. This means that plugging into a new (business) vertical will be easier than ever. You will still need to develop vertical-specific expertise and architecture, but technology like generic object detection, &lt;a href="https://www.technologyreview.com/s/603811/baidus-artificial-intelligence-lab-unveils-synthetic-speech-system/" target="_blank"&gt;speech synthesis&lt;/a&gt; or sentiment analysis will be commoditized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 3. Commodtizied ML is here, so what? &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These advancements along with better tools for AI researchers and production, will favor companies who have (i) unique business partnerships or (ii) overcome some unique regulatory hurdle. Huge opportunities are still in &lt;a href="https://www.bloomberg.com/news/articles/2017-02-28/jpmorgan-marshals-an-army-of-developers-to-automate-high-finance" target="_blank"&gt;FinTech&lt;/a&gt; and other regulation-heavy industries.&lt;/p&gt;
&lt;p&gt;However, the key point is to realize that the AI of the future will likely just serve as a preprocessing or feature engineering step feeding into the next, traditional data science, layer of algorithms. But make no mistake: this is huge. Computers, for the first time in history, will have the ability to see and create their own representations of objects. To take a practical example: I have recently &lt;a href="http://joinef.com" target="_blank"&gt;been through an accelerator&lt;/a&gt; where &lt;a href="http://observe.tech/" target="_blank"&gt;one of the companies, Observe, is working on automating fish farming&lt;/a&gt;. They have a standard object recognition algorithm that passes on its output to further systems managing feeding of fish etc., so that they maximize the fish stock and minimize waste. Their key selling proposition in this case is the second, fish feeding, algorithm, but until couple of years ago there was no easy way to explain the images of fish to the second (control) algorithm. &lt;/p&gt;
&lt;p&gt;To take a traditional computer science analogy, we can think of the case of &lt;a href="https://en.wikipedia.org/wiki/Chinese_room" target="_blank"&gt;John Searle's Chinese room experiment&lt;/a&gt;: so far the computer was just manipulating symbols with no conception of what those symbols mean. I.e. the computer could not be thought of as "understanding" something or being "conscious". I do not want to delve into the philosophical implications of this statement, but the core dilemma is: because the computer has no context, it cannot be thought of as operating on anything else than a (computationally) straightforward set of instructions (no matter how complex in human terms). Now, computers are getting more context, so, in an odd sense, we are closer to understanding. &lt;/p&gt;
&lt;p&gt;&lt;img src='https://upload.wikimedia.org/wikipedia/commons/d/de/ChineseRoom2009_CRset.jpg' style='width: 90%'&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From Wikipedia under CC license.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Keeping in line just with the practical sense, computers will now be able to auto-extract features. In fact, &lt;a href="http://eyn.vision/" target="_blank"&gt;for instance Eyn's&lt;/a&gt; core tech is already a facial detection algorithm fed into spoofing detection. This is only possible because facial detection is so close to perfect we can rely on it almost as deterministic. Andrew Ng talked about the importance of moving from 95% to 99%, I think there is another, perhaps even more important milestone in moving from 99 to 99.9% where we can use commoditized machine learning almost as a framework for our core tech.&lt;/p&gt;
&lt;p&gt;This is especially important in parts of the world where databases about the real world are not as common as in the EU. Because algorithms can now integrate into physical space and get data from it directly, we can build directly onto the physical space: i.e. we can provide value to all of the ~6.1 billion people currently underserved by technology and apply technology directly to areas they find most useful, like agriculture: &lt;a href="http://optimal.ag/" target="_blank"&gt;Optimal Labs&lt;/a&gt; is already doing that. Moreover, with all this computer vision, it will be much easier to jump-start projects like city planning: in fact, this is already happening in India.&lt;/p&gt;
&lt;p&gt;The last thing I would like to talk about is a direct implication of the previous paragraph, but has far reaching consequences. For the first time, IT systems may make sense even in situations in which there is no other IT system to connect to or IT support. Algorithms now do not need supporting database--as they can create their own or rely on pre-trained models--so they can be rolled out to a completely new company without the usual enterprise sales &amp;amp; integration cycle. In theory, you can have an entire IT system managed by external companies as it is quite expensive to have AI talent or ML maintenance on site (e.g. in agriculture) beyond simple mechanics. Now, this is wildly speculative, but if true, this absence of in-house IT people means that external companies can deploy to completely novel areas (and there'll be more of those opportunities because in-house IT is not there to pitch obvious technology solutions) and capture much more of the market. But it also means that there are no longer IT people to guide and support you so the business contacts are now more important than ever. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Conclusion &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So from the recent advancements and meta-learning theory it is clear that we are in an age of vertical AI and that the future direction is likely to heavily rely on understanding the business use-cases of particular AI technology. This seems to be further accentuated by the fact that AI can now be deployed on its own without external support or databases to connect to. This means that the business side of AI will be more important than ever, but also the opportunities are becoming more plentiful, because even the very obvious usecases might not be covered by in-house teams.  &lt;/p&gt;</content><category term="python"></category><category term="AI"></category><category term="reviews"></category><category term="StartUps"></category></entry><entry><title>Kaggle Struggle</title><link href="jakublangr.com/kaggle-2016.html" rel="alternate"></link><published>2016-05-03T21:00:00+01:00</published><updated>2016-05-03T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2016-05-03:jakublangr.com/kaggle-2016.html</id><summary type="html">&lt;p&gt;My trip to the &amp;lt;1% of data scientists. Or in the the world of competitive data science and back again. Aka: three (relative) successes and a fail. Also how not to spend a lot of money on your data problem. I need to start writing better summaries.&lt;/p&gt;</summary><content type="html">&lt;p&gt;For those of you that are not compete data nerds, I occasionally venture into the world of &lt;a href="http://www.kaggle.com"&gt;Kaggle&lt;/a&gt;: a competitive data science website where everyone from the top Data Science experts to the people who are just starting out compete against each other in the hopes of getting money (statistically unlikely), fame and massive amounts of fake nerd points (very important).&lt;/p&gt;
&lt;h1&gt;My life on Kaggle&lt;/h1&gt;
&lt;p&gt;&lt;div align="center"&gt;
&lt;img src="http://s3.amazonaws.com/theoatmeal-img/comics/working_home/6.png" alt='my life on Kaggle' style="width: 95%; height: auto "&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;I have so far participated in four different competitions: &lt;a href="https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction"&gt;Liberty Property Inspection Prediction&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management"&gt;BNP Paribas Claims Management&lt;/a&gt; and &lt;a href="https://www.kaggle.com/c/santander-customer-satisfaction"&gt;Santander Customer Satisfaction&lt;/a&gt;. All of these were fairly different competitions and I have learned loads of different things. &lt;/p&gt;
&lt;p&gt;In many cases, Kaggle is great: no more data archeology like in most commercial datasets. But this is not a rant about how when you bring an extra team on a project you should document the changes they made. Or about how when you start/stop sharding your database and you have variable names that indicate this, these should be updated as well. Or indeed that columns have swapped names or names that have nothing to do with what they contain for historical reasons. Not that I am bitter or anything.&lt;/p&gt;
&lt;h1&gt;Okay, okay... but I still don't get it.&lt;/h1&gt;
&lt;p&gt;So the idea behind Kaggle is that companies that have a data problem put their data (anonymised, well-structured and standardised) on the site provide some minimal description of the problem (e.g. we want to be able to predict which marketing campaign will be successful based on some historical data). &lt;/p&gt;
&lt;p&gt;Then about half a million data scientists form around the world can download this dataset and play around with it and even submit their predictions to be scored against the known (historical) truth. The best ones get all of the perks above, the others get nothing. Capish?&lt;/p&gt;
&lt;h1&gt;Why did I decide to join?&lt;/h1&gt;
&lt;p&gt;This website is first and foremost a great way to learn: you get to know what people much better than you are doing and learn from them: usually mostly after the competition is over. &lt;/p&gt;
&lt;p&gt;I also think that working on these kinds of competitions is great especially for benchmarking your own skills and generally what lift (percentage increase) can we expect by massaging the data a bit more: be it &lt;a href="https://en.wikipedia.org/wiki/Feature_engineering"&gt;feature engineering&lt;/a&gt; or by &lt;a href="http://mlwave.com/kaggle-ensembling-guide/"&gt;ensambling&lt;/a&gt;. So Kaggle is a great way to test out some of these things in practice and see how others use them to some extent. &lt;/p&gt;
&lt;p&gt;I think that Kaggle is a great way to connect with other data scientist and figure out how they do things and maybe even make friends along the way. Coincidentally, this is also a great way how to spend a lot of money (more on that later).&lt;/p&gt;
&lt;h1&gt;How did I do?&lt;/h1&gt;
&lt;p&gt;There is a lot to do Kaggle: you earn ranking within each competition: I earned the Top 25% badge by finishing in the top 23.6% (property inspection), 12.9% (marketing) and 20.9% (claims management). Once I missed it by a 31.3% finish (customer satisfaction). I think that overall I have learned loads during these competitions: people frequently share code and model descriptions of the best performing models (the best performers though usually hold out till the end of competition). (You thought there will be an article about data science without numbers? Ha.)&lt;/p&gt;
&lt;h1&gt;Dude, you have a serious data problem.&lt;/h1&gt;
&lt;p&gt;Hi, I'm Jakub and I am a data addict. I have spent two weeks off AWS Console... but in all seriousness, Amazon Web Services is both a gift and a curse to every techie. Gift because you can have data centres' worth of computational power at your fingertips within minutes. Curse because of what it is going to cost you:&lt;/p&gt;
&lt;table align="center"&gt; 
&lt;tr&gt;
    &lt;td&gt;
    &lt;img src='images/april_aws.png' alt="April" style="width: 95%; height: auto "align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
    &lt;td&gt;
    &lt;img src="images/may_aws.png" height="38%" alt='May' style="width: 95%; height: auto " align="center"&gt;
    &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1 style="text-align: center"&gt; &lt;b&gt; &lt;a href='https://youtu.be/CduA0TULnow?t=1m28s'&gt; Ooops. &lt;/a&gt;&lt;/b&gt; &lt;/h1&gt;

&lt;p&gt;I don't feel like I am using AWS excessively even. But perhaps it was all worth it: &lt;/p&gt;
&lt;div align="center"&gt;
&lt;a href="https://www.kaggle.com/jakublangr"&gt;
&lt;img src="images/kaggle_status.png" style="align-content: center; width: 65%; height: auto"&gt; &lt;br&gt; 
&lt;/a&gt; Top .929%, aww yeah! 
&lt;/div&gt;

&lt;h1&gt;What to make of it&lt;/h1&gt;
&lt;p&gt;But the model of competitive data science is greatly interesting to many companies: not only does Kaggle have a track record of blowing the existing industry experts out of the water with their accuracy. So it is clearly doing well in other respects: like supporting the community and spreading information. There's loads of free public datasets to explore and easily share your findings. Also, the spread of efficient techniques and algorithms is thanks to Kaggle greatly accelerated: for instance, random forest (probably one of the top algorithm) was written in 2001 and rose to prominence around 2009, while eXtreme Gradient Boosting (XGB) was written in 2014 and blew other algorithms away by late 2015. So massive acceleration. Even I personally have to say that the community is amazing and there's always loads to learn.&lt;/p&gt;
&lt;p&gt;So that's it from now. As always, if you have a comment/question, I'd love to hear from you!&lt;/p&gt;</content><category term="datascience ML technical"></category></entry><entry><title>...especially about the future</title><link href="jakublangr.com/gjp-part2.html" rel="alternate"></link><published>2015-09-14T21:00:00+01:00</published><updated>2015-09-14T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2015-09-14:jakublangr.com/gjp-part2.html</id><summary type="html">&lt;p&gt;A year ago, I wrote about the Good Judgment Project -- an attempt to use prediction markets to study our understanding of forecasting of geo-political events. I was lucky to be selected as one of the participants in this project and now I am eager to share my thoughts.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A year has passed since I started to make predictions for this fascinating project run by Philip Tetlock a man that has dedicated about 30 years of his life to understanding geo-political forecasting. Recently the project has ended so I would like to share some insights.&lt;/p&gt;
&lt;p&gt;First and foremost, I would love to start with &lt;a href="hello-world.html"&gt;evaluation of my own lessons&lt;/a&gt;: I think that overall I ended up over-correcting for the biases I have read about, which is a fascinating lesson. I already mentioned this the &lt;a href="hello-world.html"&gt;first time I was writing&lt;/a&gt; about Good Judgment Project (GJP). At least based on my rudimentary understanding the calibration and observation, it seems that normally people have the opposite bias and the curve is flipped around the 45 degree line (the picture below is from the report I received from Good Judgment Project).&lt;/p&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/calibration_curve.png" alt="(Calibration)" align='right' height='300px' weight='300px' /&gt;&lt;/p&gt;
&lt;p&gt;My second point about randomness was just due to my un-rigorous understanding of randomness that I think I now understand: the problem is that something is random given a certain distribution (so we cannot make a better guess than the observed distribution for any particular observation). My third lesson about conditional probability still remains fair, which is: it is difficult to make it correctly, but as Philip Tetlock correctly pointed out, this is the way forward, because it allows us to get exactly the probabilities we care about: if we do X what will the world do? (Because we do not care about if Iran will build a nuclear bomb, we want to know IF we do nothing will Iran build a nuclear bomb?) 
sting as the project I have been using much during GJP--but they will "soon" be announcing what they are doing next, but who knows what that will entail. &lt;/p&gt;
&lt;p&gt;So when the results rolled in they were not too bad, I ended up being the 71st percentile, with my imputed scores dragging me down a little (this was necessary, because you could not answer all the questions, though maybe I also picked questions I understood better). &lt;/p&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/mean_brier.png" height='400px' width='800px' align='center' /&gt;
&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/density_imputed_comparison.png" height='400px' width='800px' align='center' /&gt;&lt;/p&gt;
&lt;p&gt;So I think the overall results were quite decent, but I certainly I have loads to learn! While Philip Tetlock certainly tries to help us mere mortals in the &lt;a href="http://edge.org/conversation/philip_tetlock-edge-master-class-2015-a-short-course-in-superforecasting-class-iii"&gt;the political forecasting by giving us better insight in the Edge online seminar&lt;/a&gt;. I have not watched all of them, but they seem amazing so far! &lt;/p&gt;
&lt;p&gt;I think what GJP mainly showed me is the power of prediction markets: we can not only &lt;a href="https://web.archive.org/web/20150813062234/http://www.columbia.edu/~bc2656/GooglePredictionMarketPaper.pdf"&gt;use them in companies to guide our own progress&lt;/a&gt;, but we can also use them to &lt;a href="https://home.inklingmarkets.com/"&gt;get a glimpse into the future like Inkling Markets does&lt;/a&gt; by pooling our predictions together in a crowd-sourcing manner. This last link is particularly interesting, as I feel this has &lt;em&gt;many&lt;/em&gt; commercial applications if only people cared enough about evidence. &lt;/p&gt;
&lt;p&gt;Philip Tetlock outlines what are the necessary ingredients of good forecasting and he mentions that the best forecasters tend to be publicly minded software engineers, which is partially supported by the Google information flow study and generally it seems that the more quantitative the subject the better you are likely to be as one political forecasting study finds that the &lt;a href="https://web.archive.org/web/20150614075429/http://www.hamilton.edu/documents/Analysis-of-Forcast-Accuracy-in-the-Political-Media.pdf"&gt;best forecasters in US politics&lt;/a&gt; "tend to be liberal and not lawyers" (most of the good ones were economist, interestingly). &lt;/p&gt;
&lt;p&gt;But I think there is a potential array of problems: are we not likely to fall into the same illusion as with actual financial markets? E.g. that they are &lt;em&gt;always&lt;/em&gt; efficient and therefore we should almost blindly trust them (as seems to be the narrative in the Conservative US politics)? Surely, given that we now know that most people are not better at forecasting than random (though this may be because all of the stastically efficient arbitration has already been done in financial markets by the algorithmic hedge funds). I think this is up for discussion, but I would definitely not try to apply them to policy debates blindly &lt;a href="http://squid314.livejournal.com/352406.html"&gt;as some authors do&lt;/a&gt;: &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Prediction markets avoid these problems. There is no question of who the experts are: anyone can invest in a prediction market. There's no question of special interests taking it over; this just distributes free money to more honest investors.&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;My other problem is that while applications in business are fascinating, the sample sizes with exception of &lt;em&gt;huge&lt;/em&gt; (and data-driven?) companies such as Google, there may always be too small of an interest to make this a useful tool.&lt;/p&gt;
&lt;p&gt;Perhaps we should just stick to talking to each other for now.&lt;/p&gt;</content><category term="prediction markets"></category></entry><entry><title>Speaking at European Economic Congress 2015</title><link href="jakublangr.com/eec-2015.html" rel="alternate"></link><published>2015-06-30T21:00:00+01:00</published><updated>2015-06-30T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2015-06-30:jakublangr.com/eec-2015.html</id><summary type="html">&lt;p&gt;I wanted to write a personal post about my experience speaking at an international conference and what I think the trouble with policy-makers trying to spur entrepreneurship is.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was recently invited to speak at European Economic Congress in Poland partially thanks to the fact that I was working at Aspen Institute Prague back in the summer of 2011. When I saw the line-up for my panel I was a bit terrified: why should anyone care what I had to say. &lt;/p&gt;
&lt;div align="center"&gt;
&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/panel.png" alt="Panel line up" align='center'
 style="width: 95%; height: auto " /&gt;
&lt;/div&gt;

&lt;h1&gt;To Poland!&lt;/h1&gt;
&lt;p&gt;When I arrived at the Katowice airport, there was even a guy standing with my name (laminated!) on it waiting to escort me to the hotel. That freaked me out a little bit. Katowice was an interesting city: marred by Communist rule and somewhat this felt like a journey back in time -- recalling my childhood days in Prague. Poland was noticeably different from Czech Republic, but was eerily similar in some of the functionalist buildings and social interaction. &lt;/p&gt;
&lt;p&gt;I've never properly been to Poland before except for the couple of times I have barely crossed the border like when I was climbing &lt;a href="https://en.wikipedia.org/wiki/Sn%C4%9B%C5%BEka"&gt;SnÄÅ¾ka&lt;/a&gt;, so it was good to have this opportunity to visit! Unfortunately and perhaps fittingly with the atmosphere of a predominantly communist era city, it was quite cold the entire time. &lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
    &lt;td&gt;
        &lt;img src='images/IMG_20150420_175640.jpg' alt='Katowice' style="width: 99%; height: auto "&gt;     
    &lt;/td&gt;&lt;td&gt;
        &lt;img src="images/IMG_20150420_103638.jpg" alt="President speaking" style="width: 99%; height: auto "&gt; 
    &lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The conference itself was held in a fabulous newly built Conference Center in the middle of Katowice. It was started off by the then President of Poland BronisÅaw Komorowski. Despite not being explicitly on the agenda, he started discussing the issue of entrepreneurship quite a lot: the issue of retaining talent, attracting investors and so on. Although there were no concrete plans, I still appreciated this as a very important signal as well as the recognition that entrepreneurship is hard and if we want to succeed we must embrace failure. &lt;/p&gt;
&lt;p&gt;This topic was great because that is one of the topics I am passionate about. I think that it is one of the great equalisers in a society as well as an important skill for anyone to pick up. The trouble with entrepreneurship in Europe though is that the social attitudes it encourages are actively decreasing the pool of potential entrepreneurs. Like the fact that our educational systems does not encourage students to experiment and fail at all: neither in the Czech Republic nor at my University (a top UK university). I am fairly sure that most of my teachers and classmates thought I was incredibly arrogant while I was only trying to follow evidence and best practice from people around the world. It took courage to do that and sure I am sure that as a person I have an incredible amount to work on, perhaps more than most, but I still think that I was politely ignored at best and actively discouraged at worst is indicative of why Europe has no major successful start up to speak of with the &lt;a href="https://www.cbinsights.com/research-unicorn-companies"&gt;possible exception of Spotify&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;I mainly talk about digital entrepreneurship, because that is where most of my expertise comes from but I think that ultimately, this should generalise to most areas. While I value greatly that the policy makers have taken on such an challenging topic, I have to highlight that the absence of anyone except old white men among the attendees and the whole spirit of the conference was very much in the style of a conference for them, thereby missing almost the entire population and I really hope that business is more inclusive that that. Or at the very least it should be.&lt;/p&gt;
&lt;p&gt;In terms of discussion, obviously, I was not expecting something like &lt;a href="/web-summit-dublin-14.html"&gt;my experience at Web Summit Dublin&lt;/a&gt;, which specialises on tech entrepreneurship, but there was also a massive disconnect between a lot of people were saying and what really needed to happen. Which is a shame, because I think that there is so much direct and obvious stuff that the policy makers could do to ease entrepreneurs' lives: less bureaucracy, encouraging educational system that rewards experiments and reasonable welfare state that does not make failure too costly.   &lt;/p&gt;
&lt;h1&gt;Panel! (SLightly egotistical pictures, don't judge me)&lt;/h1&gt;
&lt;table&gt;
    &lt;tr&gt;
    &lt;td &gt;
        &lt;img src='images/240627_940.jpg' alt='Panel 3' style="width: 95%; height: auto "&gt;       
    &lt;/td&gt;&lt;td &gt;
        &lt;img src="images/240621_940.jpg" alt="Panel 2" style="width: 95%; height: auto " &gt;
    &lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;div align="center"&gt;
&lt;img src='images/20150421_102200.jpg' alt='Panel 1' style="width: 95%; height: auto "&gt;      
&lt;br &gt;
&lt;img src="images/20150421_102129.jpg" alt="Panel 4" style="width: 95%; height: auto "&gt; 
&lt;/div&gt;

&lt;p&gt;&lt;br &gt; 
Source of images: PTWP SA&lt;/p&gt;
&lt;p&gt;All of the typical cliches were then repeated during my panel discussion: how it would be so great to have more European entrepreneurship, young people should do it (which is also not true: anyone has a shot!) and why do all the smart people go abroad? Unfortunately, the reality that it is often too costly financially and socially (also in terms of exit options) to create a new company in Europe and the real issues were sometimes completely missing. On the plus side, during the discussion I got a few shout-outs from others such as "As the gentleman on the far-right [of the panel] noted--without alluding to his political views--the young need to be willing to fail", as mentioned by the German ambassador. Though I was the only one to laugh at that remark, which was quite embarrassing. &lt;/p&gt;
&lt;p&gt;In summary, I think that it was great that we discussed this topic at all, but I think that Europe--if it is serious about entrepreneurship--needs to examine systematic social causes and start doing rather than merely talk. &lt;/p&gt;
&lt;p&gt;Overall, though, I had a great time in Poland. I think I still learnt loads from this conference and I had an opportunity to have a drink with some of my great former colleagues from Aspen!&lt;/p&gt;</content><category term="entrepreneurship"></category></entry><entry><title>On MOOCs: Projects, Practice and Perspective</title><link href="jakublangr.com/moocs-part1.html" rel="alternate"></link><published>2015-01-23T21:00:00+00:00</published><updated>2015-01-23T21:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2015-01-23:jakublangr.com/moocs-part1.html</id><summary type="html">&lt;p&gt;I said I will stop at finishing 14 online courses... and I am doing three MOOCs again. Review of my learning journey, projects I have done, reviews of great and good courses, my (brief) take on practices. &lt;strong&gt;UPDATED&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been quite a while since I started my first MOOC at Coursera. I think now is the time to reflect on the courses I have finished, what I have learned as well as what to recommend to my fellow MOOCers. I will try to broadly classify (and score, because if you are anything like me, you occasionally dislike the imprecision of natural language) the courses I have finished and then I will try to give my take on MOOCs in general.&lt;/p&gt;
&lt;h1&gt;1. Awesome Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/sna"&gt;Social Network Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course by &lt;a href="http://www.ladamic.com/"&gt;Lada Adamic&lt;/a&gt; was definitely one of the most awesome courses regardless of platform, country or institution that I have been involved in. Not only was Lada willing to have Google Hangout sessions with the course participants, but the course was excellently done: we have worked with &lt;a href="http://gephi.github.io/"&gt;Gephi&lt;/a&gt;, programmed in &lt;a href="https://ccl.northwestern.edu/netlogo/"&gt;NetLogo&lt;/a&gt; and used the &lt;a href="http://cran.r-project.org/web/packages/sna/sna.pdf"&gt;R SNA package&lt;/a&gt;(or at least I did in my projects). The course was a great balance of (social) network analysis, problem sets and optional programming assignments. You can view mine &lt;a href="https://dl.dropboxusercontent.com/u/30848031/US_Contributions.pdf"&gt;here&lt;/a&gt; (though bear in mind, I have done almost 2 years ago so it is not something I am particularly proud of). &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; R, Gephi, NetLogo, different metrics characterizing a graph&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/dataanalysis"&gt;Data Analysis&lt;/a&gt; and &lt;a href="https://www.coursera.org/course/compdata"&gt;Computing for Data Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long before Coursera introduced the 'Data Science specialization' (that in my opinion seems way too superficial), Coursera was offering these two amazing Courses that together were actually undiluted first year graduate bioinformatics courses for students at Johns Hopkins University. The problem with a lot of the later MOOCs is that they were starting to get too simple to actually teach me anything and so the pool of people who was actually taking these courses was a lot worse. I think the latter is also an issue, because half of the learning experience is the community. Like the people putting interesting thoughts on the forums or the moderators (or in this case, the community itself) trying to prevent cheating (mostly unintentional, I hope, why else would you do a MOOC?) and so on. I have been taking this courses with statistics PhDs and other people with amazing insights and experience and I think that is probably why the learning experience was so amazing and all the four projects including &lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/Cellphone_data_Prediction.pdf"&gt;the final one&lt;/a&gt; were something that gave me a solid grounding for the Social Network Analysis MOOC to use R as well as for my &lt;a href="http://goo.gl/sEUFMa"&gt;internship in the summer of 2013&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 9/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Through exploration of R basics, statistics, basic machine learning, data preparation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-0#.VMPxF4ptN5Q"&gt;Science of Uncertainty, Introduction to Probability&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was amazing, but also &lt;strong&gt;very&lt;/strong&gt; hard. This is the undiluted MIT 6.041 course. The commitment from the teaching staff of MIT (including the professor &lt;a href="http://www.mit.edu/~jnt/home.html"&gt;John Tsitsiklis&lt;/a&gt;) was incredible. Generally, I got a great answer for any of my questions in a shorter period of time then most of my professors at my home institution. The treatment was very rigorous that keeping up with the Oxford workload while doing this 12 week course proved quite difficult. But it was so much more rewarding to see the certificate of completion afterwards.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Probability theory, Statistics, Inference &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/ml"&gt;Machine Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ah yes, this is where it all started. I had to, of course, eventually finish this course. Andrew Ng (my personal hero) created this course back in April 2012 and I believe it might be now in its 10th iteration. This course will give you a taster of Support Vector Machines, Recommender Systems, Artificial Neural Networks (basis for &lt;a href="http://en.wikipedia.org/wiki/Deep_learning"&gt;Deep Learning&lt;/a&gt;) and many others. The awesome thing about this course was that it abstracts &lt;em&gt;everything else&lt;/em&gt; apart from the ML algorithms to a package that you download so all you need to write is the machine learning algorithm. This means that this course is a mere introduction to machine learning and this alone cannot be considered to give you enough knowledge to actually go out and program some of them in production. This course was just so much fun, because of all the cool stuff that you write that I still have to give it 10/10. Plus, in the right data science tradition, this course has been continuously improved for each iteration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 10/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, MATLAB, Statistics&lt;/p&gt;
&lt;h1&gt;2. Great Courses&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-0"&gt;Computational Thinking and Data Science&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was a great application of Object-Oriented Programming and simulations. I have to say I have gotten so much support on the forums it was incredible. (I do not recall, however, whether this was mostly staff or other students. Either way, great answers.) I think that the course, however, was too short and might have been more rigorous. (Though I do understand this was only a half an MIT course and probably simplified.) But I just admire professor Guttag as a person for being one of the first to get on board with MOOCs. This course was also a very different take on data science from the usual approach, as here you focus on simulations--e.g. behavior of different virus populations based on applications of different drugs/medicines and the genetically inherited (hence the OOP design) resistance to different active components of these drugs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; OOP, Simulations, Stochastic Algorithm Testing&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/cs215"&gt;Udacity Algorithms&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I finished this course before Udacity changed their business model from offering free courses with no deadlines to one where they only offer paid courses (though the material is still free, but you cannot get a certificate). This course was quite hard and the support was sparse (for a MOOC). But this meant that I just had to spend a bit more time thinking about the problem, which was probably helpful in the end to my general CS skills. I think that all the concepts were quite well explained and overall I think that there was sufficient practice to really understand the material. Though there was not so much of a 'course feel', which there was in both Coursera and EdX. But despite this, they recorded &lt;a href="https://www.youtube.com/watch?v=stdG3BGmhqo"&gt;probably the most awesome CS theory song in existence&lt;/a&gt;, so I cannot really question anything about this course. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Algorithms, Data Structures, Social Network Computation &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.udacity.com/course/ud032"&gt;Udacity Data Wrangling with MongoDB&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course is a bit different from all the others: you can either take it for free (but no certificate at the end) or you can pay $200 for each month and have a personal coach, human-verified project at the end and get that certificate. Partially, because I wanted to evaluate Udacity's new business model and partially because I knew that data wrangling is a huge part of data science, I finished this course in about month and a half. Surprisingly there is very little about MongoDB -- it is only introduced in the last 20% of the course, but that does not mean that the course does not teach you helpful skills. It does! Web scraping and XML parsing are definitely useful skills and I think that the course is appropriate in difficulty, though I think that it could be a bit harder at times to really force the students to organize their code well and debug more complex data cleaning parsers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 8/10&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned&lt;/strong&gt;: Website Scraping, Data Wrangling with Python and MongoDB&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/networksonline"&gt;Social and Economic Networks: Models and Analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Normally economists (especially the really famous ones) are not too keen to join MOOCs, but that just makes &lt;a href="https://web.stanford.edu/~jacksonm"&gt;Matthew Jackson&lt;/a&gt; that much more awesome. This class was fun and allegedly a graduate level, though it must have been very diluted, because I found the material (although going to greater depth than the Michigan Course) quite easy. There was again some work with Gephi and this time even Pajek, though sadly no R. But I should note that there were more advanced versions of the course that one could participate in (sadly, with no recognition) that I wanted to participate in, but unfortunately I did not have the time. So I think potentially this could have been an 8/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; More computational and mathematical methods for social network analysis&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://online.stanford.edu/course/statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course covers an explanation of most of a recent book by Trevor Hastie, Robert Tibshirani and others: &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning, with Applications in R&lt;/a&gt; and some of their more advanced book, &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;Elements of Statistical Learning&lt;/a&gt;. This course is fun to take, though I mentioned 'explanation' for a reason. This course will provide you with a thorough explanation of fairly advanced statistical concepts, but definitely will not test you rigorously enough. Usually, especially if you buy the discounted versions of one of the books, you will get enough knowledge and training to apply some of these concepts sensibly. But you will still need to work hard to test yourself and to really probe the ideas with no assistance. Regardless of whether you do, the lectures are really fun to listen to and explain concepts very well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Machine Learning, Intermediate Statistics, Computation&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/initprogjava"&gt;Introduction Ã  la programmation orientÃ©e objet (en Java)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I think I pretty much took this course, because programming in English is too mainstream. But it was really nice to learn a language from 0 in a very organized environment. What I genuinely appreciate is the walk-throughs on how to set up the Java environment in Windows, Ubuntu and Mac. But what I think was by far the best thing about this course was that the actual instructors were &lt;em&gt;so commonly&lt;/em&gt; involved on the forums, helping people. Even I once received some help from one of the instructors, which was great. Coding-wise, the MOOC was quite simple, but I assume approximately on the same level every other intro course for non-CS majors would be. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Java, French, Basic Data Structures&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/bigdata"&gt;IIT's Web Intelligence and Big Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was quite fun and relatively easy to grasp. The concepts revolved around the mathematics (and some programming) of the Big Data technologies, as well as their (recent) history. The problem sets were well done, though sometimes seemed bit too shallow and I would appreciate a bit more programming. The programming assignments were great: tested a whole array of important skills and technologies, but were only three through the entire course. This course was allegedly supposed to be taken as a half-course by actual IIT engineers. I, however, seem quite skeptical of that, as it seemed a bit too easy if true. But generally quite fun and insightful. Though the MOOC seemed to be a bit mass produced with little involvement from the IIT staff after a while.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Map-Reduce and other "big data algorithms" (locality sensitive hashing, page rank etc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/introfinance"&gt;Introduction to Finance&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was my first MOOC I finished so I still remember quite a lot the odd details, as well as some of the catch-phrases the professor was using ("It's not about finance, it's about love.") as well as the fact that people (probably still stuck in the school mentality), were trying to ease their way through the course. I actually learned a lot of interesting concepts I did not know. With this said, I started this course in June 2012, which is just two or three months after Coursera launched. But that meant that some aspects of the course were quite not ready. The platform was great, but I feel like courses tended to be a bit better structured in the later courses. But the fact that I was trying out the first iteration also meant that right now the course could be a 10/10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; Basic accounting, basic financial modeling (but going beyond NPV etc.), Excel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 7/10&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.coursera.org/course/getdata"&gt;Getting and Cleaning Data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course was by the same Jeffrey Leek and Roger Peng that I admired so much and it was part of the data science specialization. But it was just not nearly as good as the previous two. And because by this point I knew that data science is 90% data cleaning and 10% complaining about data cleaning (I think I used this as a joke somewhere, but it &lt;em&gt;might&lt;/em&gt; not be my line, but I could not find the original source.), I wanted to learn more about how to do it efficiently. I wanted to try out the new specialization that the company that I so love and admire was spearheading. But the course was quite short, and although overall well-done, it was just not a very transformative experience. I think that I also really knew about 95% of this course, so I did not even watch most the lectures. Next week I always dived in in hopes of learning something new. I think for a beginner it could be a decent course, but I am still not sure if I would pay $49 for it. Though I understand the tradeoff Coursera was facing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skills learned:&lt;/strong&gt; (potentially) Data wrangling, RegEx, &lt;code&gt;reshape&lt;/code&gt; and other useful packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score&lt;/strong&gt;: 6/10&lt;/p&gt;
&lt;h1&gt;3. Good Courses&lt;/h1&gt;
&lt;p&gt;There good thing about the MOOCs is that if I don't like the course, the instruction seems crappy or I have hard time understanding the professors or I just do not like something, I can drop it. Moreover, there is so many great and awesome courses that I could not even do those that seemed 'good'--i.e. I would most likely like to take them if they were one part of the curriculum at my home institution. &lt;/p&gt;
&lt;h1&gt;Best practices&lt;/h1&gt;
&lt;p&gt;I think there are 3 key strategies to successfully finishing the MOOC. But the key insight is that most people who do not finish a MOOC usually do not finish it because of lack of time, not because of lack of aptitude. So lack of persistence is your key enemy. To crush your enemy, you have to:&lt;/p&gt;
&lt;h2&gt;1. Assess quickly the time commitment of a course.&lt;/h2&gt;
&lt;p&gt;I still remember that &lt;a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/about"&gt;convex optimization course&lt;/a&gt; by Stanford that I really wanted to take. But, alas, I could not. I did not have the time as I had a busy schedule &lt;em&gt;and&lt;/em&gt; I was doing another MOOC (or two?) at the time. I realized that this was a rigorous class that required substantial time commitment. But I needed to drop it, because if you do not take each commitment seriously, you will probably not finish any of them.&lt;/p&gt;
&lt;h2&gt;2. Honor your commitments&lt;/h2&gt;
&lt;p&gt;Meaning also that you should immediately unenroll from the courses you know you cannot finish. This will hopefully also give you the respect towards your commitment that you will also finish the ones you have elected to keep.&lt;/p&gt;
&lt;h2&gt;3. Commit to deadlines, minimum benchmarks&lt;/h2&gt;
&lt;p&gt;There is a reason why I only finished one free course on Udacity: because there are no deadlines (well, until they introduced the payment by the month). These help you get stuff done, because now you have to or all (or at least something) is lost. All major platforms have it now, so please use it.&lt;/p&gt;
&lt;p&gt;Benchmarks are also very helpful: there were courses (like both of the SNA ones) where I knew I wanted a distinction, because I wanted to know the stuff &lt;em&gt;well&lt;/em&gt;. Then there were courses, where I just wanted a brief overview so I only watched lectures (not listed, because I technically, haven't finished them). But decide quickly what your goals are and then follow them.&lt;/p&gt;
&lt;p&gt;I would also love to do a general post on MOOCs, but I think things are best kept separate. &lt;!-- Generally, I think MOOCs will fundamentally change the nature of education (for reasons that deserve a separate post), _provided_ that they manage to meaningfully integrate themselves to people's lives. We could already see many interesting initiatives like the one by &lt;a href="https://www.edsurge.com/n/2014-11-19-free-coursera-pd-thanks-to-the-president"&gt;Obama for teachers for their professional development&lt;/a&gt;. But currently MOOCs are mostly a domain of people who are happy to sacrifice some degree of social life for learning (totally worth it). Though I do not doubt there are exceptions, it seems that most people I know (and online statistics confirm this) are too busy to actually finish the courses.  --&gt;&lt;/p&gt;
&lt;p&gt;Anyway, hope this was helpful. As always, I would welcome any reaction. Get in touch!&lt;/p&gt;</content><category term="R"></category><category term="education"></category><category term="python"></category><category term="reviews"></category></entry><entry><title>On Randomness in (Social) Science</title><link href="jakublangr.com/underspecification-socsci.html" rel="alternate"></link><published>2014-12-06T04:00:00+00:00</published><updated>2014-12-06T04:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-12-06:jakublangr.com/underspecification-socsci.html</id><summary type="html">&lt;p&gt;Slightly technical, but relatively simple introduction to randomness and why it has to be everywhere around us. (Also check out the cool new LaTeX plug-in.)&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while back I was mentioning &lt;a href="/hello-world.html"&gt;how I was confused by the randomness as an emergent property of human systems&lt;/a&gt;, and so I have been giving this some thought, which I thought I might share with the world. The underlying problem always has been why is &lt;a href="http://plato.stanford.edu/entries/properties-emergent/"&gt;emergent&lt;/a&gt; randomness a good model of human behavior. At some level, we understand that our actions are &lt;a href="http://en.wikipedia.org/wiki/Determinism"&gt;deterministic&lt;/a&gt; at least on above-quantum level and I really dare not make any statements about the quantum nature of the universe. &lt;/p&gt;
&lt;p&gt;Before I start the discussion it is worth pointing out that I will try to make this as intuitive as possible, but some math will have to be involved, though I tried to make it so that it is not necessary to understand everything to really understand the point of this article. It was included to make the reasoning a bit easier. At the same time I am making some imprecise statements for the sake of understanding, but if you feel I might be leading people astray, feel free to drop me a line.&lt;/p&gt;
&lt;p&gt;The very definition of randomness is lack of predictability. That seems almost circular, but first thing to note here is that does not mean that all outcomes are equally likely. Think about a pair of dice, and our random variable ("what we measure") as the sum of the two rolls. Here there are seven has six possible combinations (6+1,5+2,3+4,4+3,2+5,1+6), giving us approximately normal distribution centered at seven:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;COUNT           NUMBER OF       PROBABILITY
                COMBINATIONS    
2               1               2.78%
3               2               5.56%
4               3               8.33%
5               4               11.11%
6               5               13.89%
7               6               16.67%
8               5               13.89%
9               4               11.11%
10              3               8.33%
11              2               5.56%
12              1               2.78%
Total           36               100%
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A better definition of randomness is that given an infinite amount of trials of random variable $Y$, we get the true distribution where the outcome of the next realization of said random variable is still unpredictable. So say you are throwing some dice in a casino and you are trying to figure out what is the bet that makes sense where in this odd casino you are meant to bet on the sum of the two dice. The number actually rolled will be a realization  (what you actually rolled) of $Y$ at time $t$. Any given roll will be unpredictable -- otherwise you could easily bet on it -- but it is deterministic -- i.e. if at the point of the throw you had a lot of time and all the laws of physics you could say with 100% certainty what the roll it is going to be. Yet right now, you do not know. However, that does not mean that it does not follow a certain distribution even though it is unpredictable. &lt;/p&gt;
&lt;p&gt;Now there is no doubt that if you come across any given pair of dice and you are asked what do you think the next number will be 7 should be your top choice. (Because it minimizes likely how far off the true number you are likely to be, both because it is in the middle and because it is most likely.) That is what can be thought of as expectation. However, generally the expectation lies in the middle of the distribution, but need not be the most common value. This becomes more obvious if we look at the formula for expectation. It is basically a weighted average of the value ($y$) and the frequency ($f(y)$). (Where $a$ and $b$ are the boundary points.)&lt;/p&gt;
&lt;p&gt;$$
E[Y] = \sum_{a=2}^{b=12} y f(y) \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,  \,\,\,\,\,\,\,\,\,
\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,
$$&lt;/p&gt;
&lt;p&gt;In theory, we can even have a distribution where the expected value is impossible, such as the one below: (for a concrete example imagine student's ratings of a &lt;em&gt;really&lt;/em&gt; controversial class on a scale from -15 to +15, where the two groups are equally split and the average rating is about zero)&lt;/p&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/download.png" alt="Distribution" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;If you look at the formula for expectation above it is all very nice and mathematical. We have our best guess that makes sense, but is far from guaranteeing that we are actually walk back home with a lot of money. But what if you could observe rolls of two dice would they be perfectly distributed according to the distribution that we have specified? Well, no. Dicemakers probably tried to make them as balanced as possible, but probably every pair of dice has certain numbers that will come less or more commonly than simple combinatorics would suggest. This deviations might be due to tiny imperfections on the table, the dice themselves or many other factors. If you find statistically significant relationships, we might be able to derive a marginally better model. But it is only going to do as well as the strength of relationships. Moreover, if we find these associations and do not really understand why -- e.g. we just observe, they might backfire, because if maybe we found some irregularities on the table and then we start throwing on other parts of the table, we will start losing more. Our model did not generalize "out of sample" (on what we did not observe) well.&lt;/p&gt;
&lt;p&gt;Now bring this back to social science. Some people choose to think of the economy as some sort of machine, which suggests determinism. Let's accept that for a second. &lt;/p&gt;
&lt;p&gt;Even if we do we have two problems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The underlying nature of the system keeps on changing. Sure, economy &lt;em&gt;can&lt;/em&gt; be a machine, but it will be a different machine in March 2014 and different in April. &lt;/li&gt;
&lt;li&gt;It does not actually explain why there is any randomness. A machine is deterministic, so shouldn't our predictions be too? It turns out that it is the emergent property (or rather the fact that we have incomplete information) that makes it random. So yes, the world may be deterministic but we need a lot more information that we ever have. Hence we are trying to abstract models that capture the key information. So in our dice example we can notice that the side with one is heavier than the others and hence might come up marginally less often and even though the dice roll is always deterministic, the best heuristic, if we cannot exactly calculate everything, is to use the base rate (how often does 7 naturally occur) plus whatever extra information we may have (heavier sides?).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But randomness only really makes sense if we are actually trying to predict something. We are essentially saying that there is a part that our model (the base rate and extra information together) can predict [1] and a part that our model cannot explain -- i.e. the error term [2]. Basically whenever we don't get our prediction exactly right we get an error -- the difference between actual and expected value. So what we are basically saying is that at this level we can only explain so much and the predicted value is our best guess. That is, after all, what we are doing intuitively with the dice as well: we all intuitively know that predicting always that the outcome is going to be 3 is not a good strategy, unless we know that the dice are biased towards them.&lt;/p&gt;
&lt;p&gt;So really, this problem is not unique to social science, but rather inference in general. Because all of the extra information we have probably included in our prediction. Think about predicting rain: to have rain you need clouds, hence if there is a huge black cloud over your head your prediction with this extra information will already be say 90%, but that is only because we have already built the model (we know) that if there's a could over our heads it it is likely to rain. The fact that it does was not a trivial insight at some point. This is what we are doing in science, except with much more difficult concepts. Hence, randomness has to be an emergent property of &lt;em&gt;any&lt;/em&gt; predictive model with incomplete information. And since we make predictions so often in every science and the error term has to be random (otherwise we could easily figure out a better model), which is why all the good models of complex behavior have to have a random part. But always keep in mind that the distribution of errors can vary, but there is always going to be a random part by definition unless we know the true model (i.e. we have complete information).&lt;/p&gt;
&lt;p&gt;I hope that this was helpful and that you know at least sort of understand why randomness is not a property of human systems, but of prediction with incomplete information and that there are different forms that randomness can put on. But I would love to hear your thoughts and feedback, let me know!&lt;/p&gt;
&lt;p&gt;[1] usually denoted as $E(Y | {\bf X } )$&lt;/p&gt;
&lt;p&gt;[2] usually denoted as $ \epsilon \sim \mathcal{N}(0,1)$, though it can be distributed according to a wide variety of distributions.&lt;/p&gt;</content><category term="technical"></category><category term="social science"></category></entry><entry><title>Web Summit Dublin 2014</title><link href="jakublangr.com/web-summit-dublin-14.html" rel="alternate"></link><published>2014-11-14T20:00:00+00:00</published><updated>2014-11-14T20:00:00+00:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-11-14:jakublangr.com/web-summit-dublin-14.html</id><summary type="html">&lt;p&gt;An amazing week filled with learning about tech and startups, in the presence of inspiring people, cool businesses and most importantly fellow scholars--probably the most awesome group of people in existence.&lt;/p&gt;</summary><content type="html">&lt;p&gt;So I rarely get this emotional about an event, but Web Summit definitely is an experience I will never forget. I was incredibly lucky to be &lt;a href="http://blog.websummit.net/student-blog-announcement/"&gt;selected as one of the 22 initial Student Scholars&lt;/a&gt; to give free attendance to Web Summit Dublin 2014. When I was applying little did I know how awesome this experience will be. When they told us that we were selected out of 6,600 applicants (final figure was apparently at 15,000 for all rounds), I was a bit skeptical. When I saw the list of people I was supposed to be going to the Summit with, my only thought was: "I don't belong here".&lt;/p&gt;
&lt;p&gt;Once it got to term time I did not quite have the brain capacity to worry about this. But somehow, when it eventually got to the Summit itself, I was not sure what to expect. I did not know anyone from the group of scholars and even the arrangements from the Summit with regards to our arrival were very much in the start up mentality -- hectic and required some figuring out. (That is not to blame the team at all, everything that was supposed to work, worked.)&lt;/p&gt;
&lt;p&gt;My 2 am bus ride to catch my 6 am flight was somewhat annoying, but hey, if it is to go such an amazing event, why not. Actually, it feels kind of boring to travel during regular hours. Already on the bus to the hotel, I met up with two other scholars and I immediately felt at home: super interesting people one of whom went to business school the other to medical school. &lt;/p&gt;
&lt;p&gt;On the first day we did some serious sightseeing with those who arrived a bit early, as the first event was in the evening. Some people really wanted to go on the Hop On, Hop Off bus tour, which I though was quite touristy, but felt like I should try it.&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-03%2013.55.12.jpg" target="_blank"&gt; &lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-03%2013.55.12.jpg" alt="Excited tourists" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-03%2013.10.56.jpg" target="_blank"&gt; &lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-03%2013.10.56.jpg" alt="Yay bus tour!" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Definitely no regrets, we got to see a lot of Dublin that we normally would not have any time to see. Then it was time for our first pub crawl. These were integrated into Web Summit and there was even a special pub crawl for the scholars with some of the more successful companies at the Summit. &lt;/p&gt;
&lt;p&gt;It comes as no surprise that the networking was awesome. But what is far more important: it felt really natural. I always hated networking for the sake of networking. I wanted to meet new people and make friends. And if they happen to be useful connections, great. If not, it's still a win, you have an awesome friend. That is really what made it so easy.&lt;/p&gt;
&lt;p&gt;Then we were escorted to dinner at Guinness factory, where on the top floor some speakers (Drew Houston!!!) along with scholars were all having dinner. I think I even shook Drew's hand so that already made me quite happy. Plus I just casually happened to speak to the CEO of Idea Paint. Incredible. &lt;/p&gt;
&lt;p&gt;After the dinner we moved to the Night Summit, which was a street rented out by Web Summit so that the attendees of the Summit can wind down and talk to each other over a pint. I think I came home at about 2 or 3 am. &lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0803.jpg" target="_blank"&gt; &lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0803.jpg" alt="Guiness Dinner with Drew et al" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2008.42.05.jpg" target="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2008.42.05.jpg" alt="Yay start up exhibition hall!" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The next day, it was start of the Summit's main part, which was absolutely &lt;em&gt;mind-blowing&lt;/em&gt;. So many awesome start-ups to talk to, the picture above only barely describe my excitement when I saw the main exhibition hall. But also, there were about seven stages that were full of the super stars of tech community: Tim O'Reilly (sorry for the bad picture), Peter Thiel (I missed the book signing :( ), GitHub CTO and many others. &lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2013.23.04.jpg" target="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2013.23.04.jpg" alt="Tim O'Reilly" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2016.35.56.jpg" target="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2016.35.56.jpg" alt="Peter Thiel" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The Night Summit made me happy to be missing sleep again. But we went to have drinks at the Lord Mayor of Dublin, which was amazing. Though at that time I was already spoiled, as earlier in the day we saw an opening of NASDAQ from Dublin by the Prime Minister of Ireland.&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2014.21.16.jpg" source="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2014.21.16.jpg" alt="NASDAQ Opening. NBD." align="right" style="width: 310px;"/&gt;&lt;td&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2018.18.52.jpg" source="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-04%2018.18.52.jpg" alt="Lord Mayor of Dublin" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Even the last day had so much to offer: Bono, Dana Burnetti and many others. Paddy Cosgrave the main organizer handled everything excellently, though he was pointing out the issues with WiFi and how that is not really their fault a bit too often. (Though to me that did not pose a problem.)&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2017.19.28.jpg" target="_blank"&gt; &lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2017.19.28.jpg" alt="Bono et al" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2017.01.49.jpg" target="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2017.01.49.jpg" alt="Dana Burnetti" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;This was also a week full of incredible occurrences: I randomly happened to speak to the founder of Twitch, without even knowing. One other scholar was just finishing his homework in the lobby of our hotel and happened to stumble upon Dave McClure -- the founder of 500 Start-ups -- the person who just a day earlier said on stage: "If you are going to pitch to my f*cking face [without being invited to], I am not going to listen to you". But at this time Dave was trying to set-up Node.js (&lt;a href="hack-zurich-14.html"&gt;framework that we all love&lt;/a&gt;) and my fellow scholar who happened to know Node got an internship offer for this one act of kindness.&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-05%2022.33.40.jpg" target="_blank"&gt; &lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-05%2022.33.40.jpg" alt="The Kooks!" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;td&gt;
    &lt;td&gt;&lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2014.51.41.jpg" target="_blank"&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/2014-11-06%2014.51.41.jpg" alt="Dave McClure" align="right" style="width: 310px;"/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Other story that I still struggle to believe is that the Kooks happened to be playing without many people even knowing about it in one of the Night Summit venues. I only found out about that less than an hour before the show from a friend of mine when we just happened to be randomly talking about what we are doing tonight. He forwarded me an email (that already was forwarded at least 5 times) that was saying that Kooks will be playing in one of the venues. I don't think that it was that the organizers were trying to hide this event -- it's just that so much else awesome was going on.&lt;/p&gt;
&lt;p&gt;Overall, words cannot describe how amazing this event was. I have learned so much from other Summit attendees, especially the scholars. I have had many interesting, fun and inspiring conversations. So if you are still in university, keep an eye out! Organizers promised they will do it again next year and you don't want to miss that!&lt;/p&gt;</content><category term="Web Summit"></category><category term="non-technical"></category><category term="Dublin"></category><category term="Start-up"></category></entry><entry><title>#hackZurich 2014: A hackhathon to remember</title><link href="jakublangr.com/hack-zurich-14.html" rel="alternate"></link><published>2014-10-12T20:00:00+01:00</published><updated>2014-10-12T20:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-10-12:jakublangr.com/hack-zurich-14.html</id><summary type="html">&lt;p&gt;My experiences at #hachZurich (http://hackzurich.com/): 30+ hours of coding, 5 hours of sleep, 14 hours of transportation door-to-door and back. One epic weekend.&lt;/p&gt;</summary><content type="html">&lt;p&gt;When you work with a lot of engineers, you start to envy all those cool personal projects they have and all those cryptic plug-ins they know and have mastered. A lot of this happened at Hackathons. So I wanted to try one myself to have a proper experience of trying to code up an app from scratch in less than 42 hours. So when I found about hackZurich, I was sold.&lt;/p&gt;
&lt;p&gt;In all seriousness, I was. The hackathon looked &lt;em&gt;amazing&lt;/em&gt;: more sponsors than any society or event I've seen. Cool APIs opened only for hackZurich and &lt;em&gt;they even reimbursed most of travel costs.&lt;/em&gt; The only annoying this was that this event was happening around exam time. But seeing as I am writing elaborate odes on this, you can probably guess what I chose. &lt;/p&gt;
&lt;p&gt;Except it was not that easy: I knew I wanted to go to a hackathon, except I've never been before. And there was an application process. Realistically, I am not sure how selective it was, but there was a cap of 300 people and given that there are always freebies and free food, I think hackathons can be quite appealing.... at least for me. But I mean we had to sleep on the ground, on the tables and on our bags and some people did not even have sleeping bags. I do not want it to sounds like I am complaining. No one actually slept a whole lot (I slept about 4 hours first night and one hour the second) so no one really cared. I found my own non-lit corridor and slept on the floor, because I really need quiet environment. But hey! It was a fun experience and I was worried I am getting a little snobby. &lt;/p&gt;
&lt;p&gt;So hacking! We had about 30-40 hours to code an app on any topic, but noting of course that some companies gave us exclusive (and some promoted only their existing) &lt;a href="http://en.wikipedia.org/wiki/Application_programming_interface"&gt;APIs&lt;/a&gt; for us to try something new. Except, as a person knowledgeable mostly of the data science stack, I soon realized that I will need to go back to coding some web apps to have something to show. That, however, was a problem, because I have not coded up a web app in over a year. So I was afraid that I simply may not be up to the challenge skill-set wise. &lt;/p&gt;
&lt;p&gt;Fortunately, organizers created a Facebook group for people to get together and discuss their preliminary ideas, but standard hackathon rules apply: no code before the official kick-off. So I joined the group and was discussing ideas with people and was quite happy to see that a lot of people were not extremely concerned about the lack of specific skills. &lt;/p&gt;
&lt;p&gt;Despite this, I was quite nervous, because this was my first hackathon and I was not quite sure what to expect. I teamed up with a friend of mine, because I figured going to a hackathon in a different country in a busy period and coding in technologies I have never mastered and even the little experience I had was long time ago, it is going to be challenging as is.&lt;/p&gt;
&lt;p&gt;We roughly discussed the idea beforehand, but as my head was full of Mathematical Methods exam that I was supposed to be taking the day before leaving, I did not quite do justice to preparation as a lot of Node JS I needed to refresh on the go and that slowed me down considerably. But let's not jump the gun. &lt;/p&gt;
&lt;p&gt;After finishing my exam on Thursday and doing chores until my departure from Oxford shortly after noon, I was quite freaked out: I still haven't really unpacked and I was already going places. But the trip was cool: I was flying from London City Airport for the first time, which was quite exiting. &lt;/p&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0752.jpg" alt="Destination: #hackZurich" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;Despite not knowing anything about Zurich, I perfectly navigated and caught the pre-planned (from about a week ago!) train straight to the hackathon venue, which made my day. Hackathon itself was held at a &lt;a href="http://technopark.ch"&gt;TechnoPark&lt;/a&gt;, which seemed to be a quite modern open space for start-ups and small companies. Upon arrival, I got my badge and bag of goodies that I still haven't fully explored, because there's just &lt;em&gt;so much stuff.&lt;/em&gt;  (Yay!) &lt;/p&gt;
&lt;p&gt;Anyway, I unfortunately missed the opening ceremony, but I arrived just in time for the pretty cool workshops: Evernote had an introduction to their APIs and Software Development Kits and then I went to presentation about Google wearables. Where Google-certified android master (presenter) sent a guy who wanted to go to the Apple workshop to the 6th floor (where there was no workshop) and after the poor guy left, he said: "Ooops, I must have been using Apple Maps." The sass is strong with this one. &lt;/p&gt;
&lt;p&gt;After the workshops, &lt;strong&gt;let the hacking commence.&lt;/strong&gt; My friend and I were using MEAN stack and &lt;a href="http://d3js.org/"&gt;D3.JS&lt;/a&gt; to build a budget tracking app. That is &lt;a href="http://mongodb.org"&gt;MongoDB&lt;/a&gt;, &lt;a href="http://expressjs.com/"&gt;Express.JS&lt;/a&gt;, &lt;a href="https://angularjs.org/"&gt;Angular.JS&lt;/a&gt; and &lt;a href="http://nodejs.org/"&gt;Node.JS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was challenging doing all this coding, but I have to say, quite fun. The atmosphere was amazing and a ton of caffeinated drinks around. Here and there was an occasional food shortage, but hey: free food. After the kick-off people, obviously, started coding. And they coded... until 430 am. At which point I decided that it &lt;em&gt;might&lt;/em&gt; be a good idea to get some rest and so I finally managed to find a nice hallway to sleep in by about 5 am. &lt;/p&gt;
&lt;p&gt;&lt;img src="https://scontent-a.xx.fbcdn.net/hphotos-xpa1/v/t1.0-9/s720x720/10689898_10203303761088371_4719824619037743016_n.jpg?oh=e9683900216a3692b0ca916a2be2f27c&amp;oe=54AE910E" align="center" alt="4 am #hackZurich" style="width: 620px;" /&gt;&lt;/p&gt;
&lt;p&gt;You think that was bad? Check out where some of these poor souls have been sleeping:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://pbs.twimg.com/media/BztdrGDCEAMmoY6.jpg:large" align="center" style="width: 620px;" alt="People sleeping on the floor."&gt;&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0772.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;td&gt;
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0777.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Despite my effort to stow myself away I was woken up several times by people sneaking through my corridor to find an even more obscure corridor to sleep in, which sometimes looked quite weird, not to mention awkward, especially if you make eye contact. Anyways, I woke up by 8:30 am to sounds of some radio (&lt;strong&gt;rage&lt;/strong&gt;) and since I was up and I got breakfast and started working. Morning was quite productive, but after a while we got quite tired and hungry and so we went on a walking tour of Zurich that organizers kindly organized for us.&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0762.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;td&gt;1
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0765.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0767.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;td&gt;1
    &lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/IMAG0771.jpg" alt="Finders, sleepers" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;I have to say Zurich is quite an amazing city. It has a very picturesque, small-town feel and it really feels too fairly-taily to be true.  &lt;br&gt;&lt;/p&gt;
&lt;p&gt;The next night was a bit more rough as we had issues with our database and Node.JS not being synchronized, which is one of the pains of working with asynchronous stacks, which took us quite a long time to fix. I stopped working at around 4:30 and got up at 6 something to go finish our app before the 930 am deadline.  &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, our app did not make it into finals--as those were only for 1/4 of the participants--but this hackathon was definitely awesome. Would do again.  &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Seeing as I yammered for too long again, I will leave it at that, but I will be back for more!&lt;/p&gt;</content><category term="hackathon"></category><category term="non-technical"></category></entry><entry><title>Tracking the CrISIS in Syria</title><link href="jakublangr.com/tracking-crisis.html" rel="alternate"></link><published>2014-09-22T21:00:00+01:00</published><updated>2014-09-22T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-09-22:jakublangr.com/tracking-crisis.html</id><summary type="html">&lt;p&gt;Exploring the power of Big Data approach (GDELT) applied to the civil war in Syria to get unconventional insights into the Syrian civil war and the terror tactics of ISIS.&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are interested more in the analysis of the conflict from a quantitative persective and not so much in the methodology, you &lt;a href='#explore'&gt; can jump straight there.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As I was working as a contractor this summer, I came across a very interesting project called &lt;a href="http://www.gdeltproject.org/"&gt;GDELT&lt;/a&gt; -- Global Database of Events, Language and Tone. This project I think is best described by the official website statement: "GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world."&lt;/p&gt;
&lt;p&gt;In addition to that, as a part of &lt;a href="../hello-world.html"&gt;Good Judgement Project&lt;/a&gt;, I was made to forecast on Syria and ISIS' actions. As I was going through the articles describing what is happening in Syria, I became to wonder: what is the bigger picture? To what extent is this hype all just because &lt;strong&gt;now&lt;/strong&gt; ISIS is the headline-grabber in the West? (But of course, no one cares until it becomes a story in the public consciousness.) That's when GDELT comes into the picture. I wanted to try it as I came across it, but since I was finishing my data science project, I did not quite have the time. I was awe-struck by the complexity and potential of it so I knew I wanted to give a go. I have not really used R all that extensively in about a year since my internship, but I am not afraid of large analytics, so I decided to refresh those skills.&lt;/p&gt;
&lt;p&gt;Fortunately, Google was so nice as to host this almost 100 GB dataset on &lt;a href="http://googlecloudplatform.blogspot.cz/2014/05/worlds-largest-event-dataset-now-publicly-available-in-google-bigquery.html"&gt;its cloud for free&lt;/a&gt;, for which I love Google. Not only that, but Google also loaded this dataset so that it can be queried using &lt;a href="http://en.wikipedia.org/wiki/BigQuertracky"&gt;BigQuery&lt;/a&gt; using public-facing APIs or a web interface. We can then query GDELT using SQL-like language as shown below and subsequently export the dataset as a CSV:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gdelt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bq&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;full&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; 
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MonthYear&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;201303&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;2015&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;NumSources&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;GoldsteinScale&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt;
&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Actor1Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="k"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(For those interested, I include a smaller version of the dataset I was working with after exporting from GDELT &lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/sample.csv"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The assumption of this endeavor is that if I include non-English media (and perhaps the less main-stream ones too) by using GDELT's information rather than Google, which will always give me traditional sources, I can get a better sense of what is going on. Thanks to the the law of large numbers, I should not get too many false positives from spurious sources, so my overall analysis should remain robust and even more objective than relying on sources such as CNN or BBC alone. One additional advantage GDELT has is that it is updated every 24 hours. In theory, this analysis could be automated so that it updates itself every 24 hours. (Actually, it should not be &lt;em&gt;that&lt;/em&gt; much more work ... but given the size of GDELT, I would have to start paying for using the BigQuery service that often.)&lt;/p&gt;
&lt;p&gt;As with any more substantive project, there was a lot of data cleaning and obstacles. Namely: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ISIS not recognized by GDELT&lt;/strong&gt;:
There's lots of International Militarized Groups (IMGs) recognized by GDELT, but ISIS seems to be too new in the headlines to be recognized by GDELT coding yet. Hence I had to look for substitutes:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMG (International Militarized Group) tag to the rescue&lt;/strong&gt;:
There is a column specifying actor &lt;em&gt;type&lt;/em&gt; (not actor name!) for each event. There is an actor type IMG, which was fortunate. However, there is no way of fully knowing whether said group is ISIS or not from this dataset. (At least with this level of granularity, but I am sure it would be technically possible, if GDELT team so chose.) I would also imagine that the fighters on the ground themselves are often not quite sure, given the number of splinter groups these organizations harbor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;But sometimes "REB"&lt;/strong&gt;:
When individually inspecting the events (just some of them, I do not have the time to go through the entire data dump), I quickly realized that the data pertaining to clearly ISIS attacks are sometimes labeled as "REBEL" actor type. So I needed to include both in the analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cleaning the data&lt;/strong&gt;:
For some events it is hard, if not impossible, to correctly specify precise coordinates. As a result, GDELT merely inputs the values of the approximate center of Syria. These records obviously needed to be removed from the dataset. Obviously, there was couple of more steps that I needed to do and a couple that tricked me for a little bit, for instance, when I found that the state code of one of the events was &lt;em&gt;IS&lt;/em&gt;, which, however, stood for Israel, as I later found out.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GDELT project classifies each event using &lt;a href="http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt"&gt;one of their internal codes&lt;/a&gt;, each of these codes then gets then a &lt;a href="http://web.pdx.edu/~kinsella/jgscale.html"&gt;Goldstein score&lt;/a&gt;, which basically expresses whether this event whether people are being nice to each other (positive score) or mean/evil towards each other (negative score). Where +10.0 is some massive humanitarian/developmental aid project, +1.0 is probably visit of some minor national politician. All the positive things were filtered out, because I wanted to focus on what the problems were. There -1.1 is, for instance, "Deny an attributed policy, action, role or position" and -10.0 is "Military attack; clash; assault".&lt;/p&gt;
&lt;p&gt;As the more SQL-knowledgeable of you might have noticed, I only considered events that came from 2 independent sources and happened on or after March 2013. &lt;/p&gt;
&lt;h1 id='explore'&gt; Let's explore &lt;/h1&gt;

&lt;p&gt;With this dataset in hand, I made several maps where I plotted each event as a point sized according to its Goldstein score and set relatively low transparency so that all hotspots appear in bright red. I plotted this dataset by month, and here's March 2013 till July 2013. We see that at the end of this period, there's was violence spearing from its initial hotspots. &lt;/p&gt;
&lt;h3&gt;March and April 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201303.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201304.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;May and July 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201305.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201307.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Now, if we look at the period August till November, we see that there's a lot more violence then. But to some extent we have to bear in mind that this dataset includes &lt;em&gt;all&lt;/em&gt; rebels and not just ISIS. &lt;/p&gt;
&lt;h3&gt;August 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201308.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;September 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201309.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;October 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201310.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;November 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201311.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we have to consider the connected events: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=Syria,+Chemical+Weapons&amp;date=1/2013+12m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Here, I am using Google Trends as a proxy for "Western interest" in the matter. We can see that the use of chemical drew Western attention to Syria, however, this interest was very short-lived.&lt;/p&gt;
&lt;p&gt;Clearly, August and September are easily explained by the usage of chemical weapons. But the violence following this, especially in October and later, cannot be explained just by looking at chemical weapons, because that is when the hype ended and has not picked up until ISIS was the next important thing.&lt;/p&gt;
&lt;p&gt;The following months were similarly violent and few in the West seemed to care. &lt;/p&gt;
&lt;h3&gt;December 2013 and January 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201312.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201401.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;February and March 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201402.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201403.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;April and May 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201404.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201405.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;If we plot the Google trend graph again, since the start of Syrian Civil War in March 2011, we see that the attention Syria got around the times of the use of chemical weapons is unparalleled. This is hardly surprising, but what is notable is that there a &lt;a href="http://www.cbsnews.com/news/syria-chemical-weapons-attack-blamed-on-assad-but-wheres-the-evidence/"&gt;disagreement regarding who actually used them&lt;/a&gt;, though I think that at least some of these attacks were sure to be made by the government. &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=Syria&amp;date=1/2009+43m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_1&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;But despite the limited interested by the West, violence was actually similar to the months during which the chemical weapons were used. &lt;/p&gt;
&lt;p&gt;What is interesting that ISIS was not picked up by Western media much until recent months, despite the fact that ISIS can be traced back to 2004: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=%22Islamic+State%22,+ISIS,+ISIL&amp;date=today+12-m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Still, violence was hovering around the same level:&lt;/p&gt;
&lt;h3&gt;June and July 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201406.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201407.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;August and September 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201408.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201409.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;It is interesting that it seems that despite the limited budget that ISIS has for its media operations, its terror tactics are largely successful: Syria is not, at least by according to this analysis, by and large, more violent than it was in the few months prior to ISIS' inclusion in the Western media's limelight. &lt;em&gt;Yet&lt;/em&gt;, ISIS get substantial amount of attention, so much so that it prodded United States and &lt;em&gt;five Arab&lt;/em&gt; states (no European states, however, at the time of writing) to launch a military operation starting today. This article used a lot of proxies so I do not want to make grandiose claims that cannot be supported by this analysis.&lt;/p&gt;
&lt;p&gt;Nevertheless, I think what this article shows that ISIS first and foremost is unparalleled by its usage of social media compared to other terrorist organiztations and splinter groups. If anything, it seems that the number of violent occurrences has decreased. But the terror-tactics and the headline-grabbing strategy of ISIS is already working well for them. West pays attention. &lt;/p&gt;
&lt;p&gt;At the same time, the brutality and the casualities is hard to really compare, because the information about the previous comparable systems (e.g. &lt;a href="http://en.wikipedia.org/wiki/Islamic_Emirate_of_Afghanistan"&gt;Islamic Emirate of Afghanistan&lt;/a&gt;) is very limited and we might just never get as complete picture as we have now. But now, largely thanks to GDELT, we have a much more complete dataset about the conditions within the Islamic State and so it might prove easier to benchmark the level of violence and discord in this society against all other -- be it present or future -- states. &lt;/p&gt;
&lt;p&gt;I am currently working on an interactive R-Shiny app for this visualization/analysis so stay tuned!&lt;/p&gt;
&lt;p&gt;Want the R script? Want to give feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</content><category term="modeling"></category><category term="GDELT"></category><category term="R"></category><category term="Syria"></category></entry><entry><title>Predictions R hard</title><link href="jakublangr.com/hello-world.html" rel="alternate"></link><published>2014-09-14T21:00:00+01:00</published><updated>2014-09-14T21:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-09-14:jakublangr.com/hello-world.html</id><summary type="html">&lt;p&gt;Quick and dirty statistical modeling in R for social scientists, decision makers and enthusiasts. Insights from my involvement in a US intelligence project.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;... or quick and dirty statistical modeling in R&lt;/h1&gt;
&lt;p&gt;Recently, I decided to start a blog and my ongoing involvement in the &lt;a href="http://goodjudgmentproject.com/"&gt;Good Judgment Project (GJP)&lt;/a&gt;, which is part of US government's &lt;a href="http://www.iarpa.gov"&gt;Intelligence Advanced Research Projects Activity (IARPA)&lt;/a&gt;, more specifically the ACE Project, proved to be a decent excuse to do so. &lt;/p&gt;
&lt;p&gt;I have joined GJP mid-Season 3 (hence I haven't participated fully) and then I tested into season 4 (combination of general intelligence test and political knowledge). This test alone was a treat, because it followed all the guidelines about best practices of hiring (to my knowledge) and I knew that if I get in, this will be a lot of fun. What I did not know at the time was that only one in ten will actually be accepted into GJP, otherwise I would probably put in a lot more work into the test.&lt;/p&gt;
&lt;p&gt;Despite this mess-up, I joined a project and met my amazing fellow forecasters in my team. At the time of joining, I already knew that I will be putting numeric probabilistic values on how the world might soon turn out to be (e.g. will there be a direct Russia-Ukraine confrontation in Crimea). All of these questions are well-defined using a very sophisticated set of definitions, but making the evaluation criteria very flexible and as to allow GJP Team to 'resolve' each question in the spirit rather than the letter of the question.&lt;/p&gt;
&lt;p&gt;Number of these questions have allowed for use of publicly available datasets and so I would like to offer some code that I used to ease up my work as a forecaster. This particular example is relates to a question that resolved yesterday: If on a certain date the area of ice on the Arctic sea will be more than what it was last year.&lt;/p&gt;
&lt;p&gt;The following R code downloads, cleans and plots some time-series representation of the record ed data of the ice for the past couple of years. It is &lt;em&gt;super-simple&lt;/em&gt;, but I was able to do it from question to quantitative probabilities in about 30 minutes, so this is just to demonstrate that even such simple code can help you make a much more solid prediction. This was necessary, because my team's forecasts were all over the place (among 9 forecasters we ranged 5-80%) and statistical modeling is a great way how to deal with data that is normally quite counter-intuitive and complex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;TTR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reshape&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tseries&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;forecast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Loading, cleaning (missing values are by default assigned value of -9999)&lt;/span&gt;
read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ijis.iarc.uaf.edu/seaice/extent/plot_v2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
&lt;span class="kp"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-9999&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
&lt;span class="kp"&gt;colSums&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Melt changes the columns (there is a column for each year) into just one column &amp;#39;value&amp;#39;&lt;/span&gt;
data_ts &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; melt&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
ts &lt;span class="o"&gt;=&lt;/span&gt; ts&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt; deltat&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data_ts &lt;span class="o"&gt;=&lt;/span&gt; na.remove&lt;span class="p"&gt;(&lt;/span&gt;ts&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Decomposes the time-series plots the decomposition and the forecast (as well as prints it)&lt;/span&gt;
decomposed_ts &lt;span class="o"&gt;=&lt;/span&gt; decompose&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;decomposed_ts&lt;span class="p"&gt;)&lt;/span&gt;
ts_forecast &lt;span class="o"&gt;=&lt;/span&gt; HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt;gamma&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts &lt;span class="o"&gt;=&lt;/span&gt; forecast.HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;ts_forecast&lt;span class="p"&gt;,&lt;/span&gt;h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts
plot.forecast&lt;span class="p"&gt;(&lt;/span&gt;forecasts&lt;span class="p"&gt;)&lt;/span&gt;
data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;250&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;260&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For GJP purposes, the confidence intervals produced by this model are especially useful, because they give you an idea of how confident you can be in your predictions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; forecasts 
Point Forecast Lo &lt;span class="m"&gt;80&lt;/span&gt; Hi &lt;span class="m"&gt;80&lt;/span&gt; Lo &lt;span class="m"&gt;95&lt;/span&gt; Hi &lt;span class="m"&gt;95&lt;/span&gt; 
&lt;span class="m"&gt;16.73710&lt;/span&gt; &lt;span class="m"&gt;4981786&lt;/span&gt; &lt;span class="m"&gt;4920648&lt;/span&gt; &lt;span class="m"&gt;5042924&lt;/span&gt; &lt;span class="m"&gt;4888284&lt;/span&gt; &lt;span class="m"&gt;5075288&lt;/span&gt; 
&lt;span class="m"&gt;16.73995&lt;/span&gt; &lt;span class="m"&gt;4963324&lt;/span&gt; &lt;span class="m"&gt;4867154&lt;/span&gt; &lt;span class="m"&gt;5059494&lt;/span&gt; &lt;span class="m"&gt;4816245&lt;/span&gt; &lt;span class="m"&gt;5110403&lt;/span&gt; 
&lt;span class="m"&gt;16.74279&lt;/span&gt; &lt;span class="m"&gt;4944862&lt;/span&gt; &lt;span class="m"&gt;4814956&lt;/span&gt; &lt;span class="m"&gt;5074768&lt;/span&gt; &lt;span class="m"&gt;4746188&lt;/span&gt; &lt;span class="m"&gt;5143536&lt;/span&gt; 
&lt;span class="m"&gt;16.74564&lt;/span&gt; &lt;span class="m"&gt;4926399&lt;/span&gt; &lt;span class="m"&gt;4762199&lt;/span&gt; &lt;span class="m"&gt;5090600&lt;/span&gt; &lt;span class="m"&gt;4675277&lt;/span&gt; &lt;span class="m"&gt;5177522&lt;/span&gt; 
&lt;span class="m"&gt;16.74849&lt;/span&gt; &lt;span class="m"&gt;4907937&lt;/span&gt; &lt;span class="m"&gt;4708313&lt;/span&gt; &lt;span class="m"&gt;5107562&lt;/span&gt; &lt;span class="m"&gt;4602638&lt;/span&gt; &lt;span class="m"&gt;5213237&lt;/span&gt; 
&lt;span class="m"&gt;16.75134&lt;/span&gt; &lt;span class="m"&gt;4889475&lt;/span&gt; &lt;span class="m"&gt;4653078&lt;/span&gt; &lt;span class="m"&gt;5125872&lt;/span&gt; &lt;span class="m"&gt;4527937&lt;/span&gt; &lt;span class="m"&gt;5251014&lt;/span&gt; 
&lt;span class="m"&gt;16.75419&lt;/span&gt; &lt;span class="m"&gt;4871013&lt;/span&gt; &lt;span class="m"&gt;4596412&lt;/span&gt; &lt;span class="m"&gt;5145614&lt;/span&gt; &lt;span class="m"&gt;4451047&lt;/span&gt; &lt;span class="m"&gt;5290979&lt;/span&gt; 
&lt;span class="m"&gt;16.75704&lt;/span&gt; &lt;span class="m"&gt;4852551&lt;/span&gt; &lt;span class="m"&gt;4538291&lt;/span&gt; &lt;span class="m"&gt;5166811&lt;/span&gt; &lt;span class="m"&gt;4371932&lt;/span&gt; &lt;span class="m"&gt;5333170&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the plots here can help elucidate why my team might have been all over the place: there are many factors influencing how much ice there will be on a given day: sure, there is the seasonal (summer-winter) cycle, which matters a lot, but there is also some random element as well as measurement error. I fear that lots of my fellow forecasters (perhaps including myself for a long time) fell victim to &lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;over-fitting&lt;/a&gt; to the random element. Sure, we could observe the pattern from the previous year, but no one from the team--to my knowledge--plotted the data to see what (approximate) influence each of the two factors have. This is why decomposition is so great:&lt;/p&gt;
&lt;h2&gt;Decomposed time-series plot&lt;/h2&gt;
&lt;p&gt;&lt;img src="Public/blog/gjp_modelling_ts_decomposition.svg" alt="Timeseries decomposition of ice-area" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;One interesting element of the decomposition is the trend, which I was expecting to see fairly clear evidence of global warming, but instead I really found out "just" evidence of climate change. Though, of course, I am no climatologist, so perhaps that interesting dip in the year 2000 had some other (spurious?) reasons. &lt;/p&gt;
&lt;p&gt;The decomposition, although helpful, was not what I considered the main advantage of this little exercise: I was then able to use a time-series statistical model called HoltWinters to predict the area of ice (no pun intended?), which uses exponential smoothing that is quite fitting in these conditions (I wonder whether these statistical puns significantly affect my readership?). This model is appropriate mainly because there is a lot of historical data and the data is always very constrained to a certain range (e.g. a 3 sigma change is &lt;em&gt;actually&lt;/em&gt; extremely unlikely, which cannot be said of all financial data this model is applied to). &lt;/p&gt;
&lt;h1&gt;Thoughts about GJP itself&lt;/h1&gt;
&lt;p&gt;GJP is what I think social science should be: quantifiable, based on best-practices and aiming to get as close to the truth as possible, not tell the best story. This was, interestingly enough, the conclusion (&lt;em&gt;simplifying tremendously&lt;/em&gt;) of Professor Philip Teltock in &lt;a href="http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715"&gt;his book Expert Political Judgment&lt;/a&gt;. Namely that the political forecasters that tell the best story are usually favored by the media, because it sells well, but they tend to do worse than most others on accuracy. Professor Tetlock is now one of the main organizers of GJP and so it is a great honor to work as his lab rat, because it helps to do cutting-edge research and it helps me to learn a ton in the process.&lt;/p&gt;
&lt;p&gt;One additional gift every forecaster got from the GJP was a set of presentations of best practices of forecasting, which I think were fascinating, albeit oftentimes simple guidelines to stick to. But as with everything, it is much more a matter of putting these into practice rather that makes a great forecaster. &lt;/p&gt;
&lt;p&gt;Couple of interesting things that I noticed about myself looking at old predictions: &lt;/p&gt;
&lt;h2&gt;1. I am overly cautious&lt;/h2&gt;
&lt;p&gt;Reading loads of literature about randomness and its role in human society (Kahneman, Taleb, Tversky, Ariely, Mlodinow etc.) I was overly careful about putting down anything close to 1% or 99%, even though some questions call for it (likelihood of a massive reform of international institutions in a short timespan). Good Judgment, after all, is not only very well calibrated, but it is also very discriminating. &lt;/p&gt;
&lt;h2&gt;2. I struggle with randomness&lt;/h2&gt;
&lt;p&gt;Interesting point about randomness--to some extent related to the previous one--how do we square the fact that randomness is just a model and yet we keep on invoking the properties of randomness when talking about the world. But, at least on the level of humans, each action is supposedly deterministic. Yes, some might call on the &lt;a href="http://www.quantumdiaries.org/2014/07/04/wrong/"&gt;famous George Box quote&lt;/a&gt;, but that does not really answer why in a situation of imperfect information &lt;em&gt;random&lt;/em&gt; is the &lt;em&gt;best&lt;/em&gt; model. This is especially confusing given the amount of strategic interaction and social influence in the world. &lt;/p&gt;
&lt;p&gt;This turned out to be a problem with with some of the GJP predictions I make, because I want to stick to randomness as a baseline (because it seems to be best practice), but then struggle to always justify this to myself and square it with the rest of information. This especially bothered me outside of GJP, however. In Mlodinow's book &lt;a href="http://www.amazon.com/The-Drunkards-Walk-Randomness-Rules/dp/0307275175"&gt;The Drunkard's Walk&lt;/a&gt;, he specifically used the example of production studio's CEOs as someone who is faced with great randomness in the movies. But each action in this immensely complex chain of interactions is deterministic, strategic and influenced by one's surroundings. Why is randomness an emergent property of human systems? Seems non-obvious.&lt;/p&gt;
&lt;h2&gt;3. Conditional interaction is hard&lt;/h2&gt;
&lt;h3&gt;(especially cross-culture)&lt;/h3&gt;
&lt;p&gt;There was a question on what would DPRK do, should the United States take a bold, but merely supportive, military action in aid of South Korea. Our team struggled to agree even if the bold action would increase chance of state DPRK being more aggressive towards South Korea or less so. Would the desire to prove something to US (or more generally, the West) outweigh the potential risks, because of the importance of the precedent or would the bold action scare off DPRK? Both ways of reasoning sounded equally plausible to me at the time so I just followed what my initial prediction was. But really, there was no reason for it. The question whether I got it right or wrong is irrelevant in this case; the question is: would I be able to make a good prediction next time? I fear that so far the answer is no and so I never swayed too far off 50%. I will probably need to devise a way of breaking these ties.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</content><category term="modeling"></category><category term="R"></category><category term="statistics"></category><category term="GJP"></category></entry><entry><title>About Me</title><link href="jakublangr.com/about.html" rel="alternate"></link><published>2014-09-14T20:00:00+01:00</published><updated>2014-09-14T20:00:00+01:00</updated><author><name>Jakub Langr</name></author><id>tag:None,2014-09-14:jakublangr.com/about.html</id><summary type="html">&lt;p&gt;About Me&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Me" src="https://dl.dropboxusercontent.com/u/30848031/blog/WebSummit_011.JPG" title="Welcome to my blog!"&gt;&lt;/p&gt;
&lt;p&gt;By this point you probably are already aware of the fact that this is a blog of a slightly eccentric individual. And for some reason decided to stay. You're a persistent one, aren't you?&lt;/p&gt;
&lt;p&gt;I went to Oxford, studied &lt;a href="https://www.theguardian.com/education/2017/feb/23/ppe-oxford-university-degree-that-rules-britain"&gt;very questionable degree&lt;/a&gt; and did some statistics/economics. I also was kind of crazy about the whole Massive Online Open Courses thing and forgot that real world existed. Then one day, after an intervention from my friends and family, I realized I should probably get off these MOOCs, but it was too late. I already finished 18 of them. But hey, at least I can remember playing with machine learning, data science, statistics, computer science and all the things awesome.&lt;/p&gt;
&lt;p&gt;I was working in Data Science at a couple of startups (Mudano and Filtered, plus some smaller ones), at Entrepreneur First, Pearson Plc (all in London last two years), but I also have full-time like experience from international consultancies and NGOs. &lt;/p&gt;
&lt;p&gt;Now I spend some time doing improv and updating this website.&lt;/p&gt;
&lt;p&gt;If you want to, feel free to get in touch at james [dot] langr [at] gmail [dot] com or via the LinkedIn profile in links! &lt;/p&gt;
&lt;p&gt;My resume available &lt;a href="http://goo.gl/sEUFMa"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="me"></category><category term="CV"></category></entry></feed>