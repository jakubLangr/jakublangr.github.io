Title:  Artificially Intelligent Business
Date: 2017-03-14 22:00
Tags: python, AI, reviews, StartUps
Category: technical
Slug: ai-2016-review
Author: Jakub Langr
Summary: AI is moving forward at an amazing rate but we should take the time to appreciate all the amazing potential of the recent advancements. I want to take the time to get some technical appreciation for the business implications the recent AI may have.

Lots of people are arguing about the theoretical nature of what AI means many years down the road, but I would like to focus on the AI commercial products in the next 2-5 years. I'd also bring all the incredible, but deeply technical advances back to the real world and business and focus on and focus on what businesses and consumers can practically expect of the future. First off I will talk about the most interesting advances in AI, second what the current applications of those advances are and wrap it up with what this means for business.

At the same time, this is __not__ intended to be a discussion of the business theory behind AI-startups. This has been covered by more qualified people, for [example here](http://venturebeat.com/2017/03/13/10000-ai-startups-need-to-learn-these-lessons/){:target="_blank"}.

** 1. Breakthroughs in AI **

Last couple of months were excellent for AI: everything from [One-Shot-Learning](https://en.wikipedia.org/wiki/One-shot_learning){:target="_blank"}, advances in Generative Adversarial Networks (GANs) and Memory-Augmented Networks to practical applications such as AlphaGo, [Liberatus](http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players){:target="_blank"} winning the world Poker championship and a neural net creating images that match textual input. 

There have been way too many practical applications to list, but all of these are a results of more theoretical work from prior years. Therefore, in order to look at what future is likely to hold, we should look at the theoretical advancements and extrapolate what kind of practical tasks these novel techniques can achieve in production. One such area is [reasoning AI](https://arxiv.org/pdf/1506.07285.pdf){:target="_blank"}: memory augmented and differentiable networks are likely to bring in a new era of higher level reasoning: as recent [Stanford papers](https://www.youtube.com/watch?v=eyovmAtoUx0&t=4s){:target="_blank"} and [recent Facebook tools to help visually impaired understand content of images](https://code.facebook.com/posts/457605107772545/under-the-hood-building-accessibility-tools-for-the-visually-impaired-on-facebook/){:target="_blank"} demonstrate. So reasoning and Q&A AI is just around the corner.
In terms of future theoretical breakthroughs, the most fruitful areas are likely to be:

> Further improvements on GANs, as they already are being tested in the field of [3D object generation](https://www.youtube.com/watch?v=kf-KViOuktc ){:target="_blank"}, [remastering old movies](https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503){:target="_blank"} and [image enhancement](https://www.youtube.com/watch?v=WovbLx8C0yA ). Even one of the fathers of deep learning, [Yann LeCunn, called it](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/ ){:target="_blank"} “the most important [recent advancement in deep learning]”.

> Further improvements on memory/attention as demonstrated for instance by a [recent study by DeepMind published in Nature](https://deepmind.com/blog/differentiable-neural-computers).

<!--It does not really matter what the names of these advances are, though if you are interested, feel free to read more about them.
--> 

** 2. Summary of state of the art, future improvements and likely future paths **

The biggest breakthrough, [in line with LeCunn]((http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/ ){:target="_blank"}, I think will come from Generative Adversarial Networks (GANs), so I would like to focus on them as they are especially impressive in the case of [Adversarial and Semi-Supervised Learning](https://ishmaelbelghazi.github.io/ALI){:target="_blank"}. In essence, this approach tries to get around the problem of not having enough labelled data. This is a problem that everyone maybe except the likes of Google faces. Hence this breakthrough can kick-start a new wave of AI companies in any sufficiently technically savvy ecosystem, such as London. The results are already extremely promising based on research from one group in Montreal (I highly recommend reading [this link](https://ishmaelbelghazi.github.io/ALI){:target="_blank"}.) as well as OpenAI: 

> “This approach [GANs for semi-supervised learning] allows us to obtain state of the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network — a result that’s very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is very promising because labeled examples can be quite expensive to obtain in practice.” [link](https://openai.com/blog/generative-models/){:target="_blank"}

To summarize: this means that people can get performance as good as the best networks in the field trained with the full dataset. 

I foresee that the next improvements will apply similar techniques to more complex datasets and using fewer and fewer labelled examples to [match the state-of-the art performance](http://rodrigob.github.io/are_we_there_yet/build){:target="_blank"}. This can probably be realized by not only using transfer learning as the “training agent” more effectively (or using [multi-task learning](https://en.wikipedia.org/wiki/Multi-task_learning){:target="_blank"}), but also by using advanced GANs themselves, as hinted to above. Or maybe even to move closer to optimal [Active Learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)){:target="_blank"} by generating exactly the hardest (real) examples for the network to classify at that point in time to improve the fastest. (This does **not** depend on advances in other areas, such as [Synthetic Gradients](https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients){:target="_blank"}, better optimizers etc.)

In other words, now we are advancing, in a sense, in the "theory of knowledge" of Machine Learning algorithms so that they can learn faster. Akin to a child in middle-school, deep learning is now discovering "learning aids" or "hacks" to learn even faster and generalize better. This means that we can move AI closer to production and usefulness. This means that plugging into a new (business) vertical will be easier than ever. You will still need to develop vertical-specific expertise and architecture, but technology like generic object detection, [speech synthesis](https://www.technologyreview.com/s/603811/baidus-artificial-intelligence-lab-unveils-synthetic-speech-system/){:target="_blank"} or sentiment analysis will be commoditized.


** 3. Commodtizied ML is here, so what? **


These advancements along with better tools for AI researchers and production, will favor companies who have (i) unique business partnerships or (ii) overcome some unique regulatory hurdle. Huge opportunities are still in [FinTech](https://www.bloomberg.com/news/articles/2017-02-28/jpmorgan-marshals-an-army-of-developers-to-automate-high-finance){:target="_blank"} and other regulation-heavy industries.

However, the key point is to realize that the AI of the future will likely just serve as a preprocessing or feature engineering step feeding into the next, traditional data science, layer of algorithms. But make no mistake: this is huge. Computers, for the first time in history, will have the ability to see and create their own representations of objects. To take a practical example: I have recently [been through an accelerator](http://joinef.com){:target="_blank"} where [one of the companies, Observe, is working on automating fish farming](http://observe.tech/){:target="_blank"}. They have a standard object recognition algorithm that passes on its output to further systems managing feeding of fish etc., so that they maximize the fish stock and minimize waste. Their key selling proposition in this case is the second, fish feeding, algorithm, but until couple of years ago there was no easy way to explain the images of fish to the second (control) algorithm. 

To take a traditional computer science analogy, we can think of the case of [John Searle's Chinese room experiment](https://en.wikipedia.org/wiki/Chinese_room){:target="_blank"}: so far the computer was just manipulating symbols with no conception of what those symbols mean. I.e. the computer could not be thought of as "understanding" something or being "conscious". I do not want to delve into the philosophical implications of this statement, but the core dilemma is: because the computer has no context, it cannot be thought of as operating on anything else than a (computationally) straightforward set of instructions (no matter how complex in human terms). Now, computers are getting more context, so, in an odd sense, we are closer to understanding. 

<img src='https://upload.wikimedia.org/wikipedia/commons/d/de/ChineseRoom2009_CRset.jpg' style='width: 90%'>

> From Wikipedia under CC license.

Keeping in line just with the practical sense, computers will now be able to auto-extract features. In fact, [for instance Eyn's](http://eyn.vision/){:target="_blank"} core tech is already a facial detection algorithm fed into spoofing detection. This is only possible because facial detection is so close to perfect we can rely on it almost as deterministic. Andrew Ng talked about the importance of moving from 95% to 99%, I think there is another, perhaps even more important milestone in moving from 99 to 99.9% where we can use commoditized machine learning almost as a framework for our core tech.

This is especially important in parts of the world where databases about the real world are not as common as in the EU. Because algorithms can now integrate into physical space and get data from it directly, we can build directly onto the physical space: i.e. we can provide value to all of the ~6.1 billion people currently underserved by technology and apply technology directly to areas they find most useful, like agriculture: [Optimal Labs](http://optimal.ag/){:target="_blank"} is already doing that. Moreover, with all this computer vision, it will be much easier to jump-start projects like city planning: in fact, this is already happening in India.

The last thing I would like to talk about is a direct implication of the previous paragraph, but has far reaching consequences. For the first time, IT systems may make sense even in situations in which there is no other IT system to connect to or IT support. Algorithms now do not need supporting database--as they can create their own or rely on pre-trained models--so they can be rolled out to a completely new company without the usual enterprise sales & integration cycle. In theory, you can have an entire IT system managed by external companies as it is quite expensive to have AI talent or ML maintenance on site (e.g. in agriculture) beyond simple mechanics. Now, this is wildly speculative, but if true, this absence of in-house IT people means that external companies can deploy to completely novel areas (and there'll be more of those opportunities because in-house IT is not there to pitch obvious technology solutions) and capture much more of the market. But it also means that there are no longer IT people to guide and support you so the business contacts are now more important than ever. 

** Conclusion **

So from the recent advancements and meta-learning theory it is clear that we are in an age of vertical AI and that the future direction is likely to heavily rely on understanding the business use-cases of particular AI technology. This seems to be further accentuated by the fact that AI can now be deployed on its own without external support or databases to connect to. This means that the business side of AI will be more important than ever, but also the opportunities are becoming more plentiful, because even the very obvious usecases might not be covered by in-house teams.  
