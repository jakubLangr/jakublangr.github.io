<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Langr's Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2014-09-22T21:00:00+02:00</updated><entry><title>Tracking the CrISIS in Syria</title><link href="/tracking-crisis.html" rel="alternate"></link><updated>2014-09-22T21:00:00+02:00</updated><author><name>Jakub Langr</name></author><id>tag:,2014-09-22:tracking-crisis.html</id><summary type="html">&lt;p&gt;As I was working as a contractor this summer, I came across a very interesting project called &lt;a href="http://www.gdeltproject.org/"&gt;GDELT&lt;/a&gt; -- Global Database of Events, Language and Tone. This project I think is best described by the official website statement: "GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world."&lt;/p&gt;
&lt;p&gt;In addition to that, as a part of &lt;a href="../hello-world.html"&gt;Good Judgement Project&lt;/a&gt;, I was made to forecast on Syria and ISIS' actions. As I was going through the articles describing what is happening in Syria I became to wonder: what is the bigger picture? To what extent is this hype all just, because &lt;strong&gt;now&lt;/strong&gt; ISIS is the headline-grabber in the West? (But of course, no one cares until it becomes a story in the public consciousness.) That's when GDELT into the picture. I wanted to try it as I came across it, but since I was finishing my data science project, I did not quite have the time. I was awe-struck by the complexity and potential of this project so I knew I wanted to give a go. But since my first internship in Pearson in 2013, I am not afraid of large analytics, especially if I can use R to do it.&lt;/p&gt;
&lt;p&gt;Fortunately, Google was so nice as to host this almost 100 GB dataset on &lt;a href="http://googlecloudplatform.blogspot.cz/2014/05/worlds-largest-event-dataset-now-publicly-available-in-google-bigquery.html"&gt;its cloud for free&lt;/a&gt;, for which I love Google. Not only that, but Google loaded their dataset so that it can be queried using &lt;a href="http://en.wikipedia.org/wiki/BigQuertracky"&gt;BigQuery&lt;/a&gt; using public-facing APIs or a web interface. We can then query GDELT using SQL like as shown below and subsequently export the dataset as a CSV:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;FROM&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gdelt&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;bq&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;full&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;WHERE&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; 
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MonthYear&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;201303&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;2015&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;NumSources&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;GoldsteinScale&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt; &lt;span class="n"&gt;AND&lt;/span&gt;
&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Actor1Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor1Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Geo_CountryCode&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SY&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;IMG&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;Actor2Code&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;REB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;ORDER&lt;/span&gt; &lt;span class="n"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(For those interested, I include a smaller version of the dataset I was working with after exporting from GDELT &lt;a href="https://dl.dropboxusercontent.com/u/30848031/blog/sample.csv"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The assumption of this endeavor is that if I include non-English media (and perhaps the less main-stream ones too) by using GDELT's information rather than Google, which will always give me traditional sources, I can get a better sense of what is going on and by the law of large numbers, I will not get too many false positives from spurious sources, so my overall analysis should remain robust and even more objective than relying on sources such as CNN or BBC alone. One additional advantage GDELT has is that it is updated every 24 hours. In theory, this analysis could be automated so that it updates itself every 24 hours. (Actually, it should not be &lt;em&gt;that&lt;/em&gt; much more work ... but given the size of GDELT, I would have to start paying for using the BigQuery service that often.)&lt;/p&gt;
&lt;p&gt;As with any more substantive project, there was a lot of data cleaning and obstacles. Namely: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ISIS not recognized by GDELT
There's lots of International Militarized Groups (IMGs) recognized by GDELT, but ISIS seems to be too new in the headlines to be recognized by GDELT coding yet. Hence I had to look for substitutes:&lt;/li&gt;
&lt;li&gt;IMG (International Militarized Group) tag to the rescue
There is a column specifying actor &lt;em&gt;type&lt;/em&gt; for each event. There is an actor type IMG, which was fortunate. However, there is no way of fully knowing (at least with this level of granularity, but I am sure it would be technically possible, if GDELT team so chose.)&lt;/li&gt;
&lt;li&gt;But sometimes "REB"
When individually inspecting the events (just some of them, I do not have the time to go through the entire data dump) &lt;/li&gt;
&lt;li&gt;Cleaning the data
For some events it is hard, if not impossible to correctly specify precise coordinates. As a result, GDELT merely inputs the values of the approximate center of Syria. These records obviously needed to be removed from the dataset. Obviously, there was couple of more steps that I needed to do and a couple that tricked me for a little bit when I found that the state code of one of the events was &lt;em&gt;IS&lt;/em&gt;, which, however, stood for Israel, as I later found out.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GDELT project classifies each event using &lt;a href="http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt"&gt;one of their internal codes&lt;/a&gt;, each of these codes then gets then a &lt;a href="http://web.pdx.edu/~kinsella/jgscale.html"&gt;Goldstein score&lt;/a&gt;, which basically expresses whether this event whether people are being nice to each other (positive score) or mean/evil towards each other (negative score). Where +10.0 is some massive humanitarian/developmental aid project, +1.0 is probably visit of some minor national politican. All the positive things were filtered out, because I wanted to focus on what the problems were. There -1.1 is, for instance, "Deny an attributed policy, action, role or position" and -10.0 is "Military attack; clash; assault".&lt;/p&gt;
&lt;p&gt;As the more SQL-knowledgeable of you might have noticed, I only considered events that came from 2 independent sources and happened on or after March 2013. &lt;/p&gt;
&lt;h1&gt;Let's explore&lt;/h1&gt;
&lt;p&gt;With this dataset in hand, I made several maps where I plotted each event as a point sized according to its Goldstein score and set relatively low transparency so that all hotspots appear in bright red. I plotted this dataset by month, and here's March 2013 till July 2013. We see that at the end of this period, there's was violence spearing from its initial hotspots. &lt;/p&gt;
&lt;h3&gt;March and April 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201303.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201304.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;May and July 2013&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201305.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201306.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Now, if we look at the period August till November, we see that there's a lot more violence then. But to some extent we have to bear in mind that this dataset includes &lt;em&gt;all&lt;/em&gt; rebels and not just ISIS. &lt;/p&gt;
&lt;h3&gt;August 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201308.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;September 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201309.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;October 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201310.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;h3&gt;November 2013&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201311.png" alt="Map of Syria" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we have to consider the connected events: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=Syria,+Chemical+Weapons&amp;date=1/2013+12m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Here, I am using Google Trends as a proxy for "Western interest" in the matter.&lt;/p&gt;
&lt;p&gt;Clearly, August and September are easily explained by the usage of chemical weapons. But the violence following this, especially in October and later, cannot be explained just by looking at chemical weapons, because that is when the hype ended and has not picked up until ISIS was the next important thing.&lt;/p&gt;
&lt;p&gt;The following months were similarly violent and few in the West seemed to care. &lt;/p&gt;
&lt;h3&gt;December 2013 and January 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201312.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201401.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;February and March 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201402.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201403.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;April and May 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201404.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201405.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;If we plot the Google trend graph again, since the start of Syrian Civil War in March 2011, we see that the attention Syria got around the times of the use of chemical weapons is unparalleled. This is hardly surprising, but what is odd is that there a very typical &lt;a href="http://www.cbsnews.com/news/syria-chemical-weapons-attack-blamed-on-assad-but-wheres-the-evidence/"&gt;disagreement regarding who actually used them&lt;/a&gt;, though I think that at least some of these attacks were sure to be made by the government. &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=Syria&amp;date=1/2009+43m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_1&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;But despite the limited interested by the West, violence was actually similar to the months during which the chemical weapons were used. &lt;/p&gt;
&lt;p&gt;What is interesting that ISIS was not picked up by Western media much until recent months, despite the fact that ISIS can be traced back to 2004: &lt;/p&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;cat=0-16-396&amp;q=%22Islamic+State%22,+ISIS,+ISIL&amp;date=today+12-m&amp;cmpt=q&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=500&amp;h=330"&gt;&lt;/script&gt;

&lt;p&gt;Still, violence was hovering around the same level:&lt;/p&gt;
&lt;h3&gt;June and July 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201406.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201407.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;h3&gt;August and September 2014&lt;/h3&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201408.png" alt="Map of Syria" align="left" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/201409.png" alt="Map of Syria" align="right" style="width: 310px;"/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;What is interesting is that it seems that despite the tons of graphical content that ISIS uploads, its terror tactics are largely successful: Syria does not seem, by and large, more violent than it was in the few months prior to ISIS' inclusion in the Western media's limelight. If anything, it seems that the number of violent occurrences has decreased. But it seems that the terror-tactics and the headline-grabbing strategy of ISIS is already working well for them. West pays attention.&lt;/p&gt;
&lt;p&gt;I am currently working on an interactive R-Shiny app for this visualization/analysis so stay tuned!&lt;/p&gt;
&lt;p&gt;Want the R script? Want to give feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</summary><category term="modeling"></category><category term="GDELT"></category><category term="interactive"></category><category term="Syria"></category></entry><entry><title>Predictions R hard</title><link href="/hello-world.html" rel="alternate"></link><updated>2014-09-14T21:00:00+02:00</updated><author><name>Jakub Langr</name></author><id>tag:,2014-09-14:hello-world.html</id><summary type="html">&lt;h1&gt;... or quick and dirty statistical modeling in R&lt;/h1&gt;
&lt;p&gt;Recently, I decided to start a blog and my ongoing involvement in the &lt;a href="http://goodjudgmentproject.com/"&gt;Good Judgment Project (GJP)&lt;/a&gt;, which is part of US government's &lt;a href="http://www.iarpa.gov"&gt;Intelligence Advanced Research Projects Activity (IARPA)&lt;/a&gt;, more specifically the ACE Project, proved to be a decent excuse to do so. &lt;/p&gt;
&lt;p&gt;I have joined GJP mid-Season 3 (hence I haven't participated fully) and then I tested into season 4 (combination of general intelligence test and political knowledge). This test alone was a treat, because it followed all the guidelines about best practices of hiring (to my knowledge) and I knew that if I get in, this will be a lot of fun. What I did not know at the time was that only one in ten will actually be accepted into GJP, otherwise I would probably put in a lot more work into the test.&lt;/p&gt;
&lt;p&gt;Despite this mess-up, I joined a project and met my amazing fellow forecasters in my team. At the time of joining, I already knew that I will be putting numeric probabilistic values on how the world might soon turn out to be (e.g. will there be a direct Russia-Ukraine confrontation in Crimea). All of these questions are well-defined using a very sophisticated set of definitions, but making the evaluation criteria very flexible and as to allow GJP Team to 'resolve' each question in the spirit rather than the letter of the question.&lt;/p&gt;
&lt;p&gt;Number of these questions have allowed for use of publicly available datasets and so I would like to offer some code that I used to ease up my work as a forecaster. This particular example is relates to a question that resolved yesterday: If on a certain date the area of ice on the Arctic sea will be more than what it was last year.&lt;/p&gt;
&lt;p&gt;The following R code downloads, cleans and plots some time-series representation of the record ed data of the ice for the past couple of years. It is &lt;em&gt;super-simple&lt;/em&gt;, but I was able to do it from question to quantitative probabilities in about 30 minutes, so this is just to demonstrate that even such simple code can help you make a much more solid prediction. This was necessary, because my team's forecasts were all over the place (among 9 forecasters we ranged 5-80%) and statistical modeling is a great way how to deal with data that is normally quite counter-intuitive and complex.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;TTR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reshape&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tseries&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;forecast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Loading, cleaning (missing values are by default assigned value of -9999)&lt;/span&gt;
read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ijis.iarc.uaf.edu/seaice/extent/plot_v2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
replace&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-9999&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
colSums&lt;span class="p"&gt;(&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Melt changes the columns (there is a column for each year) into just one column &amp;#39;value&amp;#39;&lt;/span&gt;
data_ts &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; melt&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
ts &lt;span class="o"&gt;=&lt;/span&gt; ts&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt; deltat&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data_ts &lt;span class="o"&gt;=&lt;/span&gt; na.remove&lt;span class="p"&gt;(&lt;/span&gt;ts&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Decomposes the time-series plots the decomposition and the forecast (as well as prints it)&lt;/span&gt;
decomposed_ts &lt;span class="o"&gt;=&lt;/span&gt; decompose&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;decomposed_ts&lt;span class="p"&gt;)&lt;/span&gt;
ts_forecast &lt;span class="o"&gt;=&lt;/span&gt; HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt;gamma&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts &lt;span class="o"&gt;=&lt;/span&gt; forecast.HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;ts_forecast&lt;span class="p"&gt;,&lt;/span&gt;h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts
plot.forecast&lt;span class="p"&gt;(&lt;/span&gt;forecasts&lt;span class="p"&gt;)&lt;/span&gt;
data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;250&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;260&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For GJP purposes, the confidence intervals produced by this model are especially useful, because they give you an idea of how confident you can be in your predictions:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;forecasts&lt;/span&gt; 
&lt;span class="n"&gt;Point&lt;/span&gt; &lt;span class="n"&gt;Forecast&lt;/span&gt; &lt;span class="n"&gt;Lo&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="n"&gt;Hi&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="n"&gt;Lo&lt;/span&gt; &lt;span class="mi"&gt;95&lt;/span&gt; &lt;span class="n"&gt;Hi&lt;/span&gt; &lt;span class="mi"&gt;95&lt;/span&gt; 
&lt;span class="mf"&gt;16.73710&lt;/span&gt; &lt;span class="mi"&gt;4981786&lt;/span&gt; &lt;span class="mi"&gt;4920648&lt;/span&gt; &lt;span class="mi"&gt;5042924&lt;/span&gt; &lt;span class="mi"&gt;4888284&lt;/span&gt; &lt;span class="mi"&gt;5075288&lt;/span&gt; 
&lt;span class="mf"&gt;16.73995&lt;/span&gt; &lt;span class="mi"&gt;4963324&lt;/span&gt; &lt;span class="mi"&gt;4867154&lt;/span&gt; &lt;span class="mi"&gt;5059494&lt;/span&gt; &lt;span class="mi"&gt;4816245&lt;/span&gt; &lt;span class="mi"&gt;5110403&lt;/span&gt; 
&lt;span class="mf"&gt;16.74279&lt;/span&gt; &lt;span class="mi"&gt;4944862&lt;/span&gt; &lt;span class="mi"&gt;4814956&lt;/span&gt; &lt;span class="mi"&gt;5074768&lt;/span&gt; &lt;span class="mi"&gt;4746188&lt;/span&gt; &lt;span class="mi"&gt;5143536&lt;/span&gt; 
&lt;span class="mf"&gt;16.74564&lt;/span&gt; &lt;span class="mi"&gt;4926399&lt;/span&gt; &lt;span class="mi"&gt;4762199&lt;/span&gt; &lt;span class="mi"&gt;5090600&lt;/span&gt; &lt;span class="mi"&gt;4675277&lt;/span&gt; &lt;span class="mi"&gt;5177522&lt;/span&gt; 
&lt;span class="mf"&gt;16.74849&lt;/span&gt; &lt;span class="mi"&gt;4907937&lt;/span&gt; &lt;span class="mi"&gt;4708313&lt;/span&gt; &lt;span class="mi"&gt;5107562&lt;/span&gt; &lt;span class="mi"&gt;4602638&lt;/span&gt; &lt;span class="mi"&gt;5213237&lt;/span&gt; 
&lt;span class="mf"&gt;16.75134&lt;/span&gt; &lt;span class="mi"&gt;4889475&lt;/span&gt; &lt;span class="mi"&gt;4653078&lt;/span&gt; &lt;span class="mi"&gt;5125872&lt;/span&gt; &lt;span class="mi"&gt;4527937&lt;/span&gt; &lt;span class="mi"&gt;5251014&lt;/span&gt; 
&lt;span class="mf"&gt;16.75419&lt;/span&gt; &lt;span class="mi"&gt;4871013&lt;/span&gt; &lt;span class="mi"&gt;4596412&lt;/span&gt; &lt;span class="mi"&gt;5145614&lt;/span&gt; &lt;span class="mi"&gt;4451047&lt;/span&gt; &lt;span class="mi"&gt;5290979&lt;/span&gt; 
&lt;span class="mf"&gt;16.75704&lt;/span&gt; &lt;span class="mi"&gt;4852551&lt;/span&gt; &lt;span class="mi"&gt;4538291&lt;/span&gt; &lt;span class="mi"&gt;5166811&lt;/span&gt; &lt;span class="mi"&gt;4371932&lt;/span&gt; &lt;span class="mi"&gt;5333170&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the plots here can help elucidate why my team might have been all over the place: there are many factors influencing how much ice there will be on a given day: sure, there is the seasonal (summer-winter) cycle, which matters a lot, but there is also some random element as well as measurement error. I fear that lots of my fellow forecasters (perhaps including myself for a long time) fell victim to &lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;over-fitting&lt;/a&gt; to the random element. Sure, we could observe the pattern from the previous year, but no one from the team--to my knowledge--plotted the data to see what (approximate) influence each of the two factors have. This is why decomposition is so great:&lt;/p&gt;
&lt;h2&gt;Decomposed time-series plot&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://dl.dropboxusercontent.com/u/30848031/blog/gjp_modelling_ts_decomposition.svg" alt="Timeseries decomposition of ice-area" align="center" style="width: 620px;"/&gt;&lt;/p&gt;
&lt;p&gt;One interesting element of the decomposition is the trend, which I was expecting to see fairly clear evidence of global warming, but instead I really found out "just" evidence of climate change. Though, of course, I am no climatologist, so perhaps that interesting dip in the year 2000 had some other (spurious?) reasons. &lt;/p&gt;
&lt;p&gt;The decomposition, although helpful, was not what I considered the main advantage of this little exercise: I was then able to use a time-series statistical model called HoltWinters to predict the area of ice (no pun intended?), which uses exponential smoothing that is quite fitting in these conditions (I wonder whether these statistical puns significantly affect my readership?). This model is appropriate mainly because there is a lot of historical data and the data is always very constrained to a certain range (e.g. a 3 sigma change is &lt;em&gt;actually&lt;/em&gt; extremely unlikely, which cannot be said of all financial data this model is applied to). &lt;/p&gt;
&lt;h1&gt;Thoughts about GJP itself&lt;/h1&gt;
&lt;p&gt;GJP is what I think social science should be: quantifiable, based on best-practices and aiming to get as close to the truth as possible, not tell the best story. This was, interestingly enough, the conclusion (&lt;em&gt;simplifying tremendously&lt;/em&gt;) of Professor Philip Teltock in &lt;a href="http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715"&gt;his book Expert Political Judgment&lt;/a&gt;. Namely that the political forecasters that tell the best story are usually favored by the media, because it sells well, but they tend to do worse than most others on accuracy. Professor Tetlock is now one of the main organizers of GJP and so it is a great honor to work as his lab rat, because it helps to do cutting-edge research and it helps me to learn a ton in the process.&lt;/p&gt;
&lt;p&gt;One additional gift every forecaster got from the GJP was a set of presentations of best practices of forecasting, which I think were fascinating, albeit oftentimes simple guidelines to stick to. But as with everything, it is much more a matter of putting these into practice rather that makes a great forecaster. &lt;/p&gt;
&lt;p&gt;Couple of interesting things that I noticed about myself looking at old predictions: &lt;/p&gt;
&lt;h2&gt;1. I am overly cautious&lt;/h2&gt;
&lt;p&gt;Reading loads of literature about randomness and its role in human society (Kahneman, Taleb, Tversky, Ariely, Mlodinow etc.) I was overly careful about putting down anything close to 1% or 99%, even though some questions call for it (likelihood of a massive reform of international institutions in a short timespan). Good Judgment, after all, is not only very well calibrated, but it is also very discriminating. &lt;/p&gt;
&lt;h2&gt;2. I struggle with randomness&lt;/h2&gt;
&lt;p&gt;Interesting point about randomness--to some extent related to the previous one--how do we square the fact that randomness is just a model and yet we keep on invoking the properties of randomness when talking about the world. But, at least on the level of humans, each action is supposedly deterministic. Yes, some might call on the &lt;a href="http://www.quantumdiaries.org/2014/07/04/wrong/"&gt;famous George Box quote&lt;/a&gt;, but that does not really answer why in a situation of imperfect information &lt;em&gt;random&lt;/em&gt; is the &lt;em&gt;best&lt;/em&gt; model. This is especially confusing given the amount of strategic interaction and social influence in the world. &lt;/p&gt;
&lt;p&gt;This turned out to be a problem with with some of the GJP predictions I make, because I want to stick to randomness as a baseline (because it seems to be best practice), but then struggle to always justify this to myself and square it with the rest of information. This especially bothered me outside of GJP, however. In Mlodinow's book &lt;a href="http://www.amazon.com/The-Drunkards-Walk-Randomness-Rules/dp/0307275175"&gt;The Drunkard's Walk&lt;/a&gt;, he specifically used the example of production studio's CEOs as someone who is faced with great randomness in the movies. But each action in this immensely complex chain of interactions is deterministic, strategic and influenced by one's surroundings. Why is randomness an emergent property of human systems? Seems non-obvious.&lt;/p&gt;
&lt;h2&gt;3. Conditional interaction is hard&lt;/h2&gt;
&lt;h3&gt;(especially cross-culture)&lt;/h3&gt;
&lt;p&gt;There was a question on what would DPRK do, should the United States take a bold, but merely supportive, military action in aid of South Korea. Our team struggled to agree even if the bold action would increase chance of state DPRK being more aggressive towards South Korea or less so. Would the desire to prove something to US (or more generally, the West) outweigh the potential risks, because of the importance of the precedent or would the bold action scare off DPRK? Both ways of reasoning sounded equally plausible to me at the time so I just followed what my initial prediction was. But really, there was no reason for it. The question whether I got it right or wrong is irrelevant in this case; the question is: would I be able to make a good prediction next time? I fear that so far the answer is no and so I never swayed too far off 50%. I will probably need to devise a way of breaking these ties.&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</summary><category term="modeling"></category><category term="timeseries"></category><category term="R"></category><category term="statistics"></category></entry><entry><title>About Me</title><link href="/about.html" rel="alternate"></link><updated>2014-09-14T20:00:00+02:00</updated><author><name>Jakub Langr</name></author><id>tag:,2014-09-14:about.html</id><summary type="html">&lt;p&gt;Welcome to my blog! I am a final year undergraduate at University of Oxford studying mostly economics, but really just spending loads of time on &lt;a href="http://cousera.org"&gt;Cousera.org&lt;/a&gt; and similar awesome MOOC sites (I mean, I have to love them, I already finished 12 courses...) playing with machine learning, data science, statistics, computer science and all the things awesome.&lt;/p&gt;
&lt;p&gt;I love to hack data and I worked in a data science position for a half a year at Pearson Plc, but I also have full-time like experience from international consultancies and NGOs. &lt;/p&gt;
&lt;p&gt;If you want to, feel free to get in touch at james [dot] langr [at] gmail [dot] com or via the LinkedIn profile in links!&lt;/p&gt;</summary><category term="me"></category><category term="CV"></category></entry></feed>