<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Langr's Blog</title><link href="/" rel="alternate"></link><link href="/feeds/Statistics.atom.xml" rel="self"></link><id>/</id><updated>2014-09-14T21:00:00+02:00</updated><entry><title>Hello World! What are you planning?</title><link href="/hello-world.html" rel="alternate"></link><updated>2014-09-14T21:00:00+02:00</updated><author><name>Jakub Langr</name></author><id>tag:,2014-09-14:hello-world.html</id><summary type="html">&lt;h1&gt;... or quick and dirty statistical modeling in R&lt;/h1&gt;
&lt;p&gt;Recently, I decided to start a blog and my ongoing involvement in the &lt;a href="http://goodjudgmentproject.com/"&gt;Good Judgment Project (GJP)&lt;/a&gt;, which is part of &lt;a href="http://www.iarpa.gov"&gt;Intelligence Advanced Research Projects Activity's (IARPA)&lt;/a&gt; ACE Project, proved to be a decent excuse to do so. &lt;/p&gt;
&lt;p&gt;I have joined GJP mid-Season 3 (hence I haven't participated fully) and then I tested into season 4 (combination of general intelligence test and political knowledge). This test alone was a treat, because it followed all the guidelines about best practices of hiring (to my knowledge) and I knew that if I get in, this will be a lot of fun. What I did not know at the time was that only one in ten will actually be accepted into GJP, otherwise I would probably put in a lot more work into the test.&lt;/p&gt;
&lt;p&gt;Despite this mess-up, I joined a project and met my amazing fellow forecasters in my team. At the time of joining, I already knew that I will be putting numeric probabilistic values on how the world might soon turn out to be (e.g. will there be a direct Russia-Ukraine confrontation in Crimea). All of these questions are well-defined using a very sophisticated set of definitions, but making the evaluation criteria very flexible and as to allow GJP Team to 'resolve' each question in the spirit rather than the letter of the question.&lt;/p&gt;
&lt;p&gt;Number of these questions have allowed for use of publicly available datasets and so I would like to offer some code that I used to ease up my work as a forecaster. This particular example is relates to a question that resolved yesterday: Whether on a certain date the area of ice will of a certain sea surpass given level. (I am being purposefully vague here, because this &lt;em&gt;was&lt;/em&gt; a real question and even though there was no mention of forecasters not being able to share their questions, thoughts and methodology and many of the people on the GJP team regularly post about the project and its details, I would like to stay on the safe side.) &lt;/p&gt;
&lt;p&gt;The following R code downloads, cleans and plots some time-series representation of the recorded data of the ice for the past couple of years. It is &lt;em&gt;super-simple&lt;/em&gt;, but I was able to do it from question to quantitative probabilities in about 30 minutes, so this is just to demonstrate that even such simple code can help you make a much more solid prediction. This was necessary, because my team's forecasts were all over the place ( !!!!! PROVIDE SOME EXAMPLE ) and statistical modeling is a great way how to deal with data that is normally quite counter-intuitive and complex.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;TTR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;reshape&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tseries&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
library&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;forecast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;http://www.ijis.iarc.uaf.edu/seaice/extent/plot_v2.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
replace&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;,&lt;/span&gt;data&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-9999&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; data
colSums&lt;span class="p"&gt;(&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;))&lt;/span&gt;
data_ts &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; melt&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
ts &lt;span class="o"&gt;=&lt;/span&gt; ts&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt; deltat&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data_ts &lt;span class="o"&gt;=&lt;/span&gt; na.remove&lt;span class="p"&gt;(&lt;/span&gt;ts&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
decomposed_ts &lt;span class="o"&gt;=&lt;/span&gt; decompose&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;decomposed_ts&lt;span class="p"&gt;)&lt;/span&gt;
ts_forecast &lt;span class="o"&gt;=&lt;/span&gt; HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;data_ts&lt;span class="p"&gt;,&lt;/span&gt;gamma&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts &lt;span class="o"&gt;=&lt;/span&gt; forecast.HoltWinters&lt;span class="p"&gt;(&lt;/span&gt;ts_forecast&lt;span class="p"&gt;,&lt;/span&gt;h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecasts
&lt;span class="c1"&gt;#plot.forecast(forecasts)&lt;/span&gt;
data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;250&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;260&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the plots here can help elucidate why my team might have been all over the place: there are many factors influencing how much ice there will be on a given day: sure, there is the seasonal (summer-winter) cycle, which matters a lot, but there is also some random element as well as measurement error. I fear that lots of my fellow forecasters (perhaps including myself for a long time) fell victim to &lt;a href="http://en.wikipedia.org/wiki/Overfitting"&gt;over-fitting&lt;/a&gt; to the random element. Sure, we could observe the pattern from the previous year, but no one from the team--to my knowledge--plotted the data to see what (approximate) influence each of the two factors have. This is why decomposition is so great:&lt;/p&gt;
&lt;h2&gt;Decomposed time-series plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="yay!" src="http://upload.wikimedia.org/wikipedia/commons/b/b0/NewTux.svg" title="Linux!" /&gt;&lt;/p&gt;
&lt;p&gt;One interesting element of the decomposition is the trend, which I was expecting to see fairly clear evidence of global warming, but instead I really found out "just" evidence of climate change. Though, of course, I am no climatologist, so perhaps that interesting dip in the year 2000 had some other (spurious?) reasons. &lt;/p&gt;
&lt;p&gt;The decomposition, although helpful, was not what I considered the main advantage of this little exercise: I was actually able to use a reasonable time-series statistical model HoltWinters (no pun intended?), which uses the strategy of exponential smoothing, which is quite fitting in these conditions (I feel like these statistical puns significantly decrease my readership). This is mainly because there is a lot of historical data and the data is always very constrained to a certain range (e.g. a 10 sigma movement is &lt;em&gt;actually&lt;/em&gt; extremely unlikely, which cannot be said of all financial data this model is applied to). &lt;/p&gt;
&lt;h1&gt;Thoughts about GJP itself&lt;/h1&gt;
&lt;p&gt;GJP is what I think social science should be: quantifiable, based on best-practices and aiming to get as close to the truth as possible, not tell the best story. This was, interestingly enough, the conclusion (&lt;em&gt;simplifying tremendously&lt;/em&gt;) of Professor Philip Teltock in &lt;a href="http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715"&gt;his book Expert Political Judgment&lt;/a&gt;. Namely that the political forecasters that tell the best story are usually favored by the media, because it sells well, but they tend to do worse than most others on accuracy. Professor Tetlock is now one of the main organizers of GJP and so it is a great honor to work as his lab rat, because it helps to do cutting-edge research and it helps me to learn a ton in the process.&lt;/p&gt;
&lt;p&gt;One additional gift every forecaster got from the GJP was a set of presentations of best practices of forecasting, which I think were fascinating, albeit oftentimes simple guidelines to stick to. But as with everything, it is much more a matter of putting these into practice rather that makes a great forecaster. &lt;/p&gt;
&lt;p&gt;Couple of interesting things that I noticed about myself looking at old predictions: &lt;/p&gt;
&lt;h2&gt;1. I am overly cautious&lt;/h2&gt;
&lt;p&gt;Reading loads of literature about randomness and its role in human society (Kahneman, Taleb, Tversky, Ariely, Mlodinow etc.) I was overly careful about putting down anything close to 1% or 99%, even though some questions call for it (likelihood of a massive reform of international institutions in a short timespan). Good Judgment, after all, is not only very well calibrated, but it is also very discriminating. &lt;/p&gt;
&lt;h2&gt;2. I struggle with randomness&lt;/h2&gt;
&lt;p&gt;Interesting point about randomness--to some extent related to the previous one--how do we square the fact that randomness is just a model and yet we keep on invoking the properties of randomness when talking about the world. But, at least on the level of humans, each action is supposedly deterministic. Yes some might call on the &lt;a href="http://www.quantumdiaries.org/2014/07/04/wrong/"&gt;famous George Box quote&lt;/a&gt;, but that does not really answer why in a situation of imperfect information &lt;em&gt;random&lt;/em&gt; is the &lt;em&gt;best&lt;/em&gt; model. This is especially confusing given the amount of strategic interaction and social influence in the world.&lt;/p&gt;
&lt;p&gt;This especially bothered me with Mlodinow's book &lt;a href="http://www.amazon.com/The-Drunkards-Walk-Randomness-Rules/dp/0307275175"&gt;The Drunkard's Walk&lt;/a&gt;, because he specifically used the example of production studio's CEOs as someone who is faced with great randomness in the movies. But each action in this immensely complex chain of interactions is deterministic, strategic and influenced by one's surroundings. Why is randomness an emergent property of human systems? Seems non-obvious.&lt;/p&gt;
&lt;h2&gt;3. Conditional Interaction is Hard&lt;/h2&gt;
&lt;h3&gt;(Especially Cross-Culture)&lt;/h3&gt;
&lt;p&gt;There was a question on what would a certain state &lt;strong&gt;A&lt;/strong&gt; do should the United States take a bold, but merely supportive, military action in aid of its ally &lt;strong&gt;B&lt;/strong&gt;. Our team struggled to agree even if the bold action would increase chance of state &lt;strong&gt;A&lt;/strong&gt; being more aggressive towards &lt;strong&gt;B&lt;/strong&gt; or less so. Would the desire to prove something to US (or more generally, the West) outweigh the potential risks, because of the importance of the precedent or would the bold action scare off &lt;strong&gt;A&lt;/strong&gt;? Both ways of reasoning sounded equally plausible to me at the time so I just followed what my initial prediction was. But really, there was no reason for it. The question whether I got it right or wrong is irrelevant in this case; the question is: would I be able to make a good prediction next time? I fear that so far the answer is no and so I never swayed too far off 50%.&lt;/p&gt;
&lt;p&gt;This is all for now. Thanks for reading! 
Jakub&lt;/p&gt;
&lt;p&gt;Feedback? Comments? Question? I am happy to hear it! Contact me at james [dot] langr [at] gmail [dot] com.&lt;/p&gt;</summary><category term="modeling"></category><category term="timeseries"></category><category term="R"></category><category term="statistics"></category></entry></feed>